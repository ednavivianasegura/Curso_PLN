{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ednavivianasegura/Curso_PLN/blob/main/Curso_PLN_UR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce59cdd3-f22c-4cc9-ba35-b8fbe5ce3d57",
      "metadata": {
        "id": "ce59cdd3-f22c-4cc9-ba35-b8fbe5ce3d57"
      },
      "source": [
        "***\n",
        "<center>\n",
        "<img src=\"https://github.com/ednavivianasegura/Curso_PLN/blob/main/Portada.png?raw=1\" alt=\"portada\" width=\"80%\" height=\"80%\">  \n",
        "</center>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "# ¬øQu√© es el procesamiento del lenguale natural (PLN)?\n",
        "***"
      ],
      "metadata": {
        "id": "sSrdCkOi3Ww2"
      },
      "id": "sSrdCkOi3Ww2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src=\"https://github.com/ednavivianasegura/Curso_PLN/blob/main/PLN1.png?raw=1\" alt=\"PLN1\" width=\"98%\" height=\"98%\">  \n",
        "</center>\n"
      ],
      "metadata": {
        "id": "N6btWGYwC95M"
      },
      "id": "N6btWGYwC95M"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src=\"https://github.com/ednavivianasegura/Curso_PLN/blob/main/PLN2.png?raw=1\" alt=\"PLN2\" width=\"98%\" height=\"98%\">  \n",
        "</center>\n"
      ],
      "metadata": {
        "id": "i4jyYCTXC9i5"
      },
      "id": "i4jyYCTXC9i5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "**¬°Comencemos!**\n",
        "***"
      ],
      "metadata": {
        "id": "Srd8cQihiGZF"
      },
      "id": "Srd8cQihiGZF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Contenido\n",
        "\n",
        "\n",
        "\n",
        "1.   Estad√≠stica descriptiva univariante\n",
        "\n",
        "    *   Tablas de frecuencias.\n",
        "    *   Gr√°ficos unidimensionales.\n",
        "    *   Medidas de una variable cuantitativa:\n",
        "        * de posici√≥n:            \n",
        "            * media aritm√©tica\n",
        "            * mediana y cuantiles\n",
        "            * moda\n",
        "        * de dispersi√≥n abolsuta:\n",
        "            * varianza y desviaci√≥n t√≠pica\n",
        "    * An√°lisis de frecuencias de n-gramas.\n",
        "\n",
        "2.   Introducci√≥n a la Teor√≠a de la Probabilidad\n",
        "    *   Distribuci√≥n de probabilidad\n",
        "    *   Variable aleatoria\n",
        "    *   Probabilidad condicionada. Teorema de bayes.\n",
        "3.   Inferencia estad√≠stica\n",
        "    *   Conceptos b√°sicos.\n",
        "    *   Casos de uso.\n",
        "\n",
        "4.   Estad√≠stica multivariante\n",
        "    *   Correlaci√≥n.\n",
        "    *   Regresi√≥n.\n",
        "    *   Clasificaci√≥n: Clasificador bayesiano ingenuo.\n",
        "5.  Modelizaci√≥n PLN\n",
        "    * Modelos de lenguaje.\n",
        "    * Modelos de etiquetado gramatical.\n",
        "    * Modelos de aprendizaje autim√°tico para el PLN.  "
      ],
      "metadata": {
        "id": "I7H3OSXp6kja"
      },
      "id": "I7H3OSXp6kja"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "***\n",
        "\n",
        "# 1. Estad√≠stica descriptiva univariante\n",
        "\n",
        "<center>\n",
        " <img src=\"https://github.com/ednavivianasegura/Curso_PLN/blob/main/desc-univariante.png?raw=1\" alt=\"descriptiva\" width=\"50%\" height=\"50%\">  \n",
        "</center>\n",
        "\n",
        "***\n",
        "***"
      ],
      "metadata": {
        "id": "TURmkxgx0D6J"
      },
      "id": "TURmkxgx0D6J"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ¬øQu√© es la estad√≠stica descriptiva?:\n",
        "\n",
        "La **Estad√≠stica Descriptiva** es una rama de la estad√≠stica que se enfoca en organizar, resumir y presentar de manera significativa un conjunto de datos para facilitar su comprensi√≥n. Su objetivo principal es describir las caracter√≠sticas principales de un conjunto de datos, como la tendencia central (media, mediana, moda), las medidas no centrales (cuantiles), la dispersi√≥n (desviaci√≥n est√°ndar, rango intercuart√≠lico), la forma de la distribuci√≥n, entre otros aspectos relevantes. A trav√©s de herramientas como tablas, gr√°ficos y medidas estad√≠sticas, permitiendo visualizar y analizar patrones y comportamientos presentes en los datos.\n",
        "\n",
        "Por otro lado, una ***variable en estad√≠stica*** es una caracter√≠stica o atributo que puede tomar diferentes valores en un conjunto de datos.\n",
        "\n",
        "Es la codificaci√≥n de una caracter√≠stica de una poblaci√≥n. Lo habitual es disponer de la informaci√≥n de una o varias variables en una muestra que sea parte de la poblaci√≥n.\n",
        "\n",
        "Existen, a grandes rasgos, dos tipos de variables:\n",
        "\n",
        "*   Variables num√©ricas\n",
        "  *   Discretas (cardinalidad finita o numerable): es un tipo de variable cuantitativa que solo puede tomar valores aislados, generalmente enteros, que se obtienen mediante un conteo.\n",
        "  \n",
        "        **Por ejemplo**, si estamos analizando la longitud de los p√°rrafos en un texto, la cantidad de palabras en cada p√°rrafo ser√≠a una variable discreta, ya que solo puede tomar valores enteros y no fraccionarios. Podemos contar el n√∫mero exacto de palabras en cada p√°rrafo, lo que nos da una medida discreta y espec√≠fica.\n",
        "\n",
        "  *   Continuas (cardinalidad infinita o  no numerable): es un tipo de variable que puede tomar cualquier valor dentro de un intervalo, que se obtienen mediante una medici√≥n. Admite valores intermedios o decimales.\n",
        "        \n",
        "        **Por ejemplo**, si queremos analizar la frecuencia de la palabra \"tecnolog√≠a\" en un art√≠culo, esta variable ser√≠a continua, ya que puede tomar valores en un rango infinito y no est√° limitada a valores espec√≠ficos. Podemos tener 1.5 repeticiones por p√°rrafo, 2.3 repeticiones por p√°gina, etc., lo que nos da una medida continua y no necesariamente entera.\n",
        "\n",
        "*   Variables categ√≥ricas: es un tipo de variable que describe cualidades de la poblaci√≥n.\n",
        "  *   Las variables ordinales: se pueden ordenar linealmente.\n",
        "  \n",
        "        Ejemplo: categorizar las palabras seg√∫n su longitud en diferentes grupos ordenados, como:\n",
        "        * Palabras cortas (1-3 caracteres)\n",
        "        * Palabras de longitud media (4-6 caracteres)\n",
        "        * Palabras largas (7 o m√°s caracteres)\n",
        "\n",
        "  *   Variables nominales: establecen categor√≠as que no est√°n intr√≠nsecamente ordenadas.\n",
        "  \n",
        "        Ejemplo: categor√≠as gramaticales, como sustantivos, verbos, adjetivos, adverbios, pronombres, preposiciones, conjunciones, entre otros.\n",
        "  \n",
        "  En sentido estricto, las cadenas de texto procedentes de un lenguaje son variables ordinales. Sin embargo, podr√≠amos establecer el texto como un tipo de variable por s√≠ misma, ya que tiene sus particularidades.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5n-SABJOu2Yo"
      },
      "id": "5n-SABJOu2Yo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "### Notaci√≥n:\n",
        "***\n",
        "\n",
        "Establecemos la siguiente notaci√≥n para lo que sigue.\n",
        "* De una variable $X$ tenemos $N$ datos (casos) que pueden ser repetidos o no: $ùëã_1,ùëã_2,‚Ä¶,ùëã_ùëÅ$ (en may√∫sculas).\n",
        "* Si solo indicamos los valores distintos entre s√≠: $ùë•_1,ùë•_2,‚Ä¶,ùë•_ùêæ$ (en min√∫sculas) cada uno con su respectiva frecuencia $ùëõ_1,ùëõ_2,‚Ä¶,ùëõ_ùêæ$.\n",
        "\n",
        "\n",
        "**Notas:**\n",
        "$ùëÅ=ùëõ_1+ùëõ_2+‚ãØ+ùëõ_ùêæ=‚àëùëõ_ùëñ$ representa el n√∫mero de casos (el tama√±o de la muestra).\n",
        "\n",
        "$ùêæ$ representa el n√∫mero de valores distintos entre s√≠ de la variable $ùëã$.\n",
        "\n",
        "***"
      ],
      "metadata": {
        "id": "9humrD6e8XFR"
      },
      "id": "9humrD6e8XFR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejemplo de conjunto de datos:\n",
        "\n",
        "Conjunto de datos de *Amazon Fine Food Reviews de¬†Kaggle*¬†(archivo disponible en [Reviews.csv](https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews)).\n",
        "\n",
        "Cuenta con un total de ***568454*** rese√±as de productos de ***Amazon***.\n",
        "\n",
        "En este proyecto, para aligerar la intensidad computacional, utilizaremos una *muestra* obtenida de forma aleatoria, que contiene ***45476*** rese√±as.\n",
        "\n",
        "Recurriremos a este conjunto datos reducido para ilustrar muchos de los ejemplos que realizaremos durante el curso.\n",
        "\n",
        "Las variables de este conjunto de datos son:\n",
        "\n",
        "* ProductId - Identificador del producto.\n",
        "* ProfileName - nombre del usuario.\n",
        "* HelpfulnessNumerator - fracci√≥n de usuarios a los que les result√≥ √∫til la rese√±a.\n",
        "* Score ‚Äì clasificaci√≥n del producto.\n",
        "* Time -  hora de la rese√±a.\n",
        "* Summary ‚Äì Breve resumen de la rese√±a.\n",
        "* Text ‚Äì Rese√±a completa del art√≠culo.\n",
        "\n"
      ],
      "metadata": {
        "id": "3kQUQRZOjSWE"
      },
      "id": "3kQUQRZOjSWE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aqu√≠ comenzamos con Python:\n"
      ],
      "metadata": {
        "id": "KUAyJzgFFpEc"
      },
      "id": "KUAyJzgFFpEc"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Librer√≠as necesarias**{ display-mode: \"form\" }\n",
        "\n",
        "!python -m spacy download es_core_news_sm\n",
        "!pip install swifter\n",
        "import swifter\n",
        "\n",
        "import os #Es una biblioteca est√°ndar que proporciona una interfaz para interactuar con el sistema operativo en el que se est√° ejecutando el programa Python. Permite realizar una variedad de operaciones relacionadas con el sistema operativo, como manipular archivos y directorios, obtener informaci√≥n sobre el entorno del sistema, realizar operaciones de gesti√≥n de procesos y manipular rutas de archivos de manera independiente del sistema operativo.\n",
        "import nltk #Natural Language Toolkit\n",
        "\n",
        "from nltk.util import ngrams\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stopword_en = nltk.corpus.stopwords.words('english')\n",
        "stopword_sp = nltk.corpus.stopwords.words('spanish')\n",
        "\n",
        "import networkx as nx #Para la creaci√≥n, manipulaci√≥n y estudio de la estructura, din√°mica y funciones de redes complejas.\n",
        "\n",
        "import spacy #Proporciona herramientas y recursos para realizar tareas relacionadas con el procesamiento del lenguaje natural, como tokenizaci√≥n, an√°lisis morfol√≥gico, an√°lisis sint√°ctico, reconocimiento de entidades nombradas y muchas otras.\n",
        "\n",
        "nlp_sp = spacy.load(\"es_core_news_sm\")\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "from scipy.stats import norm\n",
        "from spacy import displacy\n",
        "\n",
        "import wordcloud # Es una herramienta que permite crear visualizaciones de nubes de palabras a partir de un texto dado\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "import matplotlib.pyplot as plt #Es una librer√≠a que proporciona una amplia gama de herramientas para crear gr√°ficos est√°ticos, interactivos y animados. Es una de las bibliotecas m√°s utilizadas y populares para la visualizaci√≥n de datos en Python debido a su flexibilidad, potencia y facilidad de uso.\n",
        "import seaborn as sns #Es una biblioteca de visualizaci√≥n de datos para Python que se construye sobre Matplotlib y proporciona una interfaz de alto nivel para crear gr√°ficos estad√≠sticos atractivos y informativos. Seaborn est√° dise√±ado espec√≠ficamente para trabajar con estructuras de datos de Pandas y facilita la creaci√≥n de visualizaciones complejas con unas pocas l√≠neas de c√≥digo.\n",
        "import pandas as pd  #Es una herramienta de an√°lisis y manipulaci√≥n de datos\n",
        "import  math # Es una librer√≠a que proporciona funciones y constantes matem√°ticas predefinidas para realizar operaciones y c√°lculos matem√°ticos avanzados.\n",
        "import numpy as np # Es utilizada principalmente para realizar c√°lculos num√©ricos y manipular matrices y arreglos multidimensionales de manera eficiente.\n",
        "\n",
        "import random #proporciona funciones para generar n√∫meros aleatorios y realizar operaciones relacionadas con la aleatoriedad\n",
        "from statistics import median # proporciona funciones para realizar c√°lculos estad√≠sticos b√°sicos sobre conjuntos de datos num√©ricos\n",
        "import re #proporciona operaciones para trabajar con expresiones regulares, que son patrones utilizados para hacer coincidir secuencias de caracteres en texto. Las expresiones regulares son extremadamente √∫tiles para buscar, extraer y manipular texto de manera flexible y eficiente.\n",
        "import unicodedata #proporciona funciones para trabajar con caracteres Unicode. Unicode es un est√°ndar de codificaci√≥n de caracteres que asigna un n√∫mero √∫nico a cada car√°cter utilizado en la mayor√≠a de los sistemas de escritura del mundo, incluidos caracteres alfab√©ticos, num√©ricos, de puntuaci√≥n, s√≠mbolos y caracteres especiales.\n",
        "import collections #proporciona alternativas especializadas a los contenedores de datos incorporados de Python, como listas, diccionarios, conjuntos y tuplas.\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from scipy import stats\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "metadata": {
        "id": "_DzrAQ8kjhyN"
      },
      "id": "_DzrAQ8kjhyN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Clonar carpeta GitHub** (Para acceder al dataset) { form-width: \"5%\", display-mode: \"form\" }\n",
        "# Clona el repositorio\n",
        "try:\n",
        "    !git clone https://github.com/ednavivianasegura/Curso_PLN.git\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Cambiar al directorio \"Curso_PLN\"\n",
        "os.chdir(\"Curso_PLN\")"
      ],
      "metadata": {
        "id": "BUy1MgJqqtVA"
      },
      "id": "BUy1MgJqqtVA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad138970-7a94-490a-9099-765d22cfc5c1",
      "metadata": {
        "id": "ad138970-7a94-490a-9099-765d22cfc5c1",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Funciones predise√±adas necesarias durante el proceso\n",
        "#Estas funciones se explican posteriormente\n",
        "\n",
        "def group_by_intervals(df, column, num_bins):\n",
        "    bin_width = (df[column].max() - df[column].min()) / num_bins\n",
        "    bins, bin_edges = pd.cut(df[column], bins=num_bins, include_lowest=True, right=False, retbins=True)\n",
        "\n",
        "    # Generar etiquetas para los intervalos basados en los l√≠mites de los intervalos\n",
        "    labels = [f\"[{int(round(bin_edges[i],0))}-{int(round(bin_edges[i+1],0))})\" for i in range(len(bin_edges)-1)]\n",
        "    bins = pd.cut(df[column], bins=num_bins, include_lowest=True, right=False, retbins=True, labels=labels)[0]\n",
        "    return bins\n",
        "\n",
        "\n",
        "def marca_de_clase(intervalo):\n",
        "    limite_inferior, limite_superior = map(float, intervalo.strip(\"[]()\").split('-'))\n",
        "    return (limite_inferior + limite_superior) / 2\n",
        "\n",
        "\n",
        "def calcular_mediana(datos):\n",
        "    # Ordenar los datos\n",
        "    datos_ordenados = sorted(datos)\n",
        "    n = len(datos_ordenados)\n",
        "\n",
        "    # Calcular la mediana\n",
        "    if n % 2 == 1:\n",
        "        # Si la cantidad de datos es impar\n",
        "        mediana = datos_ordenados[n // 2]\n",
        "        print(\"n es impar\")\n",
        "    else:\n",
        "        # Si la cantidad de datos es par\n",
        "\n",
        "        indice_medio1 = n // 2 - 1\n",
        "        indice_medio2 = n // 2\n",
        "\n",
        "        print(f\"n es par, por lo tanto:\\nla posici√≥n {indice_medio1} es {datos_ordenados[indice_medio1]} y la posici√≥n {indice_medio2} es {datos_ordenados[indice_medio2]}\")\n",
        "        mediana = (datos_ordenados[indice_medio1] + datos_ordenados[indice_medio2]) / 2\n",
        "    return mediana\n",
        "\n",
        "def limpiar_texto(texto,idioma='spanish'):\n",
        "    # Convertir el texto a min√∫sculas\n",
        "    texto = texto.lower()\n",
        "\n",
        "    if idioma == \"spanish\":\n",
        "        texto = texto.replace('√°', 'a').replace('√©', 'e').replace('√≠', 'i').replace('√≥', 'o').replace('√∫', 'u').replace('√º', 'u')  # Quitar tildes\n",
        "        texto = texto.replace(',', '')  # Quitar comas\n",
        "        texto = texto.replace('.', '')  # Quitar puntos\n",
        "\n",
        "\n",
        "    if idioma == \"english\":\n",
        "        texto = re.sub(r\"won't\", \"will not\", texto)\n",
        "        texto = re.sub(r\"can\\'t\", \"can not\", texto)\n",
        "\n",
        "        # general\n",
        "        texto = re.sub(r\"n\\'t\", \" not\", texto)\n",
        "        texto = re.sub(r\"\\'re\", \" are\", texto)\n",
        "        texto = re.sub(r\"\\'s\", \" is\", texto)\n",
        "        texto = re.sub(r\"\\'d\", \" would\", texto)\n",
        "        texto = re.sub(r\"\\'ll\", \" will\", texto)\n",
        "        texto = re.sub(r\"\\'t\", \" not\", texto)\n",
        "        texto = re.sub(r\"\\'ve\", \" have\", texto)\n",
        "        texto = re.sub(r\"\\'m\", \" am\", texto)\n",
        "\n",
        "\n",
        "    # Eliminar caracteres especiales y n√∫meros\n",
        "    texto = re.sub(r'[^a-zA-Z\\s]', '', texto)\n",
        "    # Tokenizar el texto en palabras\n",
        "    palabras = word_tokenize(texto, language=idioma)\n",
        "    # Eliminar stopwords\n",
        "    if idioma == \"english\":\n",
        "        stopwords = set(stopword_en)\n",
        "    elif idioma == \"spanish\":\n",
        "        stopwords = set(stopword_sp)\n",
        "    palabras = [palabra for palabra in palabras if palabra not in stopwords]\n",
        "    # Eliminar acentos\n",
        "    palabras = [unicodedata.normalize('NFKD', palabra).encode('ASCII', 'ignore').decode('utf-8') for palabra in palabras]\n",
        "    # Unir las palabras nuevamente en un solo texto\n",
        "    texto_limpio = ' '.join(palabras)\n",
        "    return texto_limpio\n",
        "\n",
        "\n",
        "def remove_stopwords(x):\n",
        "\n",
        "    meaningful_words = []\n",
        "    my_list = x\n",
        "\n",
        "    tokenized_my_list = word_tokenize(my_list)\n",
        "    meaningful_words = [w for w in tokenized_my_list if not w in cachedStopWords]\n",
        "\n",
        "    return \" \".join(meaningful_words)\n",
        "\n",
        "\n",
        "\n",
        "def get_ngrams(text, n=2):\n",
        "    text = str(text)\n",
        "    n_grams = ngrams(text.split(), n)\n",
        "\n",
        "    returnVal = []\n",
        "\n",
        "    try:\n",
        "        for grams in n_grams:\n",
        "            returnVal.append('_'.join(grams))\n",
        "    except(RuntimeError):\n",
        "        pass\n",
        "\n",
        "    return ' '.join(returnVal).strip()\n",
        "\n",
        "# Define a function to plot word cloud\n",
        "def plot_cloud(wordcloud):\n",
        "    fig = plt.figure(figsize=(25, 17), dpi=80)\n",
        "    plt.tight_layout(pad=0)\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.axis(\"off\")\n",
        "    plt.box(False)\n",
        "    plt.show()\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1aac4de-49c6-41cc-9750-03b7d316ca0f",
      "metadata": {
        "id": "b1aac4de-49c6-41cc-9750-03b7d316ca0f"
      },
      "outputs": [],
      "source": [
        "# @title **Importar** conjunto de datos\n",
        "df = pd.read_csv('Reviews.csv')\n",
        "#muestra: N\n",
        "N=df.shape[0]\n",
        "print(f\"Informaci√≥n disponible en el dataframe:\\n{list(df.columns)}\\nEl dataframe contiene {N} rese√±as\\n\")\n",
        "display(df.loc[:,['Id', 'ProductId', 'ProfileName', 'Score', 'Time', 'Summary', 'Text','HelpfulnessNumerator', 'HelpfulnessDenominator']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para complementar la informaci√≥n proporcionada por el conjunto de datos, se crean dos variables adicionales, una llamada *Longitud_Texto* que cuenta el n√∫mero de palabras en cada rese√±a y otra, llamada *Num_Palabras_Unicas* que cuenta el n√∫mero de palabras √∫nicas (es decir sin que se repitan) en cada rese√±a."
      ],
      "metadata": {
        "id": "L0jmnPxuFLXy"
      },
      "id": "L0jmnPxuFLXy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fca7310-2e89-4401-af5e-c3567e11b5d4",
      "metadata": {
        "id": "5fca7310-2e89-4401-af5e-c3567e11b5d4",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Crear dos variables nuevas\n",
        "# Creamos una nueva variable que contiene el n√∫mero de palabras que componene\n",
        "# Contar la longitud de las palabras en la columna 'texto'\n",
        "df['Longitud_Texto'] = df['Text'].apply(lambda x: len(x.split()))\n",
        "# Calcula el n√∫mero de palabras √∫nicas en la columna 'Text'\n",
        "df['Num_Palabras_Unicas'] = df['Text'].apply(lambda x: len(set(x.split())))\n",
        "\n",
        "display(df.loc[:,['Id','Score', 'Summary', 'Text','Longitud_Texto','Num_Palabras_Unicas']].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4422bf7e-3ff6-4841-aafd-8cb875bd3403",
      "metadata": {
        "id": "4422bf7e-3ff6-4841-aafd-8cb875bd3403"
      },
      "source": [
        "***\n",
        "## TABLA DE FRECUENCIAS\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tambi√©n denominadas tabulaciones, resumen los datos de una variable.\n",
        "\n",
        "Para cada valor observado $ùë•_{ùëñ}$, en la tabla se indica:\n",
        "* La frecuencia absoluta $ùëõ_{ùëñ}$ es el n√∫mero de veces que se da el valor $ùë•_{ùëñ}$.\n",
        "* La frecuencia relativa $ùëì_{ùëñ}=\\frac{ùëõ_{ùëñ}}{ùëÅ}$ donde $ùëÅ$ es el n√∫mero de casos. Notas:\n",
        "    * $0 < f_{ùëñ} \\leq1$, representa la proporci√≥n de $ùë•_{ùëñ}$;\n",
        "    * se suele expresar en porcentaje: $ùëì_{ùëñ}\\times100$%.\n",
        "\n",
        "Si la variable es num√©rica (tambi√©n llamada cuantitativa) o si es ordinal (esto es, sus valores se pueden ordenar), ordenamos los valores observados de manera creciente $ùë•_{1}<ùë•_{2}<‚ãØ$ y tambi√©n indicamos:\n",
        "* La frecuencia absoluta acumulada $ùëÅ_{ùëñ}=ùëõ_{1}+ùëõ_{2}+‚ãØ+ùëõ_{ùëñ}$ es el n√∫mero de casos menores o iguales que $ùë•_{ùëñ}$.\n",
        "* La frecuencia relativa acumulada $ùêπ_{ùëñ}=ùëì_{1}+ùëì_{2}+‚ãØ+ùëì_{ùëñ}$ . Notas:\n",
        "    * $0 < F_{ùëñ} \\leq1$, representa la proporci√≥n de valores menores o iguales que $ùë•_{ùëñ}$;\n",
        "    * tambi√©n se suele expresar en tanto por ciento: $ùêπ_{ùëñ}\\times100$%.\n",
        "\n",
        "***\n",
        "**Ejemplo base**:\n",
        "***\n",
        "\n",
        "Crear paso a paso la tabla de frecuencias si tenemos un conjunto de edades de 10 personas:\n",
        "\n",
        "\\[ 25, 30, 35, 40, 45, 25, 35, 30, 35, 40 \\]\n",
        "\n",
        "***"
      ],
      "metadata": {
        "id": "sBER9zdtGDCE"
      },
      "id": "sBER9zdtGDCE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04cab4e4-a39f-4371-bd52-27e664180b0c",
      "metadata": {
        "id": "04cab4e4-a39f-4371-bd52-27e664180b0c",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title **Ejercicio:** Crear la tabla de frecuencias de la variable *longitud de palabras*:\n",
        "\n",
        "#Creamos la tabla de frecuencias de la variable \"longitud de palabras\":\n",
        "\n",
        "frecuencias_long = df['Longitud_Texto'].value_counts()\n",
        "frecuencias_long = frecuencias_long.sort_index()\n",
        "\n",
        "# Calcular la frecuencia relativa\n",
        "frecuencias_relativas = frecuencias_long / len(df)\n",
        "\n",
        "# Calcular las frecuencias acumuladas\n",
        "frecuencias_acumuladas = frecuencias_long.cumsum()\n",
        "\n",
        "# Calcular la frecuencia relativa acumulada\n",
        "frecuencias_relativas_acum = frecuencias_acumuladas / len(df)\n",
        "\n",
        "# Crear DataFrame con las frecuencias\n",
        "tabla_frecuencias = pd.DataFrame({\n",
        "    'x_i': frecuencias_long.index,\n",
        "    'n_i': frecuencias_long.values,\n",
        "    'f_i': frecuencias_relativas.values,\n",
        "    'N_i':frecuencias_acumuladas.values,\n",
        "    'F_i':frecuencias_relativas_acum.values\n",
        "})\n",
        "print(\"Tabla de frecuencias de la columna\\n'Longitud del texto\\n(n√∫mero de palabras dentro de cada rese√±a)':\\n\")\n",
        "display(tabla_frecuencias)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d95c96a-e15b-4544-98a0-b6f5acb53300",
      "metadata": {
        "id": "3d95c96a-e15b-4544-98a0-b6f5acb53300"
      },
      "source": [
        "### TABLA DE FRECUECIAS POR INTERVALOS"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si una variable cuenta con una gran cantidad de valores distintos, como es el caso de la variable *Longitud del texto*, lo habitual es resumirla en una una tabla de frecuencias reducida por intervalos. En este caso la frecuencia, $ùëõ_ùëñ$, es el n√∫mero de casos cuyos valores se encuentran en el ùëñ-√©simo intervalo.\n",
        "\n",
        "***Intervalos***:\n",
        "Cada intervalo est√° determinado por sus l√≠mites. Esto es, el intervalo $ùëé‚ü∑ùëè$ representa los valores comprendidos entre $ùëé$ y $ùëè$.\n",
        "\n",
        "\n",
        "Si se requiere determinar la ubicaci√≥n de los l√≠mites, estos pueden ser referidos con una notaci√≥n m√°s completa. Generalmente se utilizan intervalos abiertos por la izquierda y cerrados por la derecha: $(ùëé,ùëè]$. Esto es, incluye los valores comprendidos entre $ùëé$ y $ùëè$, $ùëè$ incluido y $ùëé$ excluido.\n",
        "\n",
        "La amplitud de intervalo $ùëé‚ü∑ùëè$ es $ùëè‚àíùëé$, que tambi√©n es la amplitud de los intervalos $(ùëé,ùëè]$ y $[ùëé,ùëè)$ (este √∫ltimo intervalo representa los valores mayores o iguales que $ùëé$ y menores que $ùëè$).\n",
        "\n",
        "En las tablas de frecuencias los intervalos pueden ser de amplitud constante (m√°s c√≥modo) o variable.\n",
        "\n",
        "La densidad de frecuencia de un intervalo es la raz√≥n de su frecuencia absoluta sobre su amplitud. El intervalo $ùëñ$ con frecuencia absoluta $ùëõ_ùëñ$ y amplitud $ùëé_ùëñ$ tiene una densidad de frecuencia $‚Ñé_ùëñ=ùëõ_ùëñ/ùëé_ùëñ$.\n",
        "\n",
        "\n",
        "En las variables cuantitativas la **marca de clase** de un intervalo es el valor concreto de la variable que representa cada intervalo, generalmente se toma el punto medio del intervalo. El punto medio de $(ùëé,ùëè]$ es: $(a+b)/2$. Para calcular medidas cuantitativas de una variable reducida por intervalos se usan las marcas de clase.\n",
        "\n",
        "***\n",
        "**Ejemplo base**:\n",
        "***\n",
        "\n",
        "Crear la tabla de frecuencias por intervalo si tenemos un conjunto de edades de 17 personas:\n",
        "\n",
        "\\[51,58,40,45,49,20,22,25,28,28, 30,30,30,38,34,36,39\\]\n",
        "\n",
        "***\n"
      ],
      "metadata": {
        "id": "SxCYYKEhD2Bi"
      },
      "id": "SxCYYKEhD2Bi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "479fb3e4-426d-4b3d-b09d-5b96846ee54f",
      "metadata": {
        "id": "479fb3e4-426d-4b3d-b09d-5b96846ee54f",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title **Ejercicio**: Resumimos los valores de la variable *Longitud del texto* por los intervalos indicados.\n",
        "\n",
        "print(\"--------------------------------------------------------------------\\nTras hacer el recuento obtenemos la siguiente tabla.\\n--------------------------------------------------------------------\\n\")\n",
        "# Calcular los grupos usando la funci√≥n creada group_by_sturges\n",
        "df['intervalo'] = group_by_intervals(df, 'Longitud_Texto',10)\n",
        "\n",
        "# Calcular la tabla de frecuencias de la columna 'Grupo'\n",
        "frecuencias_grupo = df['intervalo'].value_counts()\n",
        "frecuencias_grupo = frecuencias_grupo.sort_index()\n",
        "\n",
        "# Calcular la marca de clase para cada intervalo\n",
        "MarcaDeClase = list(map(marca_de_clase, list(frecuencias_grupo.index)))\n",
        "\n",
        "# Crear DataFrame con las frecuencias\n",
        "tabla_frecuencias_Longitud_Texto = pd.DataFrame({\n",
        "    'Intervalo': frecuencias_grupo.index,\n",
        "    'n_i': frecuencias_grupo.values,\n",
        "    'x_i':MarcaDeClase})\n",
        "\n",
        "print(tabla_frecuencias_Longitud_Texto)\n",
        "# print(frecuencias_grupo.index)\n",
        "# print(MarcaDeClase)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos ver que hay varios intervalos con muy poca densidad de frecuencia, por lo que podemos unir los intervalos consecutivos en uno solo:"
      ],
      "metadata": {
        "id": "kIgXhqc4GODG"
      },
      "id": "kIgXhqc4GODG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4563615-7c7d-4c9d-88a1-b386dd467417",
      "metadata": {
        "id": "a4563615-7c7d-4c9d-88a1-b386dd467417",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Unir los grupos contiguos con poca frecuencia en uno solo:\n",
        "# agrupar los √∫ltimos grupos en uno solo:\n",
        "\n",
        "lista=[ '[954-1143)', '[1143-1332)', '[1332-1522)',\n",
        "                  '[1522-1712)', '[1712-1903)']\n",
        "\n",
        "# Reemplazar los valores en la columna 'columna_original' con \"954-1903\" si coinciden con los valores de la lista\n",
        "df['intervalo'] = df['intervalo'].replace(lista, '[954-1903)')\n",
        "\n",
        "# Calcular la tabla de frecuencias de la columna 'Grupo'\n",
        "frecuencias_grupo = df['intervalo'].value_counts()\n",
        "frecuencias_grupo = frecuencias_grupo.sort_index()\n",
        "\n",
        "# Calcular la frecuencia relativa\n",
        "frecuencias_relativas_gr = frecuencias_grupo / len(df)\n",
        "\n",
        "# Calcular las frecuencias acumuladas\n",
        "frecuencias_acumuladas_gr = frecuencias_grupo.cumsum()\n",
        "\n",
        "# Calcular la frecuencia relativa acumulada\n",
        "frecuencias_relativas_acum_gr = frecuencias_acumuladas_gr / len(df)\n",
        "\n",
        "# Calcular la marca de clase para cada intervalo\n",
        "MarcaDeClase = list(map(marca_de_clase, list(frecuencias_grupo.index)))\n",
        "\n",
        "# Crear DataFrame con las frecuencias\n",
        "tabla_frecuencias_gr = pd.DataFrame({\n",
        "    'Intervalo': frecuencias_grupo.index,\n",
        "    'n_i': frecuencias_grupo.values,\n",
        "    'f_i': frecuencias_relativas_gr.values,\n",
        "    'N_i':frecuencias_acumuladas_gr.values,\n",
        "    'F_i':frecuencias_relativas_acum_gr.values,\n",
        "    'x_i':MarcaDeClase\n",
        "})\n",
        "\n",
        "\n",
        "print(\"Tabla de frecuencias agrupadas por intervalo de la columna\\n'Longitud del texto':\\n\")\n",
        "display(tabla_frecuencias_gr)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Ejemplo 2: Tabla de frecuencia del n√∫mero de palabras √∫nicas (es decir el conteo de palabras sin repetici√≥n) { display-mode: \"form\" }\n",
        "#Creamos la tabla de frecuencias de la variable \"Num_Palabras_Unicas\":\n",
        "\n",
        "frecuencias_long_unic = df['Num_Palabras_Unicas'].value_counts()\n",
        "frecuencias_long_unic = frecuencias_long_unic.sort_index()\n",
        "\n",
        "# Calcular la frecuencia relativa\n",
        "frecuencias_relativas_unic = frecuencias_long_unic / len(df)\n",
        "\n",
        "# Calcular las frecuencias acumuladas\n",
        "frecuencias_acumuladas_unic = frecuencias_long_unic.cumsum()\n",
        "\n",
        "# Calcular la frecuencia relativa acumulada\n",
        "frecuencias_relativas_acum_unic = frecuencias_relativas_unic / len(df)\n",
        "\n",
        "# Crear DataFrame con las frecuencias\n",
        "tabla_frecuencias_unic = pd.DataFrame({\n",
        "    'x_i': frecuencias_long_unic.index,\n",
        "    'n_i': frecuencias_long_unic.values,\n",
        "    'f_i': frecuencias_relativas_unic.values,\n",
        "    'N_i':frecuencias_acumuladas_unic.values,\n",
        "    'F_i':frecuencias_relativas_acum_unic.values\n",
        "})\n",
        "\n",
        "# Calcular los grupos usando la funci√≥n creada group_by_sturges\n",
        "df['intervalo'] = group_by_intervals(df, 'Num_Palabras_Unicas',10)\n",
        "\n",
        "# Calcular la tabla de frecuencias de la columna 'Grupo'\n",
        "frecuencias_grupo_unic = df['intervalo'].value_counts()\n",
        "frecuencias_grupo_unic = frecuencias_grupo_unic.sort_index()\n",
        "\n",
        "# Calcular la marca de clase para cada intervalo\n",
        "MarcaDeClase_unic = list(map(marca_de_clase, list(frecuencias_grupo_unic.index)))\n",
        "\n",
        "# Crear DataFrame con las frecuencias\n",
        "tabla_frecuencias_Num_Palabras_Unicas = pd.DataFrame({\n",
        "    'Intervalo': frecuencias_grupo_unic.index,\n",
        "    'n_i': frecuencias_grupo_unic.values,\n",
        "    'x_i':MarcaDeClase_unic})\n",
        "\n",
        "print(tabla_frecuencias_Num_Palabras_Unicas)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_vNsSqA88eih"
      },
      "id": "_vNsSqA88eih",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13412d14-89bc-4f2c-a7fd-ce2620e3492f",
      "metadata": {
        "id": "13412d14-89bc-4f2c-a7fd-ce2620e3492f",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title **Ejercicio:** Resumimos los valores de la variable *Score*, realizando los conteos necesarios\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------\\nTras hacer el recuento obtenemos la siguiente tabla.\\n--------------------------------------------------------------------------------\")\n",
        "# Calcular la tabla de frecuencias de la columna 'Score'\n",
        "frecuencias_score = df['Score'].value_counts()\n",
        "frecuencias_score = frecuencias_score.sort_index()\n",
        "\n",
        "# Calcular la frecuencia relativa\n",
        "frecuencias_relativas = frecuencias_score / len(df)\n",
        "\n",
        "# Calcular las frecuencias acumuladas\n",
        "frecuencias_acumuladas = frecuencias_score.cumsum()\n",
        "\n",
        "# Calcular la frecuencia relativa acumulada\n",
        "frecuencias_relativas_acum = frecuencias_acumuladas / len(df)\n",
        "\n",
        "\n",
        "# Crear DataFrame con las frecuencias\n",
        "tabla_frecuencias = pd.DataFrame({\n",
        "    'x_i': frecuencias_score.index,\n",
        "    'n_i': frecuencias_score.values,\n",
        "    'f_i': frecuencias_relativas.values,\n",
        "    \"N_i\":frecuencias_acumuladas.values,\n",
        "    \"F_i\":frecuencias_relativas_acum.values\n",
        "})\n",
        "display(tabla_frecuencias)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observa la tabla de frecuencias de la valoraci√≥n (score) de las rese√±as de Amazon y contesta a las siguientes preguntas:\n",
        "\n",
        "\n",
        "1.   ¬øCu√°ntas rese√±as fueron valoradas con 4?\n",
        "2.   ¬øCu√°ntas rese√±as fueron valoradas con menos de 3?\n",
        "3.   ¬øCu√°ntas rese√±as tienen 4 o menos de valoraci√≥n?\n",
        "4.   ¬øCu√°ntas rese√±as tienen m√°s de 4 puntos de valoraci√≥n?\n",
        "5.   ¬øCu√°l es la proporci√≥n de rese√±as con puntaje igual a 3?\n",
        "6.   ¬øCu√°l es la proporci√≥n de rese√±as menores a 3?\n",
        "7.   ¬øCu√°l es la proporci√≥n de rese√±as mayores de 2?\n",
        "\n"
      ],
      "metadata": {
        "id": "KjHoqW30G9OT"
      },
      "id": "KjHoqW30G9OT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n"
      ],
      "metadata": {
        "id": "EznjlYdeJGyY"
      },
      "id": "EznjlYdeJGyY"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Respuesta:\n",
        "print(f\"1. ¬øCu√°ntas rese√±as fueron valoradas con 4?: {tabla_frecuencias[tabla_frecuencias['x_i']==4]['n_i'].values[0]}\")\n",
        "print(f\"2. ¬øCu√°ntas rese√±as fueron valoradas con menos de 3?: {tabla_frecuencias[tabla_frecuencias['x_i']<3]['n_i'].values.sum()}\")\n",
        "print(f\"3. ¬øCu√°ntas rese√±as tienen 4 o menos de valoraci√≥n?: {tabla_frecuencias[tabla_frecuencias['x_i']<5]['n_i'].values.sum()}\")\n",
        "print(f\"4. ¬øCu√°ntas rese√±as tienen m√°s de 4 puntos de valoraci√≥n?: {tabla_frecuencias[tabla_frecuencias['x_i']>4]['n_i'].values.sum()}\")\n",
        "print(f\"5. ¬øCu√°l es la proporci√≥n de rese√±as con puntaje igual a 3?: {round(tabla_frecuencias[tabla_frecuencias['x_i']==3]['f_i'].values.sum(),4)} √≥ {round(tabla_frecuencias[tabla_frecuencias['x_i']==3]['f_i'].values.sum()*100,4)}%\")\n",
        "print(f\"6. ¬øCu√°l es la proporci√≥n de rese√±as menores a 3?: {round(tabla_frecuencias[tabla_frecuencias['x_i']==2]['F_i'].values[0],4)} √≥ {round(tabla_frecuencias[tabla_frecuencias['x_i']==2]['F_i'].values[0]*100,4)}%\")\n",
        "print(f\"7. ¬øCu√°l es la proporci√≥n de rese√±as mayores de 2?: {round(tabla_frecuencias[tabla_frecuencias['x_i']>2]['f_i'].values.sum(),3)} √≥ {round(tabla_frecuencias[tabla_frecuencias['x_i']>2]['f_i'].values.sum()*100,3)}%\")"
      ],
      "metadata": {
        "id": "1BvUhPcuMGnr",
        "cellView": "form"
      },
      "id": "1BvUhPcuMGnr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "416a82f8-198c-4cca-9f8a-5b80c708fdec",
      "metadata": {
        "id": "416a82f8-198c-4cca-9f8a-5b80c708fdec"
      },
      "source": [
        "***\n",
        "***\n",
        "\n",
        "### ALGUNOS TIPOS DE GR√ÅFICOS"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para variables **discretas** (toman pocos valores distintos):\n",
        "\n",
        "* Gr√°ficos de sectores: cada valor ocupa un sector circular cuya proporci√≥n de √°rea (con respecto al c√≠rculo que representa el total) es la frecuencia relativa.  \n",
        "* Gr√°ficos de barras. De dos tipos:\n",
        "  * de valores: una barra por cada caso ($ùëÅ$ barras), de manera que la altura de barra ùëñ-√©sima expresa el valor $X_{i}$;\n",
        "  * de frecuencias: una barra por cada valor distinto $ùë•_ùëñ$, de manera que la altura de la barra $ùë•_ùëñ$ expresa su frecuencia absoluta $ùëõ_ùëñ$.\n",
        "\n",
        "Para variables **continuas** (toman un amplio rango de posibles valores):\n",
        "* Histogramas.\n",
        "Son gr√°ficos de rect√°ngulos pegados cuyas bases representan los intervalos determinados y cuyas alturas representan sus densidades ($‚Ñé_ùëñ/ùëÅ$).\n",
        "\n",
        "    Se puede considerar que son gr√°ficos de barras de frecuencias especiales.\n",
        "\n",
        "Gr√°ficos para PLN:\n",
        "   * Pol√≠gonos de frecuencias (Secuencia de las frecuencias en orden descendente unidas por una l√≠nea).\n",
        "   * Nubes de palabras (word clouds).\n",
        "   * √Årbol de an√°lisis sint√°ctico.\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "id": "d-HCmB6WNIZQ"
      },
      "id": "d-HCmB6WNIZQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58400d04-b18f-4703-aad4-3b4ab3f3f40d",
      "metadata": {
        "id": "58400d04-b18f-4703-aad4-3b4ab3f3f40d",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Gr√°fico de sectores:\n",
        "sizes = tabla_frecuencias.n_i\n",
        "labels = tabla_frecuencias.x_i\n",
        "\n",
        "plt.figure(figsize=(6, 6))  # Tama√±o del gr√°fico (opcional)\n",
        "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)\n",
        "\n",
        "# Agregar un t√≠tulo\n",
        "plt.title('Gr√°fico de frecuencia del Score')\n",
        "# plt.savefig(\"Pie_chart_score.png\", bbox_inches='tight', pad_inches=0, dpi=600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe43711d-93d7-428d-9a83-576d1054db13",
      "metadata": {
        "id": "fe43711d-93d7-428d-9a83-576d1054db13",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title **Gr√°fico de barras**: Para visualizar un ejemplo, creamos una nueva variable ***Sentiment*** que divide en tres grupos las rese√±as. Las rese√±as con Score de 1, 2 son catalogadas como *Malo*, Score de 3 como *Neutral* y Score de 4 o 5 son catalogadas como *Buenas*.\n",
        "\n",
        "#Primero clasificaremos las rese√±as en doos grupos (buenas o malas)\n",
        "# assign reviews with score > 3 as positive sentiment\n",
        "# score < 3 negative sentiment\n",
        "# score = 3 Neutral\n",
        "df['sentiment'] = pd.cut(df['Score'], bins=[1,3,4,6], labels=['Malo (1-2)', 'Neutral (3)', 'Bueno (4-5)'], right=False)\n",
        "\n",
        "frecuencias_sentiment = df['sentiment'].value_counts()\n",
        "frecuencias_sentiment = frecuencias_sentiment.sort_index()\n",
        "\n",
        "print(f\"Frecuencias por tipo de sentimiento:\\n{frecuencias_sentiment}\")\n",
        "\n",
        "# Datos de ejemplo\n",
        "labels = frecuencias_sentiment.index\n",
        "sizes = frecuencias_sentiment.values\n",
        "\n",
        "# Crear el gr√°fico de barras\n",
        "plt.figure(figsize=(8, 6))  # Tama√±o del gr√°fico (opcional)\n",
        "plt.bar(labels, sizes, color='magenta')\n",
        "\n",
        "# Agregar etiquetas y t√≠tulo\n",
        "plt.ylabel('N¬∫ de rese√±as')\n",
        "plt.xlabel('Sentimiento')\n",
        "# plt.title('Gr√°fico de sentimeinto')\n",
        "# plt.savefig(\"bar_chart_score_agrupado.png\", bbox_inches='tight', pad_inches=0, dpi=600)\n",
        "\n",
        "# Mostrar el gr√°fico\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b23b170-fb3e-45bb-993d-baad62311f0f",
      "metadata": {
        "id": "3b23b170-fb3e-45bb-993d-baad62311f0f",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title **Histograma:** Histograma del puntaje de las rese√±as (Score):\n",
        "\n",
        "data=list(df['Score'].values)\n",
        "# Crear el histograma con las barras juntas y densidad\n",
        "plt.figure(figsize=(8, 6))  # Tama√±o del gr√°fico (opcional)\n",
        "values, bins, _ = plt.hist(data, bins=range(min(data), max(data) + 2), align='left', rwidth=1, color='magenta', density=False)\n",
        "\n",
        "# Agregar etiquetas y t√≠tulo\n",
        "plt.xlabel('Score')\n",
        "plt.ylabel('Densidad')\n",
        "plt.title('Histograma del Score')\n",
        "\n",
        "# Mostrar el histograma\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()\n",
        "\n",
        "#para mostrar las frecuencias reltativas en vez de las aboslutas usar: values, bins, _ = plt.hist(data, bins=range(min(data), max(data) + 2), align='left', rwidth=1, color='magenta', density=False)\n",
        "\n",
        "for i in range(len(values)):\n",
        "    print(f\"Frecuencia del bin {bins[i]} - {bins[i+1]}: {values[i]}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **PLN:** - Pol√≠gono de frecuencias  - Nubes de palabras - √Årbol de an√°lisis sint√°ntico.\n",
        "\n",
        "text = \"\"\"\n",
        "Natural Language Processing (NLP) is an interdisciplinary field that combines statistics, mathematics,\n",
        "and machine learning techniques to analyze and understand human language. In NLP, words, tokens,\n",
        "and their frequency play a crucial role in building models for language inference.\n",
        "The foundation of Natural Language Processing lies in understanding the nominal, ordinal,\n",
        "and categorical aspects of language. By employing sophisticated algorithms, NLP systems can decipher\n",
        "the semantics and syntax of words and sentences. In NLP, the power of machine learning models\n",
        "is harnessed to process vast amounts of textual data, enabling tasks such as sentiment analysis,\n",
        "text summarization, and information extraction. Through the application of advanced algorithms,\n",
        "NLP practitioners strive to enhance the accuracy and efficiency of language processing systems.\n",
        "Statistics and mathematics form the backbone of Natural Language Processing, providing the theoretical\n",
        "framework for modeling linguistic phenomena and deriving meaningful insights from textual data.\n",
        "By leveraging statistical techniques, NLP algorithms can identify patterns, trends, and correlations\n",
        "within language corpora. In essence, Natural Language Processing is a multifaceted discipline\n",
        "that draws upon diverse domains such as linguistics, computer science, and artificial intelligence.\n",
        "By exploring the intricate interplay between words, semantics, and context, NLP researchers\n",
        "continue to push the boundaries of language understanding and machine intelligence.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenizar el texto\n",
        "\n",
        "clear_text = limpiar_texto(text,idioma='english')\n",
        "\n",
        "tokens = word_tokenize(clear_text)\n",
        "\n",
        "# Eliminar palabras vac√≠as (stopwords)\n",
        "\n",
        "tokens = [token.lower() for token in tokens if token.isalnum() and token.lower() not in stopword_en]\n",
        "\n",
        "# Calcular la distribuci√≥n de frecuencias\n",
        "freq_dist = FreqDist(tokens)\n",
        "\n",
        "# display(freq_dist)\n",
        "\n",
        "print(\"------------------------------------------------------------\\n Pol√≠gono de frecuencias\\n------------------------------------------------------------\\n\")\n",
        "\n",
        "# Histograma de las palabras m√°s comunes\n",
        "freq_dist.plot(30, cumulative=False)\n",
        "\n",
        "print(\"------------------------------------------------------------\\n Nube de palabras\\n------------------------------------------------------------\\n\")\n",
        "\n",
        "# Crear la primera nube de palabras\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(tokens))\n",
        "\n",
        "# Crear la segunda nube de palabras con una m√°scara\n",
        "x, y = np.ogrid[:300, :300]\n",
        "mask = (x - 150) ** 2 + (y - 150) ** 2 > 130 ** 2\n",
        "mask = 255 * mask.astype(int)\n",
        "wc = WordCloud(background_color=\"white\", repeat=True, mask=mask)\n",
        "wc.generate(' '.join(tokens))\n",
        "\n",
        "# Crear el gr√°fico con subgr√°ficos\n",
        "fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Subgr√°fico 1: Primera nube de palabras\n",
        "axs[0].imshow(wordcloud, interpolation='bilinear')\n",
        "axs[0].axis('off')\n",
        "axs[0].set_title('Nube de Palabras')\n",
        "\n",
        "# Subgr√°fico 2: Segunda nube de palabras con m√°scara\n",
        "axs[1].imshow(wc, interpolation=\"bilinear\")\n",
        "axs[1].axis(\"off\")\n",
        "axs[1].set_title('Nube de Palabras con M√°scara')\n",
        "\n",
        "# Ajustar el dise√±o para evitar solapamientos\n",
        "plt.tight_layout()\n",
        "\n",
        "# Mostrar el gr√°fico combinado\n",
        "plt.show()\n",
        "\n",
        "\n",
        "######################\n",
        "\n",
        "print(\"------------------------------------------------------------\\n √Årbol Sint√°ntico\\n------------------------------------------------------------\\n\")\n",
        "\n",
        "sent_ej=\"Apple is looking at buying U.K. startup for $1 billion\"\n",
        "doc = nlp_en(sent_ej)\n",
        "\n",
        "print(f\"An√°lisis sint√°ctico de la frase:\\n'{sent_ej}'\\nTomada de [p√°gina oficial de spacy](https://spacy.io/usage/linguistic-features)\\n\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.dep_, token.head.text)\n",
        "\n",
        "\n",
        "# Visualizar el √°rbol de an√°lisis sint√°ctico\n",
        "displacy.render(doc, style=\"dep\", page= False)"
      ],
      "metadata": {
        "id": "p1PcXlFjSd6S",
        "cellView": "form"
      },
      "id": "p1PcXlFjSd6S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un **√°rbol de an√°lisis sint√°ctico** en PLN proporciona informaci√≥n sobre la estructura gramatical de una oraci√≥n.\n",
        "\n",
        "Al analizar un √°rbol sint√°ctico, se puede obtener:\n",
        "\n",
        "1. Relaciones gramaticales: Permite identificar las relaciones entre las palabras de una oraci√≥n, como sujeto, verbo, objeto, complementos, etc.\n",
        "\n",
        "2. Desambiguaci√≥n: Ayuda a eliminar ambiguedades en frases complejas al mostrar claramente la estructura y las relaciones entre las palabras.\n",
        "\n",
        "3. An√°lisis de dependencias: Permite comprender la relaci√≥n entre las palabras y c√≥mo interact√∫an para formar frases significativas\n",
        "\n",
        "4. Extracci√≥n de informaci√≥n: Facilita la extracci√≥n de informaci√≥n relevante para tareas como res√∫menes de texto, an√°lisis de sentimientos y traducci√≥n autom√°tica\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/ednavivianasegura/Curso_PLN/blob/main/abreviaciones.png?raw=1\" alt=\"abrevaciones\" width=\"70%\" height=\"70%\">  \n",
        "</center>\n",
        "\n",
        "Imagen tomada de [p√°gina oficial de spacy](https://spacy.io/usage/linguistic-features)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s44b5_tQv6FE"
      },
      "id": "s44b5_tQv6FE"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Prueba b√°sica en espa√±ol:\n",
        "\n",
        "phrase = \"Estoy aprendiendo PLN en Python y quisiera probar c√≥mo funciona un √°rbol sint√°ctico en Espa√±ol\"\n",
        "doc = nlp_sp(phrase)\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.dep_, token.head.text)\n",
        "\n",
        "\n",
        "# Visualizar el √°rbol de an√°lisis sint√°ctico\n",
        "displacy.render(doc, style=\"dep\", page= False)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "7vmxsK964otq"
      },
      "id": "7vmxsK964otq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "615df32b-c6ff-4201-a8a9-dc76376069c7",
      "metadata": {
        "id": "615df32b-c6ff-4201-a8a9-dc76376069c7"
      },
      "source": [
        "***\n",
        "## Medidas una variable **cuantitativa**\n",
        "***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para hacer manejable la masa de datos procedentes de la observaci√≥n estad√≠stica es necesario resumir el volumen de los datos. Ya hemos visto c√≥mo reducir y organizar la masa de datos con las tablas de frecuencias.\n",
        "\n",
        "En el caso de las **variables num√©ricas** es posible reducir a√∫n m√°s esta informaci√≥n, vali√©ndonos de unos pocos valores que las describan y caractericen. Estos valores, que llamamos estad√≠sticos, nos indican las caracter√≠sticas m√°s importantes de las distribuciones de frecuencias y se suelen clasificar en los siguientes grupos:\n",
        "\n",
        "* de posici√≥n:\n",
        "    * media aritm√©tica\n",
        "    * mediana\n",
        "    * cuantiles\n",
        "    * moda\n",
        "* de dispersi√≥n:\n",
        "    * varianza\n",
        "    * desviaci√≥n t√≠pica\n"
      ],
      "metadata": {
        "id": "YliNLDWXPIY4"
      },
      "id": "YliNLDWXPIY4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Medidas de posici√≥n:\n",
        "\n",
        "Las **medidas de posici√≥n** resumen la distribuci√≥n de los valores de una variable a trav√©s de unos puntos de referencia en el rango de la variable.\n",
        "\n",
        "Para que un valor pueda ser considerado una medida de posici√≥n tiene que tomar un valor **comprendido entre el m√≠nimo y el m√°ximo de la variable.**\n",
        "\n",
        "Existen dos tipos de medidas de posici√≥n: las **centrales** y las **no centrales**.\n",
        "\n",
        "De las medidas de posici√≥n central, las m√°s utilizadas son: la **media aritm√©tica**, la **mediana** y la **moda**.\n",
        "\n",
        "Los **cuantiles** son las medidas de posici√≥n no central.\n"
      ],
      "metadata": {
        "id": "epOYDGEGPprx"
      },
      "id": "epOYDGEGPprx"
    },
    {
      "cell_type": "markdown",
      "id": "e739def1-b32a-40d8-b998-6666c56cc3e7",
      "metadata": {
        "id": "e739def1-b32a-40d8-b998-6666c56cc3e7"
      },
      "source": [
        "***\n",
        "### Media Aritm√©tica\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La media aritm√©tica es la suma de todos los valores observados de la distribuci√≥n, dividida por el n√∫mero total de casos.\n",
        "$$\\bar{X}=\\frac{1}{N} \\sum_{(ùëñ=1)}^{ùëÅ}ùëã_{ùëñ} =\\frac{ùëã_1+ùëã_2+‚Ä¶+ùëã_ùëÅ}{N}.$$\n",
        "\n",
        "Si tenemos $ùêæ$ valores distintos que se repiten, y conocemos sus frecuencias, podemos calcular la media como:\n",
        "$$\\bar{X}=\\frac{1}{N} \\sum_{(ùëñ=1)}^{ùëÅ}n_{ùëñ}ùë•_{ùëñ} =\\frac{ùëõ_1 ùë•_1+ùëõ_2 ùë•_2+‚Ä¶+ùëõ_ùëò ùë•_ùêæ}{N}.$$\n",
        "\n",
        "***\n",
        "**Ejemplo base**:\n",
        "***\n",
        "\n",
        "Calcular la media de las edades de 10 personas:\n",
        "\n",
        "\\[58,45,49,22,25,28,30,30,38,36\\]\n",
        "\n",
        "***"
      ],
      "metadata": {
        "id": "SQJSt7rsPzlA"
      },
      "id": "SQJSt7rsPzlA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicios:\n",
        "\n",
        "Calcular la media aritm√©tica en las siguientes situaciones.\n",
        "\n",
        "1. Para la variable $X=(4, 7, 5, 7, 5, 4, 2, 7)$.\n",
        "\n",
        "2. Para la variable **Longitud_Texto**:\n",
        "\n",
        "    Sabiendo que $\\sum ùëã_{ùëñ}=3638051$   y $N= 45476$\n",
        "\n",
        "3. Usando la reducci√≥n por intervalos de la tabla de frecuencias de la variable **Longitud_Texto** (C√°lculo de la marca de clase)\n",
        "\n",
        "4. Usando la reducci√≥n por intervalos de la tabla de frecuencias de la variable **Num_Palabras_Unicas** (C√°lculo de la marca de clase)\n"
      ],
      "metadata": {
        "id": "Zor5i4_2Q9AL"
      },
      "id": "Zor5i4_2Q9AL"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Obtenci√≥n de la suma total y del tama√±o de la muestra para las variables *Longitud_texto*\n",
        "print(f\"La suma de X_i para el ejercicio es {df['Longitud_Texto'].sum()}\" )\n",
        "print(f\"El tama√±o (N) de X_i para el ejercicio es {df.shape[0]}\")"
      ],
      "metadata": {
        "id": "PI3Ny-GSD--n",
        "cellView": "form"
      },
      "id": "PI3Ny-GSD--n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Respuestas** 1 y 2 { display-mode: \"code\" }\n",
        "X = [4, 7, 5, 7, 5, 4, 2, 7]\n",
        "x_hat = round(sum(X) / len(X),2)\n",
        "# Imprimir el texto junto con la variable y su valor\n",
        "print(f\"1. Para la variable X = {X}: \\u0302x={x_hat}\")\n",
        "\n",
        "# Para la variable Longitud_Texto:\n",
        "x_hat = round(df[\"Longitud_Texto\"].mean(),2)\n",
        "suma = df[\"Longitud_Texto\"].sum()\n",
        "n = N\n",
        "print(f\"2. Para la variable Longitud_Texto: \\u0302x={suma}/{n}={x_hat}\")"
      ],
      "metadata": {
        "id": "JL5OERi2Xt9g"
      },
      "id": "JL5OERi2Xt9g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6ce1a628-cffc-4ec0-9354-9547312f99dd",
      "metadata": {
        "id": "6ce1a628-cffc-4ec0-9354-9547312f99dd"
      },
      "source": [
        "### Media aritm√©tica de datos agrupados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce0fca20-df91-4177-b6a0-96ecd37ed692",
      "metadata": {
        "id": "ce0fca20-df91-4177-b6a0-96ecd37ed692",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Mostramos las tablas de frecuencias (con la marca de clase) de las variables *longitud_texto* y *Num_Palabras_Unicas*:\n",
        "# Mostrar la tabla de frecuencias con la marca de clase\n",
        "print(\"\\n------------------longitud_texto---------------------------------------\\n\")\n",
        "print(tabla_frecuencias_Longitud_Texto[['Intervalo','x_i','n_i']])\n",
        "print(\"\\n------------------Num_Palabras_Unicas----------------------------------\\n\")\n",
        "print(tabla_frecuencias_Num_Palabras_Unicas[['Intervalo','x_i','n_i']])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Respuesta** 3.\n",
        "marcas_de_clase =  tabla_frecuencias_Longitud_Texto['x_i']\n",
        "frecuencias     =  tabla_frecuencias_Longitud_Texto['n_i']\n",
        "# Calculando la suma de los productos de las marcas de clase y las frecuencias\n",
        "suma_productos = sum(marcas_de_clase[i] * frecuencias[i] for i in range(len(marcas_de_clase)))\n",
        "\n",
        "print(f\"La suma de los productos es {suma_productos}\")\n",
        "# # Calculando el total de observaciones\n",
        "total_observaciones = sum(frecuencias)\n",
        "\n",
        "# # Calculando la media aritm√©tica\n",
        "x_hat = suma_productos / total_observaciones\n",
        "\n",
        "print(f\"3. Para la variable Longitud_Texto en datos agrupados la media aritm√©tica es:\\n\\u0302x={round(x_hat,2)}\")"
      ],
      "metadata": {
        "id": "EdZzerA7FfmC",
        "cellView": "form"
      },
      "id": "EdZzerA7FfmC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Respuesta** 4.\n",
        "marcas_de_clase =  tabla_frecuencias_Num_Palabras_Unicas['x_i']\n",
        "frecuencias     =  tabla_frecuencias_Num_Palabras_Unicas['n_i']\n",
        "# Calculando la suma de los productos de las marcas de clase y las frecuencias\n",
        "suma_productos = sum(marcas_de_clase[i] * frecuencias[i] for i in range(len(marcas_de_clase)))\n",
        "print(f\"La suma de los productos es {suma_productos}\")\n",
        "# # Calculando el total de observaciones\n",
        "total_observaciones = sum(frecuencias)\n",
        "\n",
        "# # Calculando la media aritm√©tica\n",
        "x_hat = suma_productos / total_observaciones\n",
        "\n",
        "print(f\"4. Para la variable Longitud_Texto en datos agrupados la media aritm√©tica es:\\n\\u0302x={round(x_hat,2)}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eOOUktyhs9n0"
      },
      "id": "eOOUktyhs9n0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Media aritm√©tica: pros\n",
        "\n"
      ],
      "metadata": {
        "id": "uWp8GFJ9wGV2"
      },
      "id": "uWp8GFJ9wGV2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tiene en cuenta **todos los valores observados**.\n",
        "\n",
        "Es **f√°cil de calcular.**\n",
        "\n",
        "Tiene un **claro significado estad√≠stico:** representa el valor que tomar√≠a cada una de las observaciones si el total se repartiera de manera equitativa.\n",
        "\n",
        "Es **√∫nica**."
      ],
      "metadata": {
        "id": "LobQIalqwiSs"
      },
      "id": "LobQIalqwiSs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Media aritm√©tica: contras"
      ],
      "metadata": {
        "id": "UUtkoVfFwcQC"
      },
      "id": "UUtkoVfFwcQC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los valores extremos ejercen gran influencia sobre el valor de la media aritm√©tica (un valor extremo es un dato excepcionalmente peque√±o o grande en comparaci√≥n con el resto)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FN7Q9wipwkR4"
      },
      "id": "FN7Q9wipwkR4"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Ejemplo ¬øCual es la media de cada conjunto de datos?:\n",
        "Datos1 = [2,5,7,9,12]\n",
        "Datos2 = [2,5,7,9,125]\n",
        "Datos = pd.DataFrame({\n",
        "    'Datos1': Datos1,\n",
        "    'Datos2': Datos2\n",
        "})\n",
        "display(Datos)"
      ],
      "metadata": {
        "id": "YtVMcdY8xoBp",
        "cellView": "form"
      },
      "id": "YtVMcdY8xoBp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Calcula la media de los dos conjuntos de datos:\n",
        "# Calcular las medias de Datos1 y Datos2\n",
        "media_datos1 = sum(Datos1) / len(Datos1)\n",
        "media_datos2 = sum(Datos2) / len(Datos2)\n",
        "\n",
        "# Crear la figura y los subgr√°ficos\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "# Gr√°fico de barras para Datos1\n",
        "axs[0].bar(range(len(Datos1)), Datos1, color='skyblue')\n",
        "axs[0].axhline(y=media_datos1, color='skyblue', linestyle='--', label=f'Media: {media_datos1}')\n",
        "axs[0].set_title('Datos1')\n",
        "axs[0].set_xticks(range(len(Datos1)))\n",
        "axs[0].set_xticklabels(range(1, len(Datos1) + 1))\n",
        "axs[0].legend()\n",
        "\n",
        "# Gr√°fico de barras para Datos2\n",
        "axs[1].bar(range(len(Datos2)), Datos2, color='salmon')\n",
        "axs[1].axhline(y=media_datos2, color='salmon', linestyle='--', label=f'Media: {media_datos2}')\n",
        "axs[1].set_title('Datos2')\n",
        "axs[1].set_xticks(range(len(Datos2)))\n",
        "axs[1].set_xticklabels(range(1, len(Datos2) + 1))\n",
        "axs[1].legend()\n",
        "\n",
        "# Ajustar el espaciado entre los subgr√°ficos\n",
        "plt.tight_layout()\n",
        "\n",
        "# Mostrar el gr√°fico\n",
        "plt.show()\n",
        "print(f'La media de los Datos 1 es {media_datos1}\\n\\nLa media de los Datos 2 es {media_datos2}')"
      ],
      "metadata": {
        "id": "OOIymyBcYA_W",
        "cellView": "form"
      },
      "id": "OOIymyBcYA_W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Media aritm√©tica: propiedades\n",
        "\n",
        "\n",
        "* La suma de las desviaciones de los valores de la variable, respecto a su media aritm√©tica, es igual a **cero**:\n",
        "$$\\sum(X_{i}‚àí\\bar{X})= 0 = \\sum X_{i}-N\\bar{X}=0, \\text{ donde } \\Big[\\bar{X}=\\frac{\\sum X_{i}}{N}\\Big].$$\n",
        "\n",
        "\n",
        "Si dividimos todas las observaciones en $k$ grupos disjuntos, cada uno de ellos con media $\\bar{X}_{i}$ y tama√±o $N_{ùëñ}$, la **media aritm√©tica de todo el conjunto** se puede calcular como\n",
        "$$\\bar{X}=\\frac{\\bar{X}_{1}N_{1}+‚Ä¶+\\bar{X}_{k}N_{k}}{N}.$$\n",
        "\n",
        "\n",
        "***\n",
        "**Ejemplo**:\n",
        "***\n",
        "\n",
        "1. En el caso de los **Datos1**, verificar que la suma de las desviaciones es 0.\n",
        "\n",
        "2. Si tenemos informaci√≥n de las edades de un grupo de personas, consideremos la siguiente distribuci√≥n de edades en tres grupos disjuntos:\n",
        "\n",
        "* Grupo 1: Edades entre 20 y 30 a√±os, con media $\\bar{X}_{1} = 25$ y tama√±o $N_{1} = 10$\n",
        "* Grupo 2: Edades entre 30 y 40 a√±os, con media $\\bar{X}_{2} = 35$ y tama√±o $N_{2} = 8$\n",
        "* Grupo 3: Edades entre 40 y 50 a√±os, con media $\\bar{X}_{3} = 45$ y tama√±o $N_{3} = 12$\n",
        "\n",
        "\n",
        "Queremos calcular la media aritm√©tica de todas las edades combinadas utilizando la propiedad:\n",
        "\n",
        "***\n",
        "\n",
        "\n",
        "Si transformamos los valores de la variable $X$ a trav√©s de un cambio de origen y escala $(U=a+bX)$. Entonces la media aritm√©tica de la variable transformada $U$ es $\\bar{U}= a+b\\bar{X}$.\n",
        "\n",
        "\n",
        "**Implicaciones:**\n",
        "\n",
        "En particular (para $b=1$) si $U=a+X$ entonces $\\bar{U}= a+\\bar{X}$.\n",
        "\n",
        "***\n",
        "**Ejemplo**:\n",
        "***\n",
        "\n",
        "Supongamos que tenemos el peso de $N = 20$ personas $X = (59, 41, 55, 79, 61, 83, 43, 54, 89, 57, 80, 86, 67, 56, 80, 42, 60, 83, 76, 87)$.\n",
        "\n",
        "Ahora, supongamos que hacemos la tranformaci√≥n $U= a+X$ d√≥nde $a=10$ y $b=1$, entonces $U = (69, 51, 65, 89, 71, 93, 53, 64, 99, 67, 90, 96, 77, 66, 90, 52, 70, 93, 86, 97)$\n",
        "\n",
        "Entonces:\n",
        "\n",
        "$\\bar{U}= a+\\bar{X} = 10 + \\frac{\\sum_{i}{X}}{20} ‚â° \\frac{\\sum_{i}U}{20}$\n",
        "  \n",
        "Vamos a comprobar...\n"
      ],
      "metadata": {
        "id": "lc32TP7c1cli"
      },
      "id": "lc32TP7c1cli"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Genera listas X y U y calcula las medias:\n",
        "# Generar la lista X\n",
        "# X = [random.randint(40, 90) for _ in range(20)]\n",
        "X = [59, 41, 55, 79, 61, 83, 43, 54, 89, 57, 80, 86, 67, 56, 80, 42, 60, 83, 76, 87]\n",
        "\n",
        "print(f\"Lista X: {X}\\ny la suma de X: {sum(X)}\")\n",
        "a = 10\n",
        "b = 1\n",
        "\n",
        "# Generar la lista U.\n",
        "U = [b*x + a for x in X]\n",
        "print(f\"Lista U: {U}\\ny la suma de U: {sum(U)}\")\n",
        "\n",
        "x_bar= sum([i for i in X]) / len([i for i in X])\n",
        "print(\"utilizando la conversi√≥n:\\n\")\n",
        "print(f\"La media de U = {b} X {x_bar} + {a}= {round(b*x_bar+a,1)}\")\n",
        "print(\"Calculando  la  media de U directamente:\\n\")\n",
        "print(f\"La media de U =  {sum([i for i in U]) / len([i for i in U])}\")"
      ],
      "metadata": {
        "id": "poiWABWkyTO3",
        "cellView": "form"
      },
      "id": "poiWABWkyTO3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "## Mediana\n",
        "***\n",
        "\n",
        "Dada una variable $X$, su mediana, $Me(X)$, es el valor tal que es mayor que la mitad (al menos) de los casos y tambi√©n es menor o igual que (al menos) la mitad de los casos.\n",
        "\n",
        "Para calcular la mediana buscamos el valor central en la lista de valores ordenados de $X$. Para ello, ordenamos los casos de manera que $X_{1} < X_{2} < ... < X_{N} $. Calculamos la posici√≥n central $\\frac{N+1}{2}$:\n",
        "\n",
        "* Si el resultado es entero (esto es, si $N$ es impar), entonces $Me(X)=X_{(N+1)/2}$.\n",
        "\n",
        "* En otro caso (si $N$ es par): $Me(X)=\\frac{(X_{N/2}+X_{(N+2)/2})}{2}$, esto es, el promedio de los valores en las posiciones inmediatamente anterior y posterior a $\\frac{N+1}{2}$.\n",
        "\n",
        "¬ø$X$ est√° agrupada por intervalos? $\\rightarrow$ Lo veremos m√°s adelante.\n",
        "\n",
        "\n",
        "\n",
        "***\n",
        "**Ejemplo**:\n",
        "***\n",
        "\n",
        "Calcula la mediana de los siguientes conjuntos de datos:\n",
        "\n",
        "1. $X=[5,2,7,12,9]$\n",
        "2. $Y=[5,2,13,7,12,9]$\n",
        "3. $Z=[18,9,66,35,8,9,7,8,3,10]$\n",
        "\n",
        "\n",
        "\n",
        "***\n",
        "**Ejercicio**:\n",
        "***\n",
        "\n",
        "Calcular la mediana del campo *Longitud_Texto* y compararla con la media del mismo campo."
      ],
      "metadata": {
        "id": "o-ejPKQKyyZB"
      },
      "id": "o-ejPKQKyyZB"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Calcula la mediana del campo Longitud_Texto:\n",
        "# Ejemplo de datos\n",
        "datos = df.sort_values(['Longitud_Texto'])\n",
        "datos = datos.reset_index()\n",
        "\n",
        "\n",
        "# # Calcular la mediana\n",
        "mediana = calcular_mediana(datos.loc[:,'Longitud_Texto'])\n",
        "\n",
        "# # Imprimir la mediana\n",
        "print(\"La mediana de la longitud de palabras de las rese√±as es:\", mediana)\n",
        "\n",
        "display(datos.loc[22737:22738,[\"Id\",'Longitud_Texto']])\n"
      ],
      "metadata": {
        "id": "afpRMY1Kyzzy",
        "cellView": "form"
      },
      "id": "afpRMY1Kyzzy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Otra forma de hacerlo (funci√≥n predise√±ada):"
      ],
      "metadata": {
        "id": "pGHRDcNSHDFg"
      },
      "id": "pGHRDcNSHDFg"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title C√°lculo de la mediana con funci√≥n de Python:\n",
        "#Se requiere el uso de la funci√≥n median de la librer√≠a statistics:\n",
        "# Calcular la mediana\n",
        "mediana2 = median(datos.loc[:,'Longitud_Texto'])\n",
        "\n",
        "# Imprimir la mediana\n",
        "print(\"La mediana de los datos es:\", mediana2)"
      ],
      "metadata": {
        "id": "DH2gbJhNFF-Z",
        "cellView": "form"
      },
      "id": "DH2gbJhNFF-Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intervalo mediano:\n",
        "\n",
        "Si la variable $X$ est√° reducida por intervalos, tomaremos como $Me(X)$ la marca de clase del **intervalo mediano**.\n",
        "\n",
        "El intervalo mediano es el primer intervalo cuya frecuencia absoluta acumulada sea mayor o igual a $N+1/2$. De manera equivalente, tambi√©n podemos obtenerlo buscando el primer intervalo tal que su frecuencia absoluta relativa sea mayor o igual a $\\frac{1}{2}=0.5$. Esto es, el intervalo mediano es el intervalo $i$ tal que:\n",
        "$$N_{i} \\geq \\frac{N+1}{2}$$ y $$N_{i-1} < \\frac{N+1}{2}$$\n",
        "\n",
        "O de manera equivalente, $F_{i} \\geq 0.5$ y $F_{i-1} < 0.5$.\n",
        "\n",
        "\n",
        "\n",
        "***\n",
        "**Ejemplo**:\n",
        "***\n",
        "\n",
        "Calcular la mediana de la siguiente tabla de frecuencias:\n"
      ],
      "metadata": {
        "id": "gRYpfIh4HY1i"
      },
      "id": "gRYpfIh4HY1i"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Tabla:\n",
        "intervalos=[\"[0-4)\",\"[4-8)\",\"[8-12)\",\"[12-16)\",\"[16-20)\"]\n",
        "x_i = [2,6,10,14,18]\n",
        "n_i = [3,5,6,4,3]\n",
        "N_i = [3,8,14,18,21]\n",
        "\n",
        "tabla = pd.DataFrame({\n",
        "    'Intervalos': intervalos,\n",
        "    'x_i':x_i,\n",
        "    'n_i': n_i,\n",
        "    'N_i':N_i,\n",
        "})\n",
        "print(tabla)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "NDL_pfLbD5BY"
      },
      "id": "NDL_pfLbD5BY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ejemplo 2**:\n",
        "\n",
        "Utilizaremos un nuevo texto (obtenido con ChatGPT) que contiene 218 palabras (119 luego de la limpieza):\n",
        "\n",
        "***\n",
        "***Este texto que est√°s leyendo ahora mismo sirve como un ejemplo ilustrativo para calcular el n√∫mero mediano de la longitud de palabras dentro de un texto determinado. Es un ejercicio pr√°ctico que nos permite comprender c√≥mo funcionan los c√°lculos de estad√≠sticas descriptivas aplicadas al an√°lisis ling√º√≠stico.\n",
        "En este contexto, la longitud de las palabras se refiere al n√∫mero de caracteres que componen cada palabra individualmente. Al analizar este texto, encontrar√°s palabras cortas, como \"este\", \"es\", \"un\", \"de\", as√≠ como palabras m√°s largas como \"ilustrativo\", \"estad√≠sticas\", \"descriptivas\", entre otras.\n",
        "El objetivo es determinar el n√∫mero mediano de caracteres que conforman las palabras en este texto. Al calcular este valor, podemos tener una idea m√°s clara de la extensi√≥n promedio de las palabras utilizadas aqu√≠. Este proceso implica ordenar las longitudes de las palabras de menor a mayor y encontrar el valor medio.\n",
        "Este ejercicio es √∫til en diversos contextos, desde an√°lisis de texto en ling√º√≠stica computacional hasta la elaboraci√≥n de informes y an√°lisis de contenido en campos como la investigaci√≥n acad√©mica, la publicidad y el procesamiento del lenguaje natural.\n",
        "Es importante destacar que el c√°lculo del n√∫mero mediano de longitud de palabras nos brinda informaci√≥n valiosa sobre la estructura y complejidad del lenguaje utilizado, lo que puede tener implicaciones significativas en la comunicaci√≥n efectiva y la comprensi√≥n del texto.***\n",
        "***\n",
        "Vamos a calcular la mediana de la longitud de las palabras dentro del texto, teniendo la tabla de frecuencias agrupadas por ***intervalos***:\n"
      ],
      "metadata": {
        "id": "9pMQAv1hJT8f"
      },
      "id": "9pMQAv1hJT8f"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Ejercicio: C√°lculo de mediana en datos agrupados por intervalos:\n",
        "\n",
        "texto = \"\"\"\n",
        "Este texto que est√°s leyendo ahora mismo sirve como un ejemplo ilustrativo para calcular el n√∫mero mediano de la longitud de palabras dentro de un texto determinado. Es un ejercicio pr√°ctico que nos permite comprender c√≥mo funcionan los c√°lculos de estad√≠sticas descriptivas aplicadas al an√°lisis ling√º√≠stico.\n",
        "\n",
        "En este contexto, la longitud de las palabras se refiere al n√∫mero de caracteres que componen cada palabra individualmente. Al analizar este texto, encontrar√°s palabras cortas, como \"este\", \"es\", \"un\", \"de\", as√≠ como palabras m√°s largas como \"ilustrativo\", \"estad√≠sticas\", \"descriptivas\", entre otras.\n",
        "\n",
        "El objetivo es determinar el n√∫mero mediano de caracteres que conforman las palabras en este texto. Al calcular este valor, podemos tener una idea m√°s clara de la extensi√≥n promedio de las palabras utilizadas aqu√≠. Este proceso implica ordenar las longitudes de las palabras de menor a mayor y encontrar el valor medio.\n",
        "\n",
        "Este ejercicio es √∫til en diversos contextos, desde an√°lisis de texto en ling√º√≠stica computacional hasta la elaboraci√≥n de informes y an√°lisis de contenido en campos como la investigaci√≥n acad√©mica, la publicidad y el procesamiento del lenguaje natural.\n",
        "\n",
        "Es importante destacar que el c√°lculo del n√∫mero mediano de longitud de palabras nos brinda informaci√≥n valiosa sobre la estructura y complejidad del lenguaje utilizado, lo que puede tener implicaciones significativas en la comunicaci√≥n efectiva y la comprensi√≥n del texto.\n",
        "\"\"\"\n",
        "\n",
        "# Limpiar el texto original\n",
        "texto_limpio = limpiar_texto(texto, \"spanish\")\n",
        "\n",
        "print(\"---------\")\n",
        "# print(f\"\\nTexto:\\n{texto_limpio}\\n\")\n",
        "palabras = texto_limpio.split()\n",
        "\n",
        "longitudes = [len(palabra) for palabra in palabras]\n",
        "\n",
        "pal_long= pd.DataFrame({\n",
        "    'palabras': palabras,\n",
        "    'longitudes': longitudes})\n",
        "pal_long['longitudes_group'] = group_by_intervals(pal_long, 'longitudes',5)\n",
        "\n",
        "N_ej3=pal_long.shape[0]\n",
        "print(f\"Tama√±o del conjunto de datos: {N_ej3} palabras\")\n",
        "# display(pal_long)\n",
        "\n",
        "# Calcular la tabla de frecuencias de la columna 'Grupo'\n",
        "frecuencias_grupo_ej3 = pal_long['longitudes_group'].value_counts()\n",
        "frecuencias_grupo_ej3 = frecuencias_grupo_ej3.sort_index()\n",
        "\n",
        "# Calcular la frecuencia relativa\n",
        "frecuencias_relativas_gr_ej3 = frecuencias_grupo_ej3 / len(pal_long)\n",
        "\n",
        "# Calcular las frecuencias acumuladas\n",
        "frecuencias_acumuladas_gr_ej3 = frecuencias_grupo_ej3.cumsum()\n",
        "\n",
        "# Calcular la frecuencia relativa acumulada\n",
        "frecuencias_relativas_acum_gr_ej3 = frecuencias_acumuladas_gr_ej3 / len(pal_long)\n",
        "\n",
        "\n",
        "# Calcular la marca de clase para cada intervalo\n",
        "MarcaDeClase_ej3 = list(map(marca_de_clase, list(frecuencias_grupo_ej3.index)))\n",
        "\n",
        "# Crear DataFrame con las frecuencias\n",
        "tabla_frecuencias_eje3 = pd.DataFrame({\n",
        "    'Intervalo': frecuencias_grupo_ej3.index,\n",
        "    'n_i': frecuencias_grupo_ej3.values,\n",
        "    'f_i': frecuencias_relativas_gr_ej3.values,\n",
        "    'N_i':frecuencias_acumuladas_gr_ej3.values,\n",
        "    'F_i':frecuencias_relativas_acum_gr_ej3.values,\n",
        "    'Marca_de_clase':MarcaDeClase_ej3\n",
        "})\n",
        "\n",
        "# Imprimir la tabla de frecuencias\n",
        "print(\"Tabla de frecuencias:\")\n",
        "print(\"-----------------------------------------------------------------------------------------------------------\")\n",
        "print(\"Longitud   | Frecuencia   | Frecuencia Relativa | Frec. Abs. Acumulada | Frec. Rel. Acumulada | Marca de clase\")\n",
        "print(\"-----------------------------------------------------------------------------------------------------------\")\n",
        "for indice, fila in tabla_frecuencias_eje3.iterrows():\n",
        "    # print(f'√çndice: {indice}')\n",
        "    # print(f'Contenido de la fila:\\n{fila}\\n')\n",
        "\n",
        "    print(f\"{fila[0]:^9}    | {fila[1]:^10}| {fila[2]:.4f}              | {fila[3]:^19}  | {fila[4]:.4f}              | {fila[5]:.1f}\")\n",
        "print(\"-----------------------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FKzf9KwjGuo_",
        "cellView": "form"
      },
      "id": "FKzf9KwjGuo_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Soluci√≥n del ejercicio: { vertical-output: true }\n",
        "pto = (N_ej3+1)/2\n",
        "print(f\"N_{2}={pto}\")\n",
        "\n",
        "mediana=round(tabla_frecuencias_eje3.loc[(tabla_frecuencias_eje3['N_i'] >= pto).idxmax(),\"Marca_de_clase\"],2)\n",
        "print(f\"La mediana de longitud de palabras del texto de prueba es: {mediana}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xYgoQc1sy2-M",
        "cellView": "form"
      },
      "id": "xYgoQc1sy2-M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mediana pros y contras:\n",
        "\n",
        "### Pros:\n",
        "\n",
        "- **Robustez ante valores extremos:** La mediana no es sensible a los valores extremos o at√≠picos en los datos. Esto significa que un valor extremo no afecta significativamente la mediana, lo que la hace √∫til cuando los datos contienen valores at√≠picos que, como dijimos antes, podr√≠an distorsionar la media.\n",
        "   \n",
        "- **Apropiada para datos ordinales o intervalos:** La mediana es √∫til cuando se trabaja con datos ordinales o de intervalos, donde el orden de los valores es importante, pero no necesariamente su magnitud exacta.\n",
        "\n",
        "- **Interpretaci√≥n sencilla:** Es f√°cil de entender y calcular. Consiste en el valor que divide a la muestra ordenada en dos partes iguales, de modo que la mitad de los valores est√°n por encima y la otra mitad por debajo.\n",
        "\n",
        "### Contras:\n",
        "\n",
        "- **Menos sensible a la distribuci√≥n de los datos:** La mediana no utiliza toda la informaci√≥n en los datos y, por lo tanto, puede ser menos sensible que la media para detectar patrones o cambios en la distribuci√≥n de los datos.\n",
        "\n",
        "\n",
        "***\n",
        "**Ejemplo**:\n",
        "***\n",
        "\n",
        "¬øCual es la mediana de los conjuntos **Dato1** y **Datos2**?\n"
      ],
      "metadata": {
        "id": "Z3qeuzT7G1Ji"
      },
      "id": "Z3qeuzT7G1Ji"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Mostramos los dos conjuntos de datos\n",
        "display(Datos)"
      ],
      "metadata": {
        "id": "h4xIQpQxF0yH",
        "cellView": "form"
      },
      "id": "h4xIQpQxF0yH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Media vs Mediana con datos extremos (variable *longitud_texto*):\n",
        "print(\"Calculamos la media y la mediana de la variable longitud de texto:\")\n",
        "\n",
        "me=round(df.loc[:,\"Longitud_Texto\"].mean(),2)\n",
        "med=round(median(df.loc[:,\"Longitud_Texto\"]),2)\n",
        "print(f\"Media de palabras en las rese√±as: {me} mientras que la mediana es {med}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "f14FCFHgOSYf"
      },
      "id": "f14FCFHgOSYf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "## Cuantiles:\n",
        "***\n",
        "\n",
        "\n",
        "Las ***medidas de posici√≥n no centrales*** m√°s conocidas son los cuantiles, que podemos ver como una generalizaci√≥n de la mediana.\n",
        "\n",
        "* La *mediana* es el cuantil de orden 2: separa los valores ordenados en dos bloques iguales en n√∫mero de casos.\n",
        "\n",
        "* Los *cuartiles* $Q_{1}$, $Q_{2}$ y $Q_{3}$son los cuantiles de orden 4: separan los valores ordenados en cuatro bloques iguales en n√∫mero de casos.\n",
        "\n",
        "* Los *percentiles* $P_{1}$, $P_{2}$, ..., $P_{99}$ son los de orden 100, y separan los valores ordenados en cien bloques iguales en n√∫mero de casos.\n",
        "\n",
        "* Los *deciles* $D_{1}$, $D_{2}$, ..., $D_{9}$ son los de orden 10.\n",
        "\n",
        "* Otros cuantiles menos habituales: los terciles (orden 3), los quintiles (orden 5) ‚Ä¶\n",
        "\n",
        "\n",
        "### ¬øC√≥mo se calculan?:\n",
        "\n",
        "Ordenamos los datos: $X_{1} \\leq X_{1} \\leq ... \\leq X_{N}$.\n",
        "\n",
        "Para el orden $q$ se definen $q-1$ cuantiles de n√∫mero $r=1, 2, ..., q-1$.\n",
        "\n",
        "El ùëü-√©simo cuantil de orden $q$ es el valor que ocupa la posici√≥n $\\frac{r}{q}(N+1)$:\n",
        "\n",
        "- si el resultado de la posici√≥n es entero, reportamos el valor de esa posici√≥n $$X_{i} \\text{ tal que } i=\\frac{r}{q}(N+1)  $$\n",
        "\n",
        "* en otro caso, es decir, si el valor $\\frac{r}{q}(N+1)$ no es entero,  el valor exacto del cuantil se calcula mediante la siguiente f√≥rmula:\n",
        "\n",
        "$$ X_{i} + \\alpha (X_{i+1}-X_{i}),$$\n",
        "\n",
        "donde $\\alpha$ es la fracci√≥n (parte decimal) de la posici√≥n entre $i$ e $i+1$.\n",
        "\n",
        "\n",
        "\n",
        "***\n",
        "**Ejemplo:**\n",
        "***\n",
        "\n",
        "Calcular, teniendo el siguiente conjunto de datos\n",
        "$$W=[183,183,187,191,195,198,199,201,204,205,209,212,213,215,220,221,225,230,232,236,239,241,245,246,250,252,256,260,263,266,267,271,275,276,277,278,280,283,285,286,286,294,297,303,307,314,317,321,325]:$$\n",
        "\n",
        "1. El cuartil 2 $Q_{2}$\n",
        "2. El percentil 81 $P_{81}$\n",
        "\n",
        "\n",
        "***\n",
        "\n",
        "\n",
        "\n",
        "**Observaci√≥n**. Muchos cuantiles de distintos ordenes coinciden, por ejemplo:\n",
        "$Q_{1} = P_{25}$ (la cuarta parte es el 25%);\n",
        "\n",
        "$Q_{2} = Me = P_{50}$ (2/4=1/2=50/100);\n",
        "\n",
        "$Q_{3} = P_{75}$ (3/4=75/100).\n",
        "\n",
        "\n",
        "***\n",
        "**¬øQu√© otros cuantiles coinciden?**\n",
        "***"
      ],
      "metadata": {
        "id": "yVfrMANlOyGp"
      },
      "id": "yVfrMANlOyGp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cuantiles: Ejercicio:\n",
        "Calcular los cuantiles\n",
        "\n",
        "El objetivo de este ejercicio es calcular los cuartiles y los percentiles 3 y 10 de la longitud de las palabras del texto de prueba:\n",
        "\n",
        "***Texto:***\n",
        "***\n",
        "***Este texto que est√°s leyendo ahora mismo sirve como un ejemplo ilustrativo para calcular el n√∫mero mediano de la longitud de palabras dentro de un texto determinado. Es un ejercicio pr√°ctico que nos permite comprender c√≥mo funcionan los c√°lculos de estad√≠sticas descriptivas aplicadas al an√°lisis ling√º√≠stico.\n",
        "En este contexto, la longitud de las palabras se refiere al n√∫mero de caracteres que componen cada palabra individualmente. Al analizar este texto, encontrar√°s palabras cortas, como \"este\", \"es\", \"un\", \"de\", as√≠ como palabras m√°s largas como \"ilustrativo\", \"estad√≠sticas\", \"descriptivas\", entre otras.\n",
        "El objetivo es determinar el n√∫mero mediano de caracteres que conforman las palabras en este texto. Al calcular este valor, podemos tener una idea m√°s clara de la extensi√≥n promedio de las palabras utilizadas aqu√≠. Este proceso implica ordenar las longitudes de las palabras de menor a mayor y encontrar el valor medio.\n",
        "Este ejercicio es √∫til en diversos contextos, desde an√°lisis de texto en ling√º√≠stica computacional hasta la elaboraci√≥n de informes y an√°lisis de contenido en campos como la investigaci√≥n acad√©mica, la publicidad y el procesamiento del lenguaje natural.\n",
        "Es importante destacar que el c√°lculo del n√∫mero mediano de longitud de palabras nos brinda informaci√≥n valiosa sobre la estructura y complejidad del lenguaje utilizado, lo que puede tener implicaciones significativas en la comunicaci√≥n efectiva y la comprensi√≥n del texto.***\n",
        "***\n",
        "\n",
        "\n",
        "Recordemos que en el ejercicio anterior limpiamos el texto, por lo que utilizaremos la variable creada **texto_limpio**.\n",
        "Calcularemos los valores que dividen la distribuci√≥n de la longitud de las palabras, primero en 4 partes iguales, es decir los cuartiles, y luego en 10 partes iguales, es decir, los deciles."
      ],
      "metadata": {
        "id": "ea1TWQLxebil"
      },
      "id": "ea1TWQLxebil"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ¬øQu√© informaci√≥n necesitamos?:\n",
        "\n",
        "try:\n",
        "    pal_long = pal_long.sort_values(by=\"longitudes\").reset_index()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "display(pal_long[['longitudes']])\n",
        "N_1=N_ej3+1\n",
        "\n",
        "print(f\"N+1 es {N_1}\")\n",
        "q=4 #entonces q-1 grupos\n",
        "print(\"------------------------------------------------------------\\nPara los cuartiles\\n------------------------------------------------------------\\n\")\n",
        "for r in range(1,q):\n",
        "    i=(r/q)*N_1\n",
        "    print(f\"Posici√≥n cuartil {r}: {i}\")\n",
        "\n",
        "\n",
        "q=100 #entonces q-1 grupos\n",
        "print(\"------------------------------------------------------------\\nPara los percentiles\\n------------------------------------------------------------\\n\")\n",
        "for r in [3,10]:\n",
        "    i=(r/q)*N_1\n",
        "    print(f\"Posici√≥n percentil {r}: {i}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uv310VXRcJdi"
      },
      "id": "uv310VXRcJdi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title C√°lculo de cuartiles y percentiles usando funci√≥n de Python:\n",
        "\n",
        "# Calculamos los cuantiles: cuartiles\n",
        "cuantiles = [0.25, 0.50, 0.75]\n",
        "resultados_cuantiles = pal_long['longitudes'].quantile(cuantiles, interpolation='linear')\n",
        "# Mostramos los resultados\n",
        "print(\"\\nCuartiles:\")\n",
        "print(resultados_cuantiles)\n",
        "\n",
        "print(\"------------------------------------------------------------------------\")\n",
        "\n",
        "percentiles = [0.03,0.1]\n",
        "resultados_percentiles = pal_long['longitudes'].quantile(percentiles, interpolation='linear')\n",
        "\n",
        "# Mostramos los resultados\n",
        "print(\"Percentiles:\")\n",
        "print(resultados_percentiles)"
      ],
      "metadata": {
        "id": "xjqpbsfZOYRH",
        "cellView": "form"
      },
      "id": "xjqpbsfZOYRH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cuartiles para una variable por intervalo\n",
        "Si la variable $X$ est√° reducida por intervalos, tomaremos como cuantil r-√©simo de orden $q$ la marca de clase del primer intervalo tal que su frecuencia absoluta acumulada sea mayor o igual a $\\frac{r}{q}(N+1)$.\n",
        "De manera equivalente, tambi√©n podemos obtenerlo buscando el primer intervalo tal que su frecuencia absoluta relativa sea mayor o igual a $\\frac{r}{q}$.\n",
        "Esto es, el intervalo $i$ tal que:\n",
        "\n",
        "$$N_{i} \\geq \\frac{r}{q}(N+1)$$\n",
        "y\n",
        "$$N_{i-1} < \\frac{r}{q}(N+1)$$\n",
        "\n",
        "O de manera equivalente, $F_{i} \\geq \\frac{r}{q} $ y $F_{i-1} < \\frac{r}{q} $\n",
        "\n",
        "\n",
        "***\n",
        "**Ejemplo**\n",
        "***\n",
        "\n",
        "Calcular el tercer cuartil y los percentiles 29 (con ayuda de la frecuencia absoluta acumulada) y 62 (con ayuda de la frecuencia relativa acumulada) para los siguientes datos agrupados:\n"
      ],
      "metadata": {
        "id": "C3WfYyPiyMQt"
      },
      "id": "C3WfYyPiyMQt"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Tabla de frecuencias ejemplo:\n",
        "intervalo=[\"[300,325)\",\"[325,350)\",\"[350,375)\",\"[375,400)\",\"[400,425)\",\"[425,450)\",\"[450,475)\",\"[475,500)\"]\n",
        "ni=[54,77,44,59,64,49,95,58]\n",
        "fi=[i/sum(ni) for i in ni]\n",
        "print(f\"Suma de ni: {sum(ni)}\")\n",
        "Ni= [54,131,175,234,298,347,442,500]\n",
        "Fi=[i/sum(ni) for i in Ni]\n",
        "\n",
        "\n",
        "\n",
        "#calcular la marca de clase\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# x_i=[312.5,350,362.5,387.5]\n",
        "\n",
        "tabla =  pd.DataFrame({\n",
        "    \"Intervalos\": intervalo,\n",
        "    \"n_i\":ni,\n",
        "    \"f_i\":fi,\n",
        "    \"N_i\":Ni,\n",
        "    \"F_i\":Fi\n",
        "})\n",
        "display(tabla)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "I_tCqkAdi92b"
      },
      "id": "I_tCqkAdi92b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Ejercicio de cuantiles:** Calcular los cuantiles de datos agrupados por intervalos: El objetivo de este ejercicio es calcular los cuartiles y el percentil 95 de la longitud de las palabras del texto que est√° agrupada en intervalos (ver tabla *tabla_frecuencias_eje3*)\n",
        "print(tabla_frecuencias_eje3)"
      ],
      "metadata": {
        "id": "uvF-Y_HAa-F6"
      },
      "id": "uvF-Y_HAa-F6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Soluci√≥n cuantiles por intervalos: { vertical-output: true }\n",
        "#cuartiles:\n",
        "print(f\"\\nLos cuartiles de los datos agrupados por intervalos son:\\n\")\n",
        "q=4\n",
        "for i in range(1,q):\n",
        "    pto=i/q\n",
        "    q_i=round(tabla_frecuencias_eje3.loc[(tabla_frecuencias_eje3['F_i'] >= pto).idxmax(),\"Marca_de_clase\"])\n",
        "    print(f\"El cuartil {i} de longitud de palabras del texto de prueba es: {q_i}\")\n",
        "\n",
        "\n",
        "print(f\"\\nPercentil 95 de los datos agrupados por intervalos:\\n\")\n",
        "q=100\n",
        "i=95\n",
        "pto=95/i\n",
        "q_i=round(tabla_frecuencias_eje3.loc[(tabla_frecuencias_eje3['F_i'] >= pto).idxmax(),\"Marca_de_clase\"])\n",
        "print(f\"El percentil {i} de longitud de palabras del texto de prueba es: {q_i}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "PIeRfd9z0Nf4"
      },
      "id": "PIeRfd9z0Nf4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "## Moda:\n",
        "***\n",
        "\n",
        "La moda es (son) el (los) valor(es) de frecuencia m√°xima. Puede haber m√°s de una moda. La denotamos como $Mo(X)$.\n",
        "\n",
        "**Ejemplo:**\n",
        "\n",
        "Para la variable $X =(4, 7, 5, 7, 5, 4, 2, 7)$, tenemos $Mo(X)=7$, que es el valor con mayor frecuencia absoluta.\n",
        "\n",
        "Se suele considerar una medida de posici√≥n central, pero no tiene porqu√© comportarse como tal.\n",
        "\n",
        "**Por ejemplo:**\n",
        "\n",
        "En el ejemplo anterior $Mo(X)= max(X)$.\n",
        "\n",
        "Para la variable $X=(4, 7, 4, 7, 5, 4, 5, 7)$, tenemos $Mo(X)={4,7}$, que son los valores m√≠nimo y m√°ximo de $X$.\n",
        "\n",
        "Si nos dan $X$ reducida por intervalos, todo aquel intervalo con densidad de frecuencia m√°xima es intervalo modal (aquellos con altura m√°xima en el histograma). Reportaremos la moda como la(s) marca(s) de clase de el (los) intervalo(s) modal(es) como $Mo(X)$.\n",
        "\n",
        "***\n",
        "**Ejemplo:**\n",
        "***\n",
        "\n",
        "Calcular la moda de los siguientes conjuntos de datos:\n",
        "\n",
        "1.  $[1, 4, 2, 4, 5, 3]$\n",
        "2.  $ [2, 6, 7, 2, 3, 6, 9]$\n",
        "3.  $[3, 3, 4, 1, 3, 4, 2, 1, 4, 5, 2, 1]$\n",
        "\n",
        "***\n",
        "\n"
      ],
      "metadata": {
        "id": "Us59_GftJ3zm"
      },
      "id": "Us59_GftJ3zm"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title C√°lculo de la(las) media(s) del ejemplo:\n",
        "# Lista 1\n",
        "lista1 = [1, 4, 2, 4, 5, 3]\n",
        "frecuencias1 = Counter(lista1)\n",
        "\n",
        "# Lista 2\n",
        "lista2 = [2, 6, 7, 2, 3, 6, 9]\n",
        "frecuencias2 = Counter(lista2)\n",
        "\n",
        "# Lista 3\n",
        "lista3 = [3, 3, 4, 1, 3, 4, 2, 1, 4, 5, 2, 1]\n",
        "frecuencias3 = Counter(lista3)\n",
        "\n",
        "# Graficar\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Gr√°fico 1\n",
        "plt.subplot(131)\n",
        "plt.bar(frecuencias1.keys(), frecuencias1.values())\n",
        "plt.title('Lista 1')\n",
        "plt.xlabel('Elemento')\n",
        "plt.ylabel('Frecuencia')\n",
        "\n",
        "# Gr√°fico 2\n",
        "plt.subplot(132)\n",
        "plt.bar(frecuencias2.keys(), frecuencias2.values())\n",
        "plt.title('Lista 2')\n",
        "plt.xlabel('Elemento')\n",
        "plt.ylabel('Frecuencia')\n",
        "\n",
        "# Gr√°fico 3\n",
        "plt.subplot(133)\n",
        "plt.bar(frecuencias3.keys(), frecuencias3.values())\n",
        "plt.title('Lista 3')\n",
        "plt.xlabel('Elemento')\n",
        "plt.ylabel('Frecuencia')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Xhdqy7088fmH"
      },
      "id": "Xhdqy7088fmH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ***Ejercicio:*** Continuando con el ejemplo anterior, del texto que hemos generado, calcularemos la moda de los datos, tanto sin agrupar, como agrupados:\n",
        "\n",
        "# Calcular la tabla de frecuencias de la columna 'Grupo'\n",
        "freq_longitudes = pal_long['longitudes'].value_counts()\n",
        "freq_longitudes = freq_longitudes.sort_index()\n",
        "print(freq_longitudes)\n",
        "\n",
        "longitudes = freq_longitudes.index.tolist()\n",
        "frecuencias = freq_longitudes.values.tolist()\n",
        "\n",
        "# Crear el gr√°fico de barras\n",
        "plt.figure(figsize=(10, 6))  # Ajusta el tama√±o del gr√°fico seg√∫n tus preferencias\n",
        "plt.bar(longitudes, frecuencias, color='skyblue')\n",
        "\n",
        "# Agregar etiquetas y t√≠tulo\n",
        "plt.xlabel('Longitud')\n",
        "plt.ylabel('Frecuencia')"
      ],
      "metadata": {
        "id": "T0vpdY0RLiQ4",
        "cellView": "form"
      },
      "id": "T0vpdY0RLiQ4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Longitudes agrupadas por intervalos: ¬øCu√°l es la moda?\n",
        "display(tabla_frecuencias_eje3)\n",
        "\n",
        "\n",
        "longitudes = tabla_frecuencias_eje3[\"Intervalo\"].tolist()\n",
        "frecuencias = tabla_frecuencias_eje3[\"n_i\"].tolist()\n",
        "\n",
        "# Crear el gr√°fico de barras\n",
        "plt.figure(figsize=(10, 6))  # Ajusta el tama√±o del gr√°fico seg√∫n tus preferencias\n",
        "plt.bar(longitudes, frecuencias, color='skyblue')\n",
        "\n",
        "# Agregar etiquetas y t√≠tulo\n",
        "plt.xlabel('Longitud')\n",
        "plt.ylabel('Frecuencia')\n"
      ],
      "metadata": {
        "id": "jA1iU5KXNNzs",
        "cellView": "form"
      },
      "id": "jA1iU5KXNNzs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Moda pros y contras:\n",
        "\n",
        "### Pros:\n",
        "- Es muy f√°cil de calcular, sobre todo para variables descritas por las frecuencias de sus valores, ya que consiste en identificar el (los) valor(es) con frecuencia m√°xima.\n",
        "- La moda es insensible a datos extremos.\n",
        "- F√°cil interpretaci√≥n\n",
        "- Al depender s√≥lo de la frecuencia, tambi√©n puede ser calculada para variables de tipo cualitativo.\n",
        "\n",
        "### Contras:\n",
        "- Realmente no es una medida de posici√≥n central. Sin embargo, para variables unimodales y con cierta simetr√≠a (esta caracter√≠stica se puede apreciar en el gr√°fico de barras de frecuencias o en el histograma), s√≠ que representa una centralidad.\n",
        "- Cuando los valores de la variable no se repiten, no tiene sentido (no nos da ninguna informaci√≥n) ya que todos los valores son modas.\n",
        "Ejemplo: para la distribuci√≥n $X=(2, 5, 7, 9, 12)$ todos los valores son moda.\n"
      ],
      "metadata": {
        "id": "IHosDESvNtH-"
      },
      "id": "IHosDESvNtH-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "## Medidas de dispersi√≥n absoluta:\n",
        "***\n",
        "\n",
        "Cuantifican cu√°nto ***var√≠a*** la variable de estudio (c√≥mo los valores se distinguen de sus valores centrales).\n",
        "\n",
        "* Geom√©tricamente, la **desviaci√≥n t√≠pica** mide lo lejos que est√° la variable de ser constante, en concreto de tomar siempre el valor $\\bar{X}$:\n",
        "\n",
        "$$S_{X}=\\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}(X_{i}-\\bar{X})^{2}}$$\n",
        "\n",
        "* **Varianza**:\n",
        "Es definida como el cuadrado de la desviaci√≥n t√≠pica.\n",
        "$$Var(X) = S_{X}^{2}=\\frac{1}{N}\\sum_{i=1}^{N}(X_{i}-\\bar{X})^{2}$$\n",
        "\n",
        "donde $N$ es el tama√±o de la poblaci√≥n y $\\bar{X}$ es conocida como la media poblacional.\n"
      ],
      "metadata": {
        "id": "Zuxww05_7H8s"
      },
      "id": "Zuxww05_7H8s"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Supongamos que estamos comparando las puntuaciones en una encuesta de alumnos de dos profesores A y B (Las puntuaciones posibles van desde 0 hasta 10) ¬øQu√© podemos decir de los profesores?:\n",
        "\n",
        "# Semilla aleatoria para reproducibilidad\n",
        "np.random.seed(0)\n",
        "\n",
        "# Generar datos para Dan con alta variabilidad (alta SD)\n",
        "mean_dan = 5.5\n",
        "sd_dan = 2\n",
        "datos_dan = np.random.normal(loc=mean_dan, scale=sd_dan, size=1000)\n",
        "datos_dan = np.clip(datos_dan, 0, 10)  # Limitar los datos al rango de 0 a 10\n",
        "\n",
        "\n",
        "# Generar datos para Mitra con baja variabilidad (baja SD)\n",
        "mean_mitra = 5.5\n",
        "sd_mitra = 0.5\n",
        "datos_mitra = np.random.normal(loc=mean_mitra, scale=sd_mitra, size=1000)\n",
        "datos_mitra = np.clip(datos_mitra, 0, 10)  # Limitar los datos al rango de 0 a 10\n",
        "\n",
        "\n",
        "# Crear gr√°ficos de histograma\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(datos_dan, bins=30, color='skyblue', edgecolor='black')\n",
        "plt.axvline(x=mean_dan, color='red', linestyle='--', linewidth=2, label='Mean: {:.2f}'.format(mean_dan))\n",
        "plt.title('Profesor A (DT={:.2f})'.format(sd_dan))\n",
        "plt.xlabel('Valor')\n",
        "plt.ylabel('Frecuencia')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(datos_mitra, bins=30, color='salmon', edgecolor='black')\n",
        "plt.axvline(x=mean_mitra, color='red', linestyle='--', linewidth=2, label='Mean: {:.2f}'.format(mean_mitra))\n",
        "plt.title('Profesor B (DT={:.2f})'.format(sd_mitra))\n",
        "plt.xlabel('Valor')\n",
        "plt.ylabel('Frecuencia')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SwVks6rCGT0n"
      },
      "id": "SwVks6rCGT0n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ¬øC√≥mo se calcula la varianza?\n",
        "\n",
        "Las siguientes expresiones son equivalentes para calcular la varianza, que dependiendo de la informaci√≥n de la que dispongamos nos pueden facilitar su c√°lculo. Esto nos permite definir la varianza como la diferencia de la media de los cuadrados con respecto al cuadrado de la media:\n",
        "\n",
        "\n",
        "$$Var(X) = S_{X}^{2}=\\frac{1}{N} \\sum_{i=1}^{N}(X_{i}-\\bar{X})^{2} = \\frac{1}{N}\\sum_{i=1}^{N}X_{i}^{2}-\\bar{X}^{2} = \\bar{X^{2}}-\\bar{X}^{2} = \\frac{1}{N}\\Big(\\sum_{i=1}^{N}X_{i}^{2}-\\frac{(\\sum_{i=1}^{N}X_{i})^{2}}{N}\\Big)$$\n",
        "\n",
        "La √∫ltima expresi√≥n es la m√°s recomendable para realizar las cuentas manualmente: requiere la suma y la suma de cuadrados.\n",
        "\n",
        "Si $X$ est√° reducida por intervalos, entonces se aproximan $S_{X}$ y $S_{X}^{2}$ usando las marcas de clase, como en el caso de la media $\\bar{X}$, mediante la siguiente f√≥rmula:\n",
        "\n",
        "\n",
        "$$Var(X)=S_{X}^{2}=\\frac{\\sum_{i=1}^{N}(x_{i}-\\bar{X})^{2}n_{i}}{N},$$\n",
        "\n",
        "donde $x_{i}$ y $n_{i}$ son la marca de clase y la frecuencia absoluta del intervalo $i$, respectivamente.\n",
        "\n",
        "***\n",
        "**Ejemplo base 1**:\n",
        "***\n",
        "\n",
        "Calcular la varianza y la desviaci√≥n t√≠pica de las edades de 5 personas:\n",
        "\n",
        "$25, 30, 35, 40, 45$\n",
        "\n",
        "***\n",
        "\n",
        "\n",
        "**Ejemplo base 2**:\n",
        "\n",
        "Calcular la varianza y la desviaci√≥n t√≠pica de las longitudes de las palabras del texto del ejemplo anterior, agrupadas por intervalos:\n",
        "\n",
        "***\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ruHXrZJVAKpJ"
      },
      "id": "ruHXrZJVAKpJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Longitudes agrupadas por intervalos: ¬øCu√°l es la varianza y la desviaci√≥n t√≠pica?\n",
        "display(tabla_frecuencias_eje3)\n",
        "\n",
        "print(\"Histograma:\")\n",
        "\n",
        "intervalos = tabla_frecuencias_eje3[\"Intervalo\"]\n",
        "puntos_medios = tabla_frecuencias_eje3[\"Marca_de_clase\"]\n",
        "frecuencias = tabla_frecuencias_eje3[\"n_i\"]\n",
        "\n",
        "# Graficar el histograma\n",
        "plt.bar(puntos_medios, frecuencias, width=2.5, color='skyblue', edgecolor='black')\n",
        "\n",
        "# A√±adir etiquetas y t√≠tulo\n",
        "plt.xlabel('Longitud de palabra')\n",
        "plt.ylabel('Frecuencia')\n",
        "plt.title('Histograma de frecuencias de longitud de palabras')\n",
        "\n",
        "# Mostrar el histograma\n",
        "plt.xticks(puntos_medios, intervalos)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "hnd9dLQ4HTSX"
      },
      "id": "hnd9dLQ4HTSX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ***Ejemplo de c√°lculo de varianza y desviaci√≥n t√≠pica:*** El prop√≥sito de este ejercicio es calcular la varianza y la desviaci√≥n est√°ndar de la longitud de las palabras del texto dado. Utilizaremos el mismo texto de ejemplo de los ejercicios anteriores y realizaremos los c√°lculos paso a paso.\n",
        "\n",
        "longitudes=pal_long['longitudes']\n",
        "\n",
        "\n",
        "# display(pal_long[['longitudes']])\n",
        "\n",
        "# #Paso1:\n",
        "# # Calculamos la media\n",
        "# media = sum(longitudes) / len(longitudes)\n",
        "# print(\"Media:\", media)\n",
        "\n",
        "\n",
        "# #Paso2:\n",
        "# # Calculamos la suma de los cuadrados de las diferencias\n",
        "# suma_cuadrados_diferencias = sum((x - media) ** 2 for x in longitudes)\n",
        "# print(\"Suma de los cuadrados de las diferencias:\", suma_cuadrados_diferencias)\n",
        "\n",
        "# #Paso3:\n",
        "\n",
        "# # Calculamos la varianza\n",
        "# varianza = suma_cuadrados_diferencias / len(longitudes)\n",
        "# print(\"Varianza:\", varianza)\n",
        "\n",
        "# # Calculamos la desviaci√≥n est√°ndar\n",
        "# desviacion_estandar = varianza ** 0.5\n",
        "# print(\"Desviaci√≥n est√°ndar:\", desviacion_estandar)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "muRulfIF6OiV",
        "cellView": "form"
      },
      "id": "muRulfIF6OiV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title C√°lculo a trav√©s de funciones de Python (dentro de la librer√≠a Numpy):\n",
        "# Calculamos la varianza y la desviaci√≥n t√≠pica con numpy\n",
        "varianza_np = np.var(longitudes)\n",
        "desviacion_estandar_np = np.std(longitudes)\n",
        "\n",
        "print(\"Varianza (con numpy):\", round(varianza_np,2))\n",
        "print(\"Desviaci√≥n est√°ndar (con numpy):\", round(desviacion_estandar_np,2))"
      ],
      "metadata": {
        "id": "-b_GLOZUY_OH",
        "cellView": "form"
      },
      "id": "-b_GLOZUY_OH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Propiedades de la desviaci√≥n t√≠pica y la varianza:\n",
        "\n",
        "Propiedades de la Varianza:\n",
        "\n",
        "* La varianza es una medida no negativa, es decir, $S^{2}_{X} \\geq 0$ para cualquier variable aleatoria $X$.\n",
        "    \n",
        "* La varianza es cero si y solo si todos los valores de la variable aleatoria son iguales, es decir, $S_{X}^{2} = 0 \\iff X = c$ para alg√∫n $c \\in \\mathbb{R}$.\n",
        "    \n",
        "* La varianza de una constante multiplicada por una variable aleatoria es igual a la constante al cuadrado multiplicada por la varianza de la variable aleatoria, es decir, $S_{cX}^{2} =  c^{2}S^{2}_{X}$.\n",
        "    \n",
        "* La varianza de la suma de dos variables aleatorias independientes es igual a la suma de sus varianzas, es decir, $S^{2}_{X + Y} = S^{2}_{X} + S^{2}_{Y}$ para variables aleatorias $X$ e $Y$ independientes.\n",
        "\n",
        "\n",
        "Propiedades de la Desviaci√≥n T√≠pica:\n",
        "    \n",
        "* Al igual que la varianza, la desviaci√≥n t√≠pica es una medida no negativa, es decir, $S_{X} \\geq 0$.\n",
        "    \n",
        "* La desviaci√≥n t√≠pica mide la dispersi√≥n de los datos alrededor de la media de la misma manera que la varianza, pero est√° en la misma escala que los datos originales, lo que facilita su interpretaci√≥n.\n",
        "    \n",
        "* Las propiedades de la desviaci√≥n t√≠pica son esencialmente las mismas que las de la varianza, ya que la desviaci√≥n t√≠pica es simplemente la ra√≠z cuadrada de la varianza.\n",
        "\n",
        "***Ejemplo:***\n",
        "\n",
        "\n",
        "Supongamos que tenemos una variable aleatoria $X$ que representa el peso de las personas en kilogramos, y $c$ es una constante.\n",
        "\n",
        "Queremos demostrar que la varianza de una constante multiplicada por $X$ es igual a la constante al cuadrado multiplicada por la varianza de $X$, es decir:\n",
        "\n",
        "$$ S_{cX}^{2} =  c^{2}S^{2}_{X} $$\n",
        "\n",
        "Para demostrar esto, consideremos el **siguiente ejemplo**:\n",
        "\n",
        "***\n",
        "\n",
        "Supongamos que tenemos una muestra de pesos de 5 personas: 70, 75, 80, 85 y 90.\n",
        "\n",
        "La media del peso, $\\bar{X}$, es $\\frac{70 + 75 + 80 + 85 + 90}{5} = 80$ kg.\n",
        "\n",
        "La varianza de $X$, $S^{2}_{X}$, se calcula como la media de los cuadrados de las desviaciones de cada peso respecto a la media.\n",
        "\n",
        "\\begin{align*}\n",
        "    S^{2}_{X} &= \\frac{(70 - 80)^2 + (75 - 80)^2 + (80 - 80)^2 + (85 - 80)^2 + (90 - 80)^2}{5} \\\\\n",
        "    &= \\frac{(-10)^2 + (-5)^2 + (0)^2 + (5)^2 + (10)^2}{5} \\\\\n",
        "    &= \\frac{100 + 25 + 0 + 25 + 100}{5} \\\\\n",
        "    &= \\frac{250}{5} \\\\\n",
        "    &= 50 \\text{ kg}^2\n",
        "\\end{align*}\n",
        "\n",
        "Por lo tanto la desviaci√≥n t√≠pica del peso de esta muestra de personas es de $\\sqrt{50}=7.07 kg$.\n",
        "\n",
        "***\n",
        "\n",
        "Supongamos que debido a unos malos h√°bitos alimenticios, las personas ahora pesan el doble ($c=2$).\n",
        "\n",
        "La variable $cX$ representa el doble del peso de cada persona. Entonces, los nuevos pesos ser√≠an: 140 kg, 150 kg, 160 kg, 170 kg y 180 kg.\n",
        "\n",
        "La media del nuevo peso, $\\bar{X}$, es $\\frac{140 + 150 + 160 + 170 + 180}{5} = 160$ kg.\n",
        "\n",
        "La varianza de $cX$, $S^{2}_{cX}$, se calcula de manera similar a $S^{2}_{X}$.\n",
        "\n",
        "\\begin{align*}\n",
        "    S^{2}_{cX} &= \\frac{(140 - 160)^2 + (150 - 160)^2 + (160 - 160)^2 + (170 - 160)^2 + (180 - 160)^2}{5} \\\\\n",
        "    &= \\frac{(-20)^2 + (-10)^2 + (0)^2 + (10)^2 + (20)^2}{5} \\\\\n",
        "    &= \\frac{400 + 100 + 0 + 100 + 400}{5} \\\\\n",
        "    &= \\frac{1000}{5} \\\\\n",
        "    &= 200 \\text{ kg}^2\n",
        "\\end{align*}\n",
        "\n",
        "Por lo tanto la desviaci√≥n t√≠pica del peso de esta muestra de personas es de $\\sqrt{200}=14.14 kg$.\n",
        "\n",
        "\n",
        "Finalmente, verifiquemos la propiedad:\n",
        "\n",
        "$$S^{2}_{cX} =  c^{2}S^{2}_{X} $$\n",
        "$$200 \\text{ kg}^2 = (2^2) \\times 50 \\text{ kg}^2$$\n",
        "$$ 200 \\text{ kg}^2 = 4 \\times 50 \\text{ kg}^2$$\n",
        "$$ 200 \\text{ kg}^2 = 200 \\text{ kg}^2 $$\n",
        "\n",
        "\n",
        "**¬øQu√© implica la desviaci√≥n t√≠pica en cada caso?**\n"
      ],
      "metadata": {
        "id": "CPeplhRuLVHj"
      },
      "id": "CPeplhRuLVHj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cuasivarianza y cuasidesviaci√≥n t√≠pica\n",
        "La cuasivarianza y la cuasidesviaci√≥n t√≠pica se definen de manera muy similar a la sus versiones *plenas*. La √∫nica diferencia es que, en este caso, se divide por $N-1$, donde $N$ es el tama√±o de la mustra y se suelen usar en inferencia estad√≠stica por sus buenas propiedades como estimadores no sesgados de la varianza y la desviaci√≥n t√≠pica poblacionales.\n",
        "\n",
        "$$S_{uX}=\\sqrt{\\frac{1}{(N-1)}\\sum_{i=1}^{N}(X_{i}-\\bar{X})^{2}}$$\n",
        "$$Var(X) = S_{uX}^{2}=\\frac{1}{(N-1)}\\sum_{i=1}^{N}(X_{i}-\\bar{X})^{2},$$\n",
        "donde $\\bar{X}$ es la media aritm√©tica de la variable.\n",
        "\n",
        "- Las propiedades son las mismas que en el caso de la varianza y la desviaci√≥n t√≠pica.\n",
        "<!-- <font color='red'>bar</font> -->"
      ],
      "metadata": {
        "id": "Su_QnUuGvsp3"
      },
      "id": "Su_QnUuGvsp3"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Calculamos la (cuasi) varianza y la (cuasi) desviaci√≥n t√≠pica con numpy\n",
        "varianza_np = np.var(longitudes, ddof=1) #con ddof=1 es para ajustar el divisor a n-1\n",
        "desviacion_estandar_np = np.std(longitudes, ddof=1)\n",
        "\n",
        "print(\"Varianza (con numpy):\", round(varianza_np,2))\n",
        "print(\"Desviaci√≥n est√°ndar (con numpy):\", round(desviacion_estandar_np,2))\n"
      ],
      "metadata": {
        "id": "BOPuok7PJ2XN",
        "cellView": "form"
      },
      "id": "BOPuok7PJ2XN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "## An√°lisis de frecuencia de los N-gramas:\n",
        "***\n",
        "\n",
        "El an√°lisis de frecuencia de los n-gramas es una t√©cnica fundamental en el (PLN) que se utiliza para analizar la frecuencia de ocurrencia de secuencias de $n$ elementos, conocidas como n-gramas, en un texto. Los n-gramas son ampliamente utilizados en tareas como la modelizaci√≥n del lenguaje.\n",
        "\n",
        "**Definici√≥n:**\n",
        "Un n-grama es una secuencia contigua de $n$ elementos de un texto o una cadena de caracteres. Los elementos pueden ser palabras, caracteres, s√≠labas, entre otros. Por ejemplo, en la oraci√≥n *El perro corre r√°pidamente*, algunos ejemplos de n-gramas ser√≠an los siguientes:\n",
        "\n",
        "* Unigramas (1-gramas): El, perro, corre, r√°pidamente.\n",
        "* Bigramas (2-gramas): El perro, perro corre, corre r√°pidamente.\n",
        "* Trigramas (3-gramas): El perro corre, perro corre r√°pidamente.\n",
        "\n",
        "\n",
        "**C√°lculo de Frecuencia:**\n",
        "\n",
        "Para calcular la frecuencia de los n-gramas de palabras en un texto, se cuentan las ocurrencias de cada n-grama y se registran en una tabla. Luego, se puede calcular la frecuencia relativa de cada n-grama dividiendo el n√∫mero de ocurrencias de ese n-grama por el n√∫mero total de n-gramas en el texto.\n",
        "\n",
        "**Implementaci√≥n en Python:**\n",
        "\n",
        "A continuaci√≥n, se muestra una implementaci√≥n simple en Python para calcular la frecuencia de los n-gramas en un texto utilizando la biblioteca NLTK (Natural Language Toolkit):\n",
        "\n",
        "Este script tomar√° el texto de ejemplo, lo tokenizar√° en palabras, calcular√° los bigramas y mostrar√° la frecuencia de cada bigrama en el texto.\n"
      ],
      "metadata": {
        "id": "y6QUj4YZ3T-F"
      },
      "id": "y6QUj4YZ3T-F"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Ejemplo **n-gramas** $1$:\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"Natural language processing is a field of study focused on the interactions between human language and computers. Natural language processing is a very important area in AI\"\n",
        "\n",
        "cl_sentence = limpiar_texto(sentence,\"english\")\n",
        "\n",
        "print(f\"El texto limpio es:\\n{cl_sentence}\")\n",
        "\n",
        "# first get individual words\n",
        "tokenized = cl_sentence.split()\n",
        "\n",
        "com = 16\n",
        "\n",
        "#unigramas\n",
        "n=1\n",
        "esUnigrams = ngrams(tokenized,n)\n",
        "\n",
        "# get the frequency of each bigram in our corpus\n",
        "esUnigramsFreq = collections.Counter(esUnigrams)\n",
        "\n",
        "print(f\"\\n---------------\\nN√∫mero total de Unigramas posibles: {len(list(esUnigramsFreq.elements()))}\")\n",
        "print(f\"\\n---------------\\nN√∫mero √∫nico de Unigramas: {len(list(set(esUnigramsFreq.elements())))}\")\n",
        "\n",
        "# what are the ten most popular ngrams in this Spanish corpus?\n",
        "print(f\"Unigramas:\\n{esUnigramsFreq.most_common(com)}\")\n",
        "\n",
        "#bigramas\n",
        "# and get a list of all the bi-grams\n",
        "n=2\n",
        "esBigrams = ngrams(tokenized, n)\n",
        "\n",
        "# get the frequency of each bigram in our corpus\n",
        "esBigramFreq = collections.Counter(esBigrams)\n",
        "\n",
        "\n",
        "print(f\"\\n---------------\\nN√∫mero total de Unigramas posibles: {len(list(esBigramFreq.elements()))}\")\n",
        "print(f\"\\n---------------\\nN√∫mero √∫nico de Unigramas: {len(list(set(esBigramFreq.elements())))}\")\n",
        "\n",
        "# what are the ten most popular ngrams in this Spanish corpus?\n",
        "print(f\"\\nBigramas:\\n{esBigramFreq.most_common(com)}\")\n",
        "\n",
        "\n",
        "#trigramas:\n",
        "# and get a list of all the bi-grams\n",
        "n=3\n",
        "esTrigrams = ngrams(tokenized, n)\n",
        "\n",
        "# get the frequency of each bigram in our corpus\n",
        "esTrigramsFreq = collections.Counter(esTrigrams)\n",
        "\n",
        "print(f\"\\n---------------\\nN√∫mero total de Unigramas posibles: {len(list(esTrigramsFreq.elements()))}\")\n",
        "print(f\"\\n---------------\\nN√∫mero √∫nico de Unigramas: {len(list(set(esTrigramsFreq.elements())))}\")\n",
        "\n",
        "# what are the ten most popular ngrams in this Spanish corpus?\n",
        "print(f\"\\nTrigramas:\\n{esTrigramsFreq.most_common(com)}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "YGVEDPYzWy-l"
      },
      "id": "YGVEDPYzWy-l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "***"
      ],
      "metadata": {
        "id": "xrumT6ymmPCD"
      },
      "id": "xrumT6ymmPCD"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Crea frecuencia de los distintos **n-gramas** (1-2-3) teniendo en cuenta las rese√±as, discriminadas por **Positivas** y **Negativas**:\n",
        "\n",
        "#Vamos a comparar las palabras m√°s comunes en las rese√±as consideradas positivas vs las rese√±as consideradas negativas:\n",
        "\n",
        "#Para esto, lo primero que hacemos es crear dos bases diferentes:\n",
        "\n",
        "#Recordemos que la categor√≠a de los sentimientos son: ['Bueno (4-5)', 'Neutral (3)', 'Malo (1-2)']\n",
        "\n",
        "positivos = df.loc[df.sentiment=='Bueno (4-5)',]\n",
        "negativos = df.loc[df.sentiment=='Malo (1-2)',]\n",
        "\n",
        "positivos['Text'] = positivos['Text'].swifter.apply(limpiar_texto)\n",
        "negativos['Text'] = negativos['Text'].swifter.apply(limpiar_texto)\n",
        "\n",
        "cachedStopWords = nlp_en.Defaults.stop_words\n",
        "cachedStopWords = [x.lower() for x in cachedStopWords]\n",
        "cachedStopWords.extend(list(stopwords.words('english')))\n",
        "cachedStopWords = list(set(cachedStopWords))\n",
        "\n",
        "positivos['Text'] = positivos['Text'].swifter.apply(remove_stopwords)\n",
        "negativos['Text'] = negativos['Text'].swifter.apply(remove_stopwords)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rIR233PgJNus"
      },
      "id": "rIR233PgJNus",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Rese√±as positivas**:\n",
        "print(\"\\n-------------------\\nAn√°lisis de frecuencia de las palabras de las rese√±as categorizadas como Positivas\\n-------------------\\n\")\n",
        "\n",
        "print(\"\\n-------------------\\nUnigramas Positivos\\n-------------------\\n\")\n",
        "\n",
        "n=1\n",
        "positivos[\"Unigram_text\"] = positivos[\"Text\"].swifter.apply(get_ngrams, n=n)\n",
        "resena_string_list = positivos['Unigram_text'].tolist()\n",
        "resena_string = ' '.join(resena_string_list)\n",
        "\n",
        "# Calcular la frecuencia de los unigramas\n",
        "unigram_freq_dist = FreqDist(resena_string.split())\n",
        "\n",
        "# Ordenar la tabla de frecuencias por frecuencia en orden descendente\n",
        "sorted_freq_dist = sorted(unigram_freq_dist.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "total_unigrams=len(sorted_freq_dist)\n",
        "print(f\"\\n---------------\\nN√∫mero √∫nico de unigramas posibles: {total_unigrams}\")\n",
        "suma=sum(freq for _, freq in sorted_freq_dist)\n",
        "print(f\"\\n---------------\\nN√∫mero total de unigramas es: {suma}\\n\")\n",
        "\n",
        "\n",
        "# Imprimir solo los primeros 20 unigramas con mayor frecuencia\n",
        "print(\"Los 20 unigramas m√°s frecuentes:\")\n",
        "for word, frequency in sorted_freq_dist[:20]:\n",
        "    print(f\"{word}: {frequency}\")\n",
        "\n",
        "\n",
        "wordcloud = WordCloud(width = int(2000/2), height = int(1334/2), random_state=1, background_color='black', colormap='Pastel1', max_words = 20, collocations=False, normalize_plurals=False).generate(resena_string)\n",
        "plot_cloud(wordcloud)\n",
        "\n",
        "\n",
        "print(\"\\n-------------------\\nBigramas Positivos\\n-------------------\\n\")\n",
        "n=2\n",
        "positivos[\"Bigram_text\"] = positivos[\"Text\"].swifter.apply(get_ngrams, n=n)\n",
        "resena_string_list = positivos['Bigram_text'].tolist()\n",
        "resena_string = ' '.join(resena_string_list)\n",
        "\n",
        "\n",
        "# Calcular la frecuencia de los Bigramas\n",
        "Bigram_freq_dist = FreqDist(resena_string.split())\n",
        "\n",
        "# Ordenar la tabla de frecuencias por frecuencia en orden descendente\n",
        "sorted_freq_dist = sorted(Bigram_freq_dist.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "total_bigrams=len(sorted_freq_dist)\n",
        "print(f\"\\n---------------\\nN√∫mero √∫nico de bigramas posibles: {total_bigrams}\")\n",
        "suma=sum(freq for _, freq in sorted_freq_dist)\n",
        "print(f\"\\n---------------\\nN√∫mero total de bigramas es: {suma}\\n\")\n",
        "\n",
        "print(\"Los 20 Bigramas m√°s frecuentes:\")\n",
        "for word, frequency in sorted_freq_dist[:20]:\n",
        "    print(f\"{word}: {frequency}\")\n",
        "\n",
        "\n",
        "wordcloud = WordCloud(width = int(2000/2), height = int(1334/2), random_state=1, background_color='black', colormap='Pastel1', max_words = 20, collocations=False, normalize_plurals=False).generate(resena_string)\n",
        "plot_cloud(wordcloud)\n",
        "\n",
        "print(\"\\n-------------------\\nTrigramas Positivos\\n-------------------\\n\")\n",
        "n=3\n",
        "positivos[\"Trigram_text\"] = positivos[\"Text\"].swifter.apply(get_ngrams, n=n)\n",
        "resena_string_list = positivos['Trigram_text'].tolist()\n",
        "resena_string = ' '.join(resena_string_list)\n",
        "\n",
        "# Calcular la frecuencia de los Trigramas\n",
        "Trigram_freq_dist = FreqDist(resena_string.split())\n",
        "\n",
        "# Ordenar la tabla de frecuencias por frecuencia en orden descendente\n",
        "sorted_freq_dist = sorted(Trigram_freq_dist.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "\n",
        "total_trigrams=len(sorted_freq_dist)\n",
        "print(f\"\\n---------------\\nN√∫mero √∫nico de trigramas posibles: {total_trigrams}\")\n",
        "suma=sum(freq for _, freq in sorted_freq_dist)\n",
        "print(f\"\\n---------------\\nN√∫mero total de trigramas es: {suma}\\n\")\n",
        "\n",
        "print(\"Los 20 Trigramas m√°s frecuentes:\")\n",
        "for word, frequency in sorted_freq_dist[:20]:\n",
        "    print(f\"{word}: {frequency}\")\n",
        "\n",
        "\n",
        "wordcloud = WordCloud(width = int(2000/2), height = int(1334/2), random_state=1, background_color='black', colormap='Pastel1', max_words = 20, collocations=False, normalize_plurals=False).generate(resena_string)\n",
        "plot_cloud(wordcloud)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "VNXJ8WHJPvMg"
      },
      "id": "VNXJ8WHJPvMg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Rese√±as Negativas**:\n",
        "print(\"\\n-------------------\\nAn√°lisis de frecuencia de las palabras de las rese√±as categorizadas como Negativas\\n-------------------\\n\")\n",
        "\n",
        "print(\"\\n-------------------\\nUnigramas Negativos\\n-------------------\\n\")\n",
        "\n",
        "negativos[\"Unigram_text\"] = negativos[\"Text\"].swifter.apply(get_ngrams, n=1)\n",
        "resena_string_list = negativos['Unigram_text'].tolist()\n",
        "resena_string = ' '.join(resena_string_list)\n",
        "\n",
        "\n",
        "# Calcular la frecuencia de los unigramas\n",
        "unigram_freq_dist = FreqDist(resena_string.split())\n",
        "\n",
        "# Ordenar la tabla de frecuencias por frecuencia en orden descendente\n",
        "sorted_freq_dist = sorted(unigram_freq_dist.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Calcular el n√∫mero de unigramas posibles\n",
        "total_unigrams=len(sorted_freq_dist)\n",
        "print(f\"\\n---------------\\nN√∫mero √∫nico de unigramas posibles: {total_unigrams}\")\n",
        "suma=sum(freq for _, freq in sorted_freq_dist)\n",
        "print(f\"\\n---------------\\nN√∫mero total de unigramas es: {suma}\\n\")\n",
        "\n",
        "# Imprimir solo los primeros 10-15 unigramas con mayor frecuencia\n",
        "print(\"Los 20 unigramas m√°s frecuentes:\")\n",
        "for word, frequency in sorted_freq_dist[:20]:\n",
        "    print(f\"{word}: {frequency}\")\n",
        "\n",
        "\n",
        "wordcloud = WordCloud(width = int(2000/2), height = int(1334/2), random_state=1, background_color='black', colormap='Pastel1', max_words = 20, collocations=False, normalize_plurals=False).generate(resena_string)\n",
        "plot_cloud(wordcloud)\n",
        "\n",
        "\n",
        "print(\"\\n-------------------\\nBigramas Negativos\\n-------------------\\n\")\n",
        "\n",
        "negativos[\"Bigram_text\"] = negativos[\"Text\"].swifter.apply(get_ngrams, n=2)\n",
        "resena_string_list = negativos['Bigram_text'].tolist()\n",
        "resena_string = ' '.join(resena_string_list)\n",
        "\n",
        "# Calcular la frecuencia de los Bigramas\n",
        "Bigram_freq_dist = FreqDist(resena_string.split())\n",
        "\n",
        "# Ordenar la tabla de frecuencias por frecuencia en orden descendente\n",
        "sorted_freq_dist = sorted(Bigram_freq_dist.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "\n",
        "total_bigrams=len(sorted_freq_dist)\n",
        "print(f\"\\n---------------\\nN√∫mero √∫nico de bigramas posibles: {total_bigrams}\")\n",
        "suma=sum(freq for _, freq in sorted_freq_dist)\n",
        "print(f\"\\n---------------\\nN√∫mero total de bigramas es: {suma}\\n\")\n",
        "\n",
        "print(\"Los 20 Bigramas m√°s frecuentes:\")\n",
        "for word, frequency in sorted_freq_dist[:20]:\n",
        "    print(f\"{word}: {frequency}\")\n",
        "\n",
        "\n",
        "wordcloud = WordCloud(width = int(2000/2), height = int(1334/2), random_state=1, background_color='black', colormap='Pastel1', max_words = 20, collocations=False, normalize_plurals=False).generate(resena_string)\n",
        "plot_cloud(wordcloud)\n",
        "\n",
        "print(\"\\n-------------------\\nTrigramas Negativos\\n-------------------\\n\")\n",
        "\n",
        "negativos[\"Trigram_text\"] = negativos[\"Text\"].swifter.apply(get_ngrams, n=3)\n",
        "resena_string_list = negativos['Trigram_text'].tolist()\n",
        "resena_string = ' '.join(resena_string_list)\n",
        "\n",
        "# Calcular la frecuencia de los Trigramas\n",
        "Trigram_freq_dist = FreqDist(resena_string.split())\n",
        "\n",
        "# Ordenar la tabla de frecuencias por frecuencia en orden descendente\n",
        "sorted_freq_dist = sorted(Trigram_freq_dist.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "total_trigrams=len(sorted_freq_dist)\n",
        "print(f\"\\n---------------\\nN√∫mero √∫nico de trigramas posibles: {total_trigrams}\")\n",
        "suma=sum(freq for _, freq in sorted_freq_dist)\n",
        "print(f\"\\n---------------\\nN√∫mero total de trigramas es: {suma}\\n\")\n",
        "\n",
        "print(\"Los 20 Trigramas m√°s frecuentes:\")\n",
        "for word, frequency in sorted_freq_dist[:20]:\n",
        "    print(f\"{word}: {frequency}\")\n",
        "\n",
        "\n",
        "wordcloud = WordCloud(width = int(2000/2), height = int(1334/2), random_state=1, background_color='black', colormap='Pastel1', max_words = 20, collocations=False, normalize_plurals=False).generate(resena_string)\n",
        "plot_cloud(wordcloud)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mDGqjJFuZaJX"
      },
      "id": "mDGqjJFuZaJX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "# Probabilidad ¬°La clave!\n",
        "***"
      ],
      "metadata": {
        "id": "tY-CrT1IUku9"
      },
      "id": "tY-CrT1IUku9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<center>\n",
        " <img src=\"https://github.com/ednavivianasegura/Curso_PLN/blob/main/prob.png?raw=1\" alt=\"prob\" width=\"50%\" height=\"50%\">\n",
        "</center>\n",
        "\n",
        "**Motivaci√≥n**\n",
        "\n",
        "Frente a un fen√≥meno aleatorio, el resultado es incierto y su predicci√≥n se vuelve un desaf√≠o. La probabilidad se convierte en en una **br√∫jula** en este territorio de incertidumbre.\n",
        "\n",
        "**Analicemos ejemplos:**\n",
        "\n",
        "* ***Juegos de azar casinos:***\n",
        "\n",
        "Los casinos utilizan la probabilidad para calcular las posibilidades de ganar en los juegos de azar, asegurando que tengan una ventaja matem√°tica.\n",
        "\n",
        "\n",
        "* ***Esperanza de vida:***\n",
        "\n",
        "Se calcula bas√°ndose en la probabilidad de muerte o vida de una poblaci√≥n, utilizando datos demogr√°ficos y estad√≠sticos.\n",
        "\n",
        "\n",
        "* ***Primas de seguros:***\n",
        "\n",
        "Las compa√±√≠as de seguros analizan datos como la edad y el historial del cliente para determinar las primas, considerando la probabilidad de accidentes u enfermedades.\n",
        "\n",
        "\n",
        "* ***¬øCu√°l ser√° el siguiente t√©rmino en una secuencia de palabras generada por un modelo de lenguaje?***\n",
        "\n",
        "Aunque no podemos predecir exactamente qu√© palabra seguir√°, podemos calcular la probabilidad condicionada de que una palabra determinada siga a otra dentro del contexto de la secuencia.  As√≠, podr√≠amos estimar que la siguiente palabra sea la que tenga mayor probabilidad.\n",
        "\n",
        "\n",
        "* ***¬øEs Spam?***\n",
        "\n",
        "Aunque juzgar si un mensajes es *Spam* se puede realizar con cierta sencillez. Lo quese busca es que un **servicio de mensajer√≠a** sea capaz de catalogar apropiadamente un mensaje como spam para que no aparezca en la bandeja de entrada.\n",
        "Esto es posible, por medio de modelos que estimen la probabilidad asociada a la condici√≥n de ser *spam*.\n"
      ],
      "metadata": {
        "id": "HHmzlP6tcWTf"
      },
      "id": "HHmzlP6tcWTf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conceptos:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kOkS8p8Tendr"
      },
      "id": "kOkS8p8Tendr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Distribuci√≥n de probabilidad:**\n",
        "\n",
        "La distribuci√≥n de probabilidad es una funci√≥n matem√°tica que describe las posibles ocurrencias de un evento y la probabilidad asociada a cada una de ellas.\n",
        "\n",
        "Una **distribuci√≥n de probabilidad** es una funci√≥n $ P: \\{ sucesos \\} \\rightarrow [0,1]$.\n",
        "\n",
        "Denotamos por $\\Omega$ al conjunto total de resultados posibles, donde cada elemento en $ \\Omega$ se llama suceso elemental.\n",
        "\n",
        "El suceso imposible se denota como $\\emptyset$.\n",
        "\n",
        "Un suceso es un conjunto formado mediante la uni√≥n, intersecci√≥n o complementaci√≥n de **sucesos elementales**.\n",
        "\n",
        "Si $A$ es un **suceso**, $P(A)$  es la probabilidad de $A$, donde $ 0 \\leq P(A) \\leq 1$.\n",
        "\n",
        "Para dos sucesos $A$ y $B$:\n",
        "* $A \\cup B $ representa la uni√≥n de $A$ y $B$, se cumple si al menos uno de los dos sucesos ocurre.\n",
        "* $A \\cap B$ representa la intersecci√≥n de $A$ y $B$, se cumple si ambos sucesos ocurren.\n",
        "* $A^c$ es el complemento de $A$: se cumple exactamente cuando $A$ no ocurre.\n",
        "\n",
        "**Propiedades:**\n",
        "Una distribuci√≥n de probabilidad $P(X)$ para una variable aleatoria discreta $X$ debe satisfacer las siguientes propiedades:\n",
        "1. $0 \\leq P(X = x_i) \\leq 1$ para todo valor de $x_i$.\n",
        "2. La suma de las probabilidades para todos los posibles valores de $X$ es igual a 1:\n",
        "$ \\sum_{i} P(X = x_i) = 1 $.\n",
        "\n",
        "Adem√°s:\n",
        "\n",
        "*  El **suceso total** es $P(\\Omega)=1$.\n",
        "*  $P(\\emptyset)=0$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1RvMtf0oYYWk"
      },
      "id": "1RvMtf0oYYWk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regla de Laplace"
      ],
      "metadata": {
        "id": "EcqX30fSYu6_"
      },
      "id": "EcqX30fSYu6_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el caso en el que los sucesos elementales sean equiprobables, podemos calcular la probabilidad de un suceso $A$ de la siguiente forma:\n",
        "\n",
        "$$P(A)=\\frac{|A|}{|\\Omega|} = \\frac{\\text{# de casos favorables}}{\\text{# de casos posibles}}$$\n",
        "\n",
        "\n",
        "\n",
        "***\n",
        "**Ejemplos de sucesos**:\n",
        "***\n",
        "\n",
        "1. Sorteo de 3 n√∫meros:\n",
        "\n",
        "    **Suceso elemental**: Obtener un 1, un 2 o un 3, en un sorteo, cada uno con probabilidad 1/3\n",
        "\n",
        "    **Suceso**: Obtener un n√∫mero  mayor que 1, entonces A={2,3}\n",
        "\n",
        "2. Sacar una carta de una baraja de 52 cartas:\n",
        "\n",
        "    **Suceso elemental**: Sacar una carta espec√≠fica como el 3 de corazones, el rey de espadas, etc, cada uno con probabilidad 1/52\n",
        "\n",
        "    **Suceso**: Sacar una carta roja (suceso A), sacar una carta negra (suceso B).\n",
        "\n",
        "\n",
        "\n",
        "***\n",
        "**Ejercicios**:\n",
        "***\n",
        "\n",
        "En el experimento **lanzar un dado equilibrado**:\n",
        "\n",
        "1. ¬øCu√°les son los sucesos elementales?\n",
        "2. ¬øQu√© probabilidad tiene cada suceso elemental?\n",
        "3. $A$ es el evento de obtener un n√∫mero par al lanzar un dado y $B$ es el evento de obtener un n√∫mero mayor que 3, ¬øQu√© elementos pertenecen a $A$ y cuales a $B$?\n",
        "4. Para los mismos $A$ y $B$, ¬øcu√°l es la uni√≥n de ambos sucesos $A \\cup B $?\n",
        "5. Para los mismos $A$ y $B$, ¬øcu√°l es la intersecci√≥n de ambos sucesos $A \\cap B $?\n",
        "6. ¬øQu√© elementos pertenecen al suceso $A^{c}$.\n",
        "7. Calcular:\n",
        "    * $P(A)$\n",
        "    * $P(B)$\n",
        "    * $P(A \\cup B)$\n",
        "    * $P(A \\cap B)$\n",
        "    * $P(A^{c})$"
      ],
      "metadata": {
        "id": "1E5GVENnNNO6"
      },
      "id": "1E5GVENnNNO6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ¬øSucesos NO EQUIPROBABLES?:\n",
        "\n",
        "Supongamos que en una urna hay bolas de varios colores:\n",
        "\n",
        "* 3 Rojas\n",
        "* 4 Azules\n",
        "* 2 Verdes\n",
        "\n",
        "El experimento consiste en extraer una bola al azar y anotar su color.\n",
        "\n",
        "* ¬øCu√°l es el espacio muestral?\n",
        "\n",
        "* ¬øQu√© probabilidad tiene cada suceso elemental?\n",
        "\n",
        "* ¬øQu√© posibles sucesos podemos encontrar?\n"
      ],
      "metadata": {
        "id": "aYz9J4e9y_y4"
      },
      "id": "aYz9J4e9y_y4"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Crear la urna:\n",
        "from matplotlib.patches import Rectangle\n",
        "# Definimos los colores y la cantidad de bolas en la urna\n",
        "verde = [(-3,3),(-1,0)]\n",
        "azul  = [(0,4),(0,1),(3,-2),(-2,0)]\n",
        "rojo  = [(1,0),(0,-1),(2,2)]\n",
        "\n",
        "# Creamos el gr√°fico de dispersi√≥n sin ejes\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "for v in verde:\n",
        "    plt.scatter(v[0], v[1], color=\"g\", marker='o', s=400)\n",
        "\n",
        "for a in azul:\n",
        "    plt.scatter(a[0], a[1], color=\"b\", marker='o', s=400)\n",
        "\n",
        "for r in rojo:\n",
        "    plt.scatter(r[0], r[1], color=\"r\", marker='o', s=400)\n",
        "\n",
        "\n",
        "# Agregamos un cuadrado negro alrededor del gr√°fico\n",
        "plt.gca().add_patch(Rectangle((-3.5, -3), 8, 8, linewidth=2, edgecolor='black', fill=None))\n",
        "\n",
        "plt.text(3.7,4,r\"$\\Omega$\", size=22)\n",
        "\n",
        "# Eliminamos los ejes\n",
        "plt.gca().spines['top'].set_visible(False)\n",
        "plt.gca().spines['right'].set_visible(False)\n",
        "plt.gca().spines['left'].set_visible(False)\n",
        "plt.gca().spines['bottom'].set_visible(False)\n",
        "\n",
        "# Eliminamos los ticks y los ticks labels\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "\n",
        "# Mostramos el gr√°fico\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SQS4FAOczvS1"
      },
      "id": "SQS4FAOczvS1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ***Ejemplo 1 (PLN):*** Tomando como espacio de sucesos el texto que venimos trabajando, ¬øcu√°l es la probabilidad de que una palabra elegida al azar tenga longitud mayor o igual a 10?\n",
        "\n",
        "print(\"Todas las palabras tienen la misma probabilidad de ser escogidas\\n\")\n",
        "\n",
        "display(texto_limpio)\n",
        "\n",
        "# Dividir el texto en palabras\n",
        "palabras = texto_limpio.split()\n",
        "\n",
        "# Contar cu√°ntas palabras tienen una longitud mayor o igual a 10 caracteres\n",
        "num_palabras_largas = sum(1 for palabra in palabras if len(palabra) >= 10)\n",
        "\n",
        "# Calcular la probabilidad\n",
        "\n",
        "probabilidad = num_palabras_largas / len(palabras)\n",
        "\n",
        "print(\"\\nN√∫mero total de palabras (# de casos posibles):\", len(palabras))\n",
        "print(\"N√∫mero de palabras con longitud mayor o igual a 10 (# de casos favorables):\", num_palabras_largas)\n",
        "print(\"Probabilidad de seleccionar una palabra de longitud mayor o igual a 10:\", round(probabilidad,2), \" o \", round(probabilidad*100,2),\"%\")\n"
      ],
      "metadata": {
        "id": "9wwgb5te4Iju",
        "cellView": "form"
      },
      "id": "9wwgb5te4Iju",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejemplo 2 (PLN):\n",
        "¬øCu√°l es la probabilidad de que al seleccionar una palabra al azar del texto de ejemplo, comience por la letra *e* sabiendo que hay 119 palabras, de las cales 11 comienzan por \"e\"."
      ],
      "metadata": {
        "id": "jcO-PFSIU_Tm"
      },
      "id": "jcO-PFSIU_Tm"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Respuesta:\n",
        "# Dividir el texto en palabras\n",
        "palabras = texto_limpio.split()\n",
        "# Contar cu√°ntas palabras comienzan con la letra \"e\"\n",
        "num_palabras_e = sum(1 for palabra in palabras if palabra.lower().startswith('e'))\n",
        "\n",
        "# Calcular la probabilidad\n",
        "probabilidad_palabra_e = num_palabras_e / len(palabras)\n",
        "\n",
        "print(\"N√∫mero total de palabras:\", len(palabras))\n",
        "print(\"N√∫mero de palabras que comienzan con 'e':\", num_palabras_e)\n",
        "print(\"Probabilidad de que una palabra comience con 'e':\", round(probabilidad_palabra_e,4), \"o\", round(probabilidad_palabra_e*100,2),\"%\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nQuitando las palabras repeetidas:\\n\")\n",
        "\n",
        "palabras_unicas = set(palabras)\n",
        "\n",
        "# Contar cu√°ntas palabras comienzan con la letra \"e\"\n",
        "num_palabras_e = sum(1 for palabra in palabras_unicas if palabra.lower().startswith('e'))\n",
        "\n",
        "# Calcular la probabilidad\n",
        "probabilidad_palabra_e = num_palabras_e / len(palabras_unicas)\n",
        "print(\"N√∫mero de palabras √∫nicas:\", len(palabras_unicas))\n",
        "print(\"N√∫mero de palabras que comienzan con 'e' en el texto con palabras √∫nicas:\", num_palabras_e)\n",
        "print(\"Probabilidad de que una palabra comience con 'e' dentro del conjunto de palabras √∫nicas:\", round(probabilidad_palabra_e,4), \"o\", round(probabilidad_palabra_e*100,2),\"%\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fy1aUg1XbA8W"
      },
      "id": "fy1aUg1XbA8W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpretaci√≥n frecuentista\n",
        "\n",
        "La probabilidad emp√≠rica o interpretaci√≥n frecuentista de la probabilidad afirma que si repetimos el *experimento* $N$ veces (bajo las mismas condiciones), si la frecuencia de un suceso $A$ es $n_{A}$, entonces cuando $N$ es muy grande, la frecuencia relativa de $A$ se aproxima a la probabilidad de $A$:\n",
        "\n",
        "$$\\frac{n_A}{N}\\xrightarrow[N \\rightarrow +\\infty]{} P(A)$$\n",
        "\n",
        "\n",
        "***\n",
        "**Ejemplo**\n",
        "***\n",
        "Si lanzamos un dado un mill√≥n de veces, arpoximadamente la mitad de ellas sacaremos un n√∫mero par, ya que:\n",
        "\n",
        "$$A=\\{''\\text{sacar par}''\\} = \\{2,4,6\\} \\rightarrow P(A) = \\frac{3}{6}=\\frac{1}{2}$$"
      ],
      "metadata": {
        "id": "ysMNK2K4-2wC"
      },
      "id": "ysMNK2K4-2wC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "***\n",
        "\n",
        "### Paradoja de Montyhall\n",
        "\n",
        "***\n",
        "***\n",
        "\n",
        "<center>\n",
        " <img src=\"https://github.com/ednavivianasegura/Curso_PLN/blob/main/monty.png?raw=1\" alt=\"prob\" width=\"50%\" height=\"50%\">\n",
        "</center>\n",
        "\n",
        "\n",
        "***\n",
        "***"
      ],
      "metadata": {
        "id": "_lMAS6ObblIg"
      },
      "id": "_lMAS6ObblIg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Probabilidad condicionada:**\n",
        "\n",
        "Dados dos sucesos, $A$ y $B$, si $n_{A \\cap B}$ es la frecuencia de resultados en los que se cumplen $A$  y $B$ a la vez en $N$ *experimentos*, entonces $\\frac{n_{A \\cap B}}{n_{B}}$ expresa la proporci√≥n de casos en los que sucede $A$ entre los que sucede $B$.\n",
        "\n",
        "Como $$\\frac{n_{A \\cap B}}{n_{B}} = \\frac{n_{A \\cap B}/N}{n_{B}/N} \\xrightarrow[N \\rightarrow +\\infty]{}\\frac{P(A \\cap B)}{P(B)},$$\n",
        "\n",
        "se define la **probabilidad de $A$ condiciona a $B$** como la probabilidad de $A$ si damos por supuesto que sucede $B$, esto es:\n",
        "\n",
        "$$P(A|B)=\\frac{P(A \\cap B)}{P(B)}.$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tVcQ7dPh3-N8"
      },
      "id": "tVcQ7dPh3-N8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ejemplo**\n",
        "\n",
        "Supongamos que contamos con los datos de las frecuencias de estudiantes de √∫ltimo a√±o de una escuela, discriminados por sexo y por el hecho de si trabajan o no trabajan.\n",
        "\n",
        "<table border=\"1\">\n",
        "  <tr>\n",
        "    <th></th>\n",
        "    <th>Trabaja</th>\n",
        "    <th>No Trabaja</th>\n",
        "    <th>Total</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <th>Hombre</th>\n",
        "    <td>60</td>\n",
        "    <td>40</td>\n",
        "    <td>100</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <th>Mujer</th>\n",
        "    <td>20</td>\n",
        "    <td>110</td>\n",
        "    <td>130</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <th>Total</th>\n",
        "    <td>80</td>\n",
        "    <td>150</td>\n",
        "    <td>230</td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "¬øCu√°l es la probabilidad de que si, se escoge al azar un alumno hombre, √©ste trabaje?"
      ],
      "metadata": {
        "id": "30HDT70keIuy"
      },
      "id": "30HDT70keIuy"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ***Ejemplo:*** Supongamos que queremos calcular la probabilidad de que al seleccionar al azar, de nuestro texto de ejemplo, una palabra de longitud mayor o igual a *n_largas* caracteres, √©sta comience con la letra *letra*. Contamos con la siguiente informaci√≥n:\n",
        "\n",
        "# Dividir el texto en palabras\n",
        "palabras = texto_limpio.split()\n",
        "\n",
        "n_largas= 7\n",
        "letra=\"p\"\n",
        "\n",
        "num_palabras=len(palabras)\n",
        "\n",
        "# Contar palabras con longitud mayor o igual a n_largas caracteres\n",
        "num_palabras_largas = sum(1 for palabra in palabras if len(palabra) >= n_largas)\n",
        "\n",
        "# Contar palabras con longitud menor a n_largas caracteres\n",
        "num_palabras_cortas = sum(1 for palabra in palabras if len(palabra) < n_largas)\n",
        "\n",
        "# Contar palabras que comienzan con la letra \"letra\"\n",
        "num_palabras_e = sum(1 for palabra in palabras if palabra.lower().startswith(letra))\n",
        "\n",
        "# Contar palabras que NO comienzan con la letra \"letra\"\n",
        "num_palabras_e_not = sum(1 for palabra in palabras if not palabra.lower().startswith(letra))\n",
        "\n",
        "# Contar palabras que comienzan con \"e\" y tienen longitud mayor o igual a n_largas caracteres\n",
        "num_palabras_e_largas = sum(1 for palabra in palabras if len(palabra) >= n_largas and palabra.lower().startswith(letra))\n",
        "\n",
        "# Contar palabras que comienzan con \"e\" y tienen longitud menor a n_largas caracteres\n",
        "num_palabras_e_cortas = sum(1 for palabra in palabras if len(palabra) < n_largas and palabra.lower().startswith(letra))\n",
        "\n",
        "# Calcular la probabilidad condicionada\n",
        "probabilidad_condicionada = num_palabras_e_largas / num_palabras_largas\n",
        "\n",
        "# Calcular la probabilidad condicionada 2\n",
        "probabilidad_condicionada2 = num_palabras_e_cortas / num_palabras_cortas\n",
        "\n",
        "print(f\"\\nN√∫mero total de palabras:\", num_palabras)\n",
        "print(f\"\\nN√∫mero total de palabras con longitud mayor o igual a {n_largas} (palabras largas): {num_palabras_largas}\")\n",
        "# print(f\"\\nN√∫mero total de palabras con longitud menor a {n_largas} (palabras cortas): {num_palabras_cortas}\")\n",
        "print(f\"\\nN√∫mero total de palabras que comienzan con la letra {letra}: {num_palabras_e}\")\n",
        "# print(f\"\\nN√∫mero total de palabras que NO comienzan con la letra {letra}: {num_palabras_e_not}\")\n",
        "print(f\"\\nN√∫mero de palabras que comienzan con {letra} y tienen longitud mayor o igual a {n_largas}: {num_palabras_e_largas}\")\n",
        "# print(f\"\\nProbabilidad de que una palabra comience con {letra} dado que tiene longitud mayor o igual a {n_largas}: {probabilidad_condicionada}\")\n",
        "# print(f\"\\nProbabilidad de que una palabra comience con longitud menor a {n_largas} comience por {letra}: {probabilidad_condicionada2}\")"
      ],
      "metadata": {
        "id": "Hjcch5cNaAlO",
        "cellView": "form"
      },
      "id": "Hjcch5cNaAlO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Ejercicio:***\n",
        "\n",
        "En el texto dado, consideramos las palabras que tienen una longitud menor a 5 caracteres. Si seleccionamos una palabra al azar de entre estas palabras, ¬øcu√°l es la probabilidad de que esa palabra termine con la letra \"s\"?"
      ],
      "metadata": {
        "id": "_cEj8w4_Baxs"
      },
      "id": "_cEj8w4_Baxs"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Respuesta\n",
        "\n",
        "# Dividir el texto en palabras\n",
        "palabras = texto_limpio.split()\n",
        "\n",
        "n_largas= 5\n",
        "letra=\"s\"\n",
        "\n",
        "num_palabras=len(palabras)\n",
        "\n",
        "# Contar palabras con longitud menor a n_largas caracteres\n",
        "num_palabras_cortas = sum(1 for palabra in palabras if len(palabra) < n_largas)\n",
        "\n",
        "# Contar palabras que comienzan con la letra \"letra\"\n",
        "num_palabras_e = sum(1 for palabra in palabras if palabra.lower().endswith(letra))\n",
        "\n",
        "# Contar palabras que terminan con letra y tienen longitud menor a n_largas caracteres\n",
        "num_palabras_e_cortas = sum(1 for palabra in palabras if len(palabra) < n_largas and palabra.lower().endswith(letra))\n",
        "\n",
        "# Calcular la probabilidad condicionada\n",
        "probabilidad_condicionada = num_palabras_e_cortas / num_palabras_cortas\n",
        "\n",
        "\n",
        "print(f\"\\nN√∫mero total de palabras:\", num_palabras)\n",
        "print(f\"\\nN√∫mero total de palabras con longitud menor a {n_largas} (palabras cortas): {num_palabras_cortas}\")\n",
        "print(f\"\\nN√∫mero total de palabras que terminan con la letra {letra}: {num_palabras_e}\")\n",
        "print(f\"\\nN√∫mero de palabras que terminan con {letra} y tienen longitud menor a {n_largas}: {num_palabras_e_cortas}\")\n",
        "print(f\"\\nProbabilidad de que una palabra termine con {letra} dado que tiene longitud menor a {n_largas}: {probabilidad_condicionada}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TNwrpZ6JBCyq",
        "cellView": "form"
      },
      "id": "TNwrpZ6JBCyq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Propiedades de la probabilidad condicional:\n",
        "\n",
        "   \n",
        "* Independencia: Dos eventos $A$ y $B$ son independientes si y solo si la probabilidad de que ocurra $A$ no se ve afectada por la ocurrencia de $B$, y viceversa. Esto se expresa matem√°ticamente como:\n",
        "\n",
        "    $$P(A|B) = P(A) \\quad \\text{y} \\quad P(B|A) = P(B)$$\n",
        "    que es equivalente a\n",
        "    $$P(A\\cap B) = P(A)P(B)$$\n",
        "        \n",
        "* Regla del producto: La probabilidad de la intersecci√≥n de dos eventos $A$ y $B$ puede calcularse utilizando la probabilidad condicionada:\n",
        "\n",
        "$$P(A \\cap B) = P(A|B) \\cdot P(B)$$\n",
        "        \n",
        "* Teorema de Bayes: Es una herramienta poderosa que nos permite actualizar nuestras creencias sobre la ocurrencia de un evento $A$ dado que ha ocurrido otro evento $B$.\n",
        "\n",
        "    Como $$P(A|B)=\\frac{P(A \\cap B)}{P(B)} \\quad \\text{y} \\quad P(B|A)=\\frac{P(A \\cap B)}{P(A)},$$\n",
        "\n",
        "    y\n",
        "    \n",
        "    $$P(A \\cap B) =P(B) \\cdot P(A|B) = P(A) \\cdot P(B|A)$$\n",
        "\n",
        "    Entonces:\n",
        "    \n",
        "    $$P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$$\n",
        "    \n",
        "* Teorema de la pobabilidad total:\n",
        "\n",
        " El teorema de la probabilidad total es un resultado fundamental en teor√≠a de la probabilidad que se utiliza para calcular la probabilidad de un evento mediante la descomposici√≥n de dicho evento en varios casos o situaciones posibles, y luego sumando las probabilidades de estos casos individuales.\n",
        "\n",
        " Formalmente, supongamos que $A_1, A_2, \\ldots, A_n$ son eventos mutuamente excluyentes que forman una partici√≥n del espacio muestral $\\Omega$. Esto significa que $A_i \\cap A_j = \\emptyset$ para $i \\neq j$ y que $A_1 \\cup A_2 \\cup \\ldots \\cup A_n = \\Omega$.\n",
        "\n",
        " Entonces, para cualquier evento $B$ que pueda ocurrir en el espacio muestral $\\Omega$, la probabilidad de $B$ se puede calcular como la suma de las probabilidades condicionales de $B$ dados los eventos $A_i$, ponderadas por las probabilidades de $A_i$. Matem√°ticamente, esto se expresa como:\n",
        "\n",
        " $$ P(B) = \\sum_{i=1}^{n} P(B|A_i) \\cdot P(A_i) $$\n",
        "\n",
        " Donde:\n",
        " * $P(B|A_i)$ es la probabilidad de que el evento $B$ ocurra dado que el evento $A_i$ ha ocurrido.\n",
        " * $P(A_i)$ es la probabilidad del evento $A_i$.\n",
        " * $P(B)$ es la probabilidad del evento $B$.\n",
        "\n",
        " Este teorema es √∫til cuando calcular la probabilidad de un evento no se puede hacer directamente, pero es posible calcular la probabilidad condicional de este evento dado diferentes casos o situaciones que cubren todo el espacio muestral.\n",
        "\n",
        " ***\n",
        " **Ejemplo**:\n",
        " ***\n",
        "\n",
        " En una urna hay 5 bolas azules, 2 rojas y 1 verde, si se sacan 2 bolas consecutivas al azar con reemplazo. Calcular la probabilidad de sacar una bola azul y una verde.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5Uwl7QJHCuX4"
      },
      "id": "5Uwl7QJHCuX4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regla de la cadena:\n",
        "\n",
        "La regla de la cadena establece que la probabilidad conjunta de varios eventos condicionales puede calcularse multiplicando las probabilidades condicionales de cada evento dado el conjunto de eventos anteriores.\n",
        "\n",
        "\n",
        "$$P(A_1 \\cap A_2 \\cap A_3 \\cap \\ldots \\cap A_n) = P(A_1) \\cdot P(A_2 | A_1) \\cdot P(A_3 | A_1 \\cap A_2) \\cdot \\ldots \\cdot P(A_n | A_1 \\cap A_2 \\cap \\ldots \\cap A_{n-1}),$$\n",
        "\n",
        "donde $P(A_{i})$ es la probabilidad del evento $A_{i}$ y $P(A_j | A_1 \\cap A_2 \\cap \\ldots \\cap A_{j-1})$ es la probabilidad condicional del evento $A_{j}$, dado que los eventos $A_{1}, A_{2}, \\ldots, A_{j-1}$ han ocurrido.\n",
        "\n",
        "\n",
        "**Supongamos** que tenemos un modelo de lenguaje que intenta predecir la probabilidad de una oraci√≥n dada.\n",
        "Para simplificar, consideremos la oraci√≥n ***El gato negro est√° durmiendo***.\n",
        "\n",
        "Para calcular la probabilidad conjunta de esta oraci√≥n, podemos aplicar la regla de la cadena de la probabilidad descomponiendo la probabilidad de cada palabra dada la secuencia de palabras anteriores.\n",
        "$$P(\\text{\"el gato negro est√° durmiendo\"}) = P(\\text{\"el\"}) \\cdot P(\\text{\"gato\"} | \\text{\"el\"}) \\cdot P(\\text{\"negro\"} | \\text{\"el gato\"}) \\cdot P(\\text{\"est√°\"} | \\text{\"el gato negro\"}) \\cdot P(\\text{\"durmiendo\"} | \\text{\"el gato est√°})$$\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_cEvWIqzbMm4"
      },
      "id": "_cEvWIqzbMm4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "\n",
        "### Volvemos al caso de las puertas...\n",
        "\n",
        "* ¬øCu√°l es la probabilidad de ganar (en el caso de **NO** cambiar)?\n",
        "* ¬øCu√°l es la probabilidad de ganar (en el caso de cambiar)?\n",
        "\n",
        "***\n",
        "\n",
        "**Definici√≥n de los sucesos**\n",
        "\n",
        "* Llamamos al suceso:\n",
        "\n",
        "    * C1: El coche est√° en la puerta 1\n",
        "    * C2: El coche est√° en la puerta 2\n",
        "    * C3: El coche est√° en la puerta 3\n",
        "\n",
        "    * A1: El presentador abre la puerta 1\n",
        "    * A2: El presentador abre la puerta 2\n",
        "    * A3: El presentador abre la puerta 3\n",
        "\n",
        "* Las probabilidades a priori:\n",
        "\n",
        "La probabilidad de que el coche se encuentre en una de las puertas, al empezar el experimento, es la misma\n",
        "\n",
        "* P(C1) =1/3\n",
        "* P(C2) =1/3\n",
        "* P(C3) =1/3\n",
        "\n",
        "### Supongamos que el concursante escoge, al principio, la puerta 1, puede pasar dos cosas:$\n",
        "\n",
        "1. El coche est√° en la puerta 1\n",
        "2. El coche NO est√° en la puerta 1\n",
        "\n",
        "### Supongamos que el presentador abre la puerta 2 $ \\rightarrow$\n",
        "\n",
        "\n",
        "1. Si el coche est√° en la puerta 1 $P(A2|C1)=1/2$ ya puede abrir cualquiera de las otras dos puertas\n",
        "2. En el caso de que el coche no est√© en la puerta 1, entonces la $P(A2|C3)=1$, ya que no tiene opci√≥n de elegir la otra puerta, porque ah√≠ estar√≠a el coche. *Recordemos que el presentador **SI** sabe en d√≥nde est√° el coche*. Entonces, la puerta 1 es la elegida y en la puerta 3 est√° el coche.\n",
        "\n",
        "Debemos calcular:\n",
        "\n",
        "* $P(C1|A2)$ (Me quedo con la puerta inicial)\n",
        "* $P(C3|A2)$ (Cambio la puerta)\n",
        "\n",
        "¬øCu√°l es mayor?\n",
        "\n",
        "**Nota:**\n",
        "\n",
        "C√≥mo el coche es seguro que est√° en una de las dos puertas, $P(C1|A2)+P(C3|A2)=1$\n",
        "\n",
        "## C√°lculo de las probabilidades (mediante el teorema de bayes):\n",
        "\n",
        "* $P(C1|A2) = \\frac{P(A2|C1)P(C1)}{P(A2)}=\\frac{\\frac{1}{2}*\\frac{1}{3}}{P(A2)}$\n",
        "* $P(C3|A2) = \\frac{P(A2|C3)P(C3)}{P(A2)}=\\frac{1*\\frac{1}{3}}{P(A2)}$\n",
        "\n",
        "\n",
        "\n",
        "## Para el c√°lculo de $P(A2)$ usamos el teorema de la probabilidad total:\n",
        "\n",
        "$$P(A2)=P(C1).P(A2|C1)+P(C2).P(A2|C2)+P(C3).P(A2|C3)=$$\n",
        "\n",
        "$$= 1/3.1/2+1/3.0+1/3.1=1/2$$\n",
        "\n",
        "\n",
        "**Nota**\n",
        "\n",
        "$P(A2|C2)=0$ ya que si premio (coche) est√° en la puerta 2, el presentador *CLARAMENTE* no la va a abrir.\n",
        "\n",
        "\n",
        "\n",
        "<center>\n",
        " <img src=\"https://github.com/ednavivianasegura/Curso_PLN/blob/main/J1.png?raw=1\" alt=\"descriptiva\" width=\"99%\" height=\"99%\">  \n",
        "</center>\n",
        "\n"
      ],
      "metadata": {
        "id": "vmPH3qW8iMhH"
      },
      "id": "vmPH3qW8iMhH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "\n",
        "Diagrama completo disponible en: [Diagrama de √°rbol problema de Monty Hall](https://github.com/ednavivianasegura/Curso_PLN/blob/main/MontyHall_compressed.pdf)\n",
        "\n",
        "***\n",
        "<center>\n",
        " <img src=\"https://github.com/ednavivianasegura/Curso_PLN/blob/main/MontyHall_1.png?raw=1\" alt=\"descriptiva\" width=\"100%\" height=\"100%\">  \n",
        "</center>\n",
        "\n",
        "<center>\n",
        " <img src=\"https://github.com/ednavivianasegura/Curso_PLN/blob/main/MontyHall_2.png?raw=1\" alt=\"descriptiva\" width=\"100%\" height=\"100%\">  \n",
        "</center>\n",
        "\n",
        "<center>\n",
        " <img src=\"https://github.com/ednavivianasegura/Curso_PLN/blob/main/MontyHall_3.png?raw=1\" alt=\"descriptiva\" width=\"100%\" height=\"100%\">  \n",
        "</center>\n",
        "\n",
        "***"
      ],
      "metadata": {
        "id": "Rm36zOYnI8bY"
      },
      "id": "Rm36zOYnI8bY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "***\n",
        "### **Variables aleatorias:**\n",
        "\n",
        "Una variable aleatoria (v.a.) es como un *contenedor* de resultados posibles en un experimento. Piensa en ella como una herramienta que nos ayuda a asignar n√∫meros a los resultados de un evento que no es fijo (aleatorio).\n",
        "\n",
        "Por ejemplo, imagina que est√°s lanzando un dado. Cada vez que lo lanzas, obtienes un n√∫mero diferente. Ese n√∫mero que obtienes, ya sea un 1, un 2, un 3, hasta un 6, es el valor de tu variable aleatoria para ese lanzamiento.\n",
        "\n",
        "Si est√°s midiendo la temperatura de una habitaci√≥n y obtienes diferentes lecturas cada vez que tomas la temperatura, entonces esa temperatura medida es una variable aleatoria. Puedes obtener 25 grados, 25.5 grados, 26 grados, etc.\n",
        "\n",
        "En resumen, una variable aleatoria es una manera de representar num√©ricamente los resultados de un experimento aleatorio. No es solo un n√∫mero aleatorio, sino una manera de organizar y entender la informaci√≥n que obtenemos de nuestros experimentos.\n",
        "\n",
        "\n",
        "**Formalmente:**\n",
        "\n",
        "En t√©rminos m√°s formales, una (v.a.) es una funci√≥n que asigna un n√∫mero real a cada resultado posible de un experimento aleatorio. Si denotamos la variable aleatoria como $X$, entonces para cada resultado $x$ del experimento, $X$ asigna un valor que es un n√∫mero real ($x \\in \\mathbb{R}$):\n",
        "\n",
        "$$X: \\Omega \\rightarrow \\mathbb{R}$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZWNDnM_OIkNV"
      },
      "id": "ZWNDnM_OIkNV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables aleatorias discretas:\n",
        "\n",
        "Una v.a. es discreta si la cantidad de valores que puede tomar es numerable:\n",
        "$$x_{1}, x_{2}, x_{3},...$$\n",
        "\n",
        "En una v.a. discreta $ p_{k}= P(X=k)$ es la funci√≥n de probabilidad o de masa. Se cumple $\\sum p_{x_{i}}=1$, esto es, las probabilidades de todos los valores que la v.a. discreta puede tomar suman 1.\n",
        "\n",
        "***Ejemplo:***\n",
        "\n",
        "- Variable de Bernulli. $$X \\thicksim B(p), 0 < p < 1,$$\n",
        "    de manera que\n",
        "   \n",
        "   $$ X = \\begin{cases}\n",
        "  1 & \\text{ con probabilidad } p \\\\\n",
        "  0 & \\text{ con probabilidad } (1-p)\n",
        "\\end{cases}$$\n",
        "\n",
        "Esto es, $p_{1}=p$ y $p_{0}=1-p$\n",
        "\n",
        "**En el an√°lisis del sentimiento**, determinar si la cr√≠tica de una pel√≠cula es positiva (1) o negativa (0) representa un ensayo Bernoulli.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nAASqdGNLgJN"
      },
      "id": "nAASqdGNLgJN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables aleatorias continuas:\n",
        "\n",
        "Una v.a. **continua** $X$ cumple que para todo n√∫mero $k$, la probabilidad de que $X$ lo tome es nula, esto es $P(X=k)=0$. La probabilidad que interesa es la de que tome valores en cualquier intervalo, esto es:\n",
        "* La probabilidad de (a,b) es $P(a < x < b)$, que es la misma que la de $[a,b]$, $[a,b)$ √≥ $(a,b]$.\n",
        "* La probabilidad de $(a, +\\infty)$ es $P(X>a)$.\n",
        "* La probabilidad de $(-\\infty, a)$ es $P(X<a)$.\n",
        "\n",
        "***\n",
        "**Ejemplo:**\n",
        "***\n",
        "\n",
        "Un ejemplo cl√°sico de una variable aleatoria continua es la altura de una persona. La altura puede tomar cualquier valor dentro de un rango espec√≠fico, y generalmente se puede medir con una precisi√≥n arbitraria (por ejemplo, en cent√≠metros o pulgadas).\n",
        "\n",
        "Para formalizar este ejemplo, consideremos una variable aleatoria $ X $ que representa la altura de una persona. La variable aleatoria $ X $ puede tomar cualquier valor dentro de un rango continuo, por ejemplo, desde 140 cent√≠metros hasta 200 cent√≠metros.\n",
        "\n"
      ],
      "metadata": {
        "id": "oR8T8AmBjTIT"
      },
      "id": "oR8T8AmBjTIT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Funci√≥n de densidad:\n",
        "\n",
        "Dada $X$ una v.a. continua, una funci√≥n $f\\geq0$ es la funci√≥n de densidad de $X$ si la probabilidad de $(a,b)$ es igual al √°rea de la regi√≥n limitada por la gr√°fica $y=f(x)$, y las rectas $y=0$, $x=a$ y $x=b$.\n"
      ],
      "metadata": {
        "id": "xrAX_meZrHND"
      },
      "id": "xrAX_meZrHND"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title funci√≥n de densidad { vertical-output: true, display-mode: \"form\" }\n",
        "# Definir la funci√≥n de densidad de probabilidad (PDF)\n",
        "def f(x):\n",
        "    # Puedes definir tu propia funci√≥n de densidad aqu√≠\n",
        "    funcion= x * np.exp(-x**2 / 3) / np.sqrt(2 * np.pi)\n",
        "    return funcion\n",
        "\n",
        "# Definir los l√≠mites del intervalo [a, b]\n",
        "a = 0.2\n",
        "b = 3\n",
        "\n",
        "# Generar valores de x en el intervalo [a, b]\n",
        "x_values = np.linspace(a, b, 1000)\n",
        "\n",
        "# Calcular los valores de y = f(x)\n",
        "y_values = f(x_values)\n",
        "\n",
        "# Crear la gr√°fica\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Graficar la funci√≥n de densidad de probabilidad (PDF)\n",
        "plt.plot(x_values, y_values, 'b-', linewidth=2)\n",
        "\n",
        "# Rellenar el √°rea bajo la curva entre x=a y x=b\n",
        "plt.fill_between(x_values, 0.05, y_values, where=(x_values >= a) & (x_values <= b-1), color='skyblue', alpha=0.5)\n",
        "\n",
        "# L√≠neas verticales en x=a y x=b\n",
        "plt.axvline(x=a, color='gray', linestyle='--', linewidth=1)\n",
        "plt.axvline(x=b-1, color='gray', linestyle='--', linewidth=1)\n",
        "\n",
        "# L√≠neas horizontal en y=0\n",
        "plt.axhline(y=0.081, xmin=0, xmax=1.6, color='gray', linestyle='--', linewidth=1)\n",
        "\n",
        "# Eliminar los ticks de los ejes x e y\n",
        "plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
        "plt.tick_params(axis='y', which='both', left=False, right=False, labelleft=False)\n",
        "\n",
        "# Agregar texto en x=-2 (x=a) y x=2 (x=b)\n",
        "plt.text(a, 0.07, 'x=a', ha='center', va='center', color='black', fontsize=12)\n",
        "plt.text(b-1, 0.07, 'x=b', ha='center', va='center', color='black', fontsize=12)\n",
        "\n",
        "plt.text(0.7, 0.275, 'y=f(x)', ha='center', va='center', color='black', fontsize=12)\n",
        "\n",
        "plt.text(1.15, 0.25, 'P(a < X < b)', ha='center', va='center', color='black', fontsize=12)\n",
        "\n",
        "plt.text(2.9, 0.085, 'y=0', ha='center', va='center', color='black', fontsize=12)\n",
        "\n",
        "\n",
        "# Establecer l√≠mites y mostrar la gr√°fica\n",
        "plt.ylim(0.08, 0.3)  # Ajustar el l√≠mite y\n",
        "plt.xlim(0, np.max(x_values))  # Ajustar el l√≠mite x\n",
        "# Mostrar la gr√°fica\n",
        "plt.grid(True)\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6nZMNHxLsT-Q"
      },
      "id": "6nZMNHxLsT-Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Propiedades:\n",
        "\n",
        "* El √°rea total entre $y=0$ e $y=f(x)$ es igual a 1.\n",
        "* El √°rea a la derecha de $x=b$ delimitada por $y=0$ e $y=f(x)$ es $P(X > b)$.\n",
        "* El √°rea a la izquierda de $x=a$ delimitada por $y=0$ e $y=f(x)$ es $P(X < a)$.\n"
      ],
      "metadata": {
        "id": "7C0QTJ7xxMcK"
      },
      "id": "7C0QTJ7xxMcK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Distribuci√≥n normal est√°ndar:\n",
        "La d.p. continua m√°s importante es la **distribuci√≥n normal est√°ndar** $N(0,1)$, esto es, la distribuci√≥n normal de *media* 0 y *desviaci√≥n t√≠pica* 1.\n",
        "\n",
        "Su funci√≥n de densidad tiene la forma llamada ***campana de Gauss***, y es sim√©trica respecto a $x=0$.\n",
        "\n",
        "**Nota:**\n",
        "Una variable gen√©rica con distribuci√≥n $N(0,1)$ se suele denotar $Z$ (en lugar de $X$), y escribimos $Z \\sim N(0,1)$.\n",
        "\n"
      ],
      "metadata": {
        "id": "Y5ryaEC4yM_T"
      },
      "id": "Y5ryaEC4yM_T"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Crea distribuci√≥n normal est√°ndar { vertical-output: true }\n",
        "def f(x):\n",
        "    # Puedes definir tu propia funci√≥n de densidad aqu√≠\n",
        "    # Por ejemplo, una distribuci√≥n normal est√°ndar\n",
        "    return np.exp(-x**2 / 2) / np.sqrt(2 * np.pi)\n",
        "\n",
        "# Definir los l√≠mites del intervalo [a, b]\n",
        "a = -2\n",
        "b = 2\n",
        "\n",
        "# Generar valores de x en el intervalo [a, b]\n",
        "x_values = np.linspace(a, b, 1000)\n",
        "\n",
        "# Calcular los valores de y = f(x)\n",
        "y_values = f(x_values)\n",
        "\n",
        "# Crear la gr√°fica\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Graficar la funci√≥n de densidad de probabilidad (PDF)\n",
        "plt.plot(x_values, y_values, 'b-', linewidth=2)\n",
        "\n",
        "# L√≠neas horizontal en y=0\n",
        "plt.hlines(y=0.055, xmin=-2, xmax=2, color='blue', linestyle='--', linewidth=1)\n",
        "\n",
        "\n",
        "# L√≠neas verticales en x=a y x=b\n",
        "plt.axvline(x=0, color='blue', linestyle='--', linewidth=1)\n",
        "\n",
        "# Eliminar los ticks de los ejes x e y\n",
        "plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
        "plt.tick_params(axis='y', which='both', left=False, right=False, labelleft=False)\n",
        "\n",
        "plt.text(2.15, 0.055, 'y=0', ha='center', va='center', color='black', fontsize=12)\n",
        "plt.text(-0.01, 0.03, 'x=0', ha='center', va='center', color='black', fontsize=12)\n",
        "\n",
        "# Mostrar la gr√°fica\n",
        "plt.grid(True)\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "E-lcrD6Fyoao"
      },
      "id": "E-lcrD6Fyoao",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- ### Variables aleatorias continua:\n",
        "\n",
        "Una v.a. continua ùëã cumple que para todo n√∫mero ùëò, la probabilidad de que ùëã lo tome es nula, esto es ùëÉ(ùëã=ùëò)=0. La probabilidad que interesa es la de que tome valores en cualquier intervalo, esto es:\n",
        "La probabilidad de (ùëé, ùëè) es ùëÉ(ùëé<ùëã<ùëè), que es la misma que la de [ùëé, ùëè), (ùëé, ùëè] o [ùëé, ùëè].\n",
        "La probabilidad de (ùëé,+‚àû) es ùëÉ(ùëã>ùëé).\n",
        "La probabilidad de (‚àí‚àû, ùëé) es ùëÉ(ùëã<ùëé) -->"
      ],
      "metadata": {
        "id": "r6_5sH3vNxiy"
      },
      "id": "r6_5sH3vNxiy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Distribuci√≥n normal:\n",
        "Si $Z \\thicksim N(0,1)$, dados dos valores $\\mu$ y $\\sigma (\\sigma > 0)$ la v.a. $X = \\mu + \\sigma Z$  tiene la distribuci√≥n normal $N(\\mu,\\sigma)$ de media $\\mu$ y desviaci√≥n t√≠pica $\\sigma$. Se escribe $X \\thicksim N(\\mu,\\sigma)$.\n",
        "\n",
        "Su funci√≥n de densidad es tambi√©n una campana de Gauss, centrada en $x=\\mu$ y sim√©trica respecto a esta recta. En $\\mu-\\sigma$ y $\\mu+\\sigma$ est√°n los puntos de inflexi√≥n.\n"
      ],
      "metadata": {
        "id": "eCHlRgvRC6LY"
      },
      "id": "eCHlRgvRC6LY"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Crea distribuci√≥n normal est√°ndar { vertical-output: true }\n",
        "def f(x):\n",
        "    # Puedes definir tu propia funci√≥n de densidad aqu√≠\n",
        "    # Por ejemplo, una distribuci√≥n normal est√°ndar\n",
        "    return np.exp(-x**2 / 2) / np.sqrt(2 * np.pi)\n",
        "\n",
        "# Definir los l√≠mites del intervalo [a, b]\n",
        "a = -2\n",
        "b = 2\n",
        "\n",
        "# Generar valores de x en el intervalo [a, b]\n",
        "x_values = np.linspace(a, b, 1000)\n",
        "\n",
        "# Calcular los valores de y = f(x)\n",
        "y_values = f(x_values)\n",
        "\n",
        "# Crear la gr√°fica\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Graficar la funci√≥n de densidad de probabilidad (PDF)\n",
        "plt.plot(x_values, y_values, 'b-', linewidth=2)\n",
        "\n",
        "\n",
        "# Rellenar el √°rea bajo la curva entre x=a y x=b\n",
        "plt.fill_between(x_values, 0, y_values, where=(x_values >= a+1) & (x_values <= b-1), color='skyblue', alpha=0.2)\n",
        "\n",
        "\n",
        "\n",
        "# L√≠neas verticales en x=a y x=b\n",
        "plt.axvline(x=0, color='blue', linestyle='--', linewidth=1)\n",
        "plt.axvline(x=-1, color='blue', linestyle='--', linewidth=1)\n",
        "plt.axvline(x=1, color='blue', linestyle='--', linewidth=1)\n",
        "\n",
        "# L√≠neas horizontal en y=0\n",
        "plt.hlines(y=0.082, xmin=-1.79, xmax=1.79, color='blue', linestyle='--', linewidth=1)\n",
        "\n",
        "\n",
        "# Eliminar los ticks de los ejes x e y\n",
        "plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
        "plt.tick_params(axis='y', which='both', left=False, right=False, labelleft=False)\n",
        "\n",
        "\n",
        "plt.annotate('', xy=(0, 0.4), xytext=(0.45, 0.4),\n",
        "             arrowprops=dict(facecolor='black', arrowstyle='<|-|>'))\n",
        "\n",
        "plt.annotate('', xy=(0.55, 0.4), xytext=(1, 0.4),\n",
        "             arrowprops=dict(facecolor='black', arrowstyle='<|-|>'))\n",
        "plt.text(0.5, 0.4, '$\\sigma$', ha='center', va='center', color='black', fontsize=12)\n",
        "\n",
        "plt.text(2, 0.082, 'y=0', ha='center', va='center', color='black', fontsize=12)\n",
        "plt.text(-0.01, 0.07, f'$x=\\mu$', ha='center', va='center', color='black', fontsize=12)\n",
        "plt.text(-1, 0.07, f'$x=\\mu-\\sigma$', ha='center', va='center', color='black', fontsize=12)\n",
        "plt.text(1, 0.07, f'$x=\\mu+\\sigma$', ha='center', va='center', color='black', fontsize=12)\n",
        "\n",
        "plt.text(-0.01, 0.15, f'$P(\\mu-\\sigma<X<\\mu+\\sigma)= 0.6827$ (‚âà 2/3)', ha='center', va='center', color='black', fontsize=11)\n",
        "\n",
        "plt.ylim(0.08, 0.45)  # Ajustar el l√≠mite y\n",
        "\n",
        "\n",
        "# Mostrar la gr√°fica\n",
        "plt.grid(False)\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nS2im2unNy5c",
        "cellView": "form"
      },
      "id": "nS2im2unNy5c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Propiedades de la distribuci√≥n normal:\n",
        "\n",
        "Dada una v.a. $X$ (normal o no) de media $\\mu$ y desviaci√≥n t√≠pica $\\sigma$ la probabilidad de que $X$ tome un valor que se aleje de su media una distancia inferior a $k$ veces su desviaci√≥n t√≠pica se expresa:\n",
        "\n",
        "$$P(|X-\\mu|<k \\sigma) = P(-k\\sigma <X-\\mu < k\\sigma)=P(\\mu-k\\sigma<X<\\mu+k\\sigma)$$\n",
        "\n",
        "\n",
        "Dada $ X \\thicksim N(\\mu, \\sigma)$, destacamos las siguientes probabilidades:\n",
        "\n",
        "* $P(\\mu-\\sigma<X<\\mu+\\sigma)= 0.6827$, por lo tanto, algo m√°s de 2 de cada 3 veces el valor de una *distribuci√≥n normal* no se aleja de la media m√°s de 1 desviaci√≥n t√≠pica $(2/3 = 0.6)$;\n",
        "\n",
        "* $P(\\mu -2\\sigma < X < \\mu+2\\sigma)=0.9545$,  por lo tanto, algo m√°s del 95% de las veces el valor de una distribuci√≥n normal no se aleja de la media m√°s de 2 desviaciones t√≠picas;\n",
        "\n",
        "* $P(\\mu -3\\sigma < X < \\mu+3\\sigma)=0.9973$, por lo tanto, es muy probable que el valor de una distribuci√≥n normal no se aleje de la media m√°s de 3 desviaciones t√≠picas.\n",
        "\n",
        "* Rec√≠procamente, si $X \\thicksim N(\\mu, \\sigma)$ entonces su tipificada\n",
        "$Z=(X-\\mu)/\\sigma$ cumple que $Z \\thicksim N(0, 1)$. Esto es, al tipificar una distribuci√≥n normal obtenemos la distribuci√≥n normal est√°ndar.\n",
        "\n",
        "    La distribuci√≥n normal aparece con frecuencia en amplios campos de la investigaci√≥n. Es una d.p. muy interesante ya que, hablando informalmente, si el resultado de la variable a estudiar viene dado por la suma de un gran n√∫mero de variables id√©nticas e independientes (que seguramente no sabremos medir) es de esperar que la d.p. resultante sea muy aproximadamente normal.\n",
        "\n"
      ],
      "metadata": {
        "id": "KE0aIA26EXD-"
      },
      "id": "KE0aIA26EXD-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Distribuci√≥n $t$ de student:\n",
        "\n",
        "La $t$ de Student con $n$ grados de libertad $n=(1,2,3,...)$ es una distribuci√≥n continua. Si una v.a. $T$ sigue una distribuci√≥n $t$ de Student con $n$ grados de libertad, se escribe $T \\thicksim t_{n}$.\n",
        "\n",
        "Como $N(0,1)$, $t_{n}$ est√° centrada en ùë•$x=0$ y es sim√©trica con respecto a esta recta.\n",
        "Se tiene $$t_{n} \\xrightarrow[N \\rightarrow +\\infty]{} N(0,1)$$.\n",
        "\n",
        "Es una d.p. que se usa para hacer inferencia sobre la media. Dadas $X_{1}, X_{2}, ..., X_{n}$ variables aleatorias independientes con una misma distribuci√≥n normal $N(\\mu,\\sigma)$, con media muestral $\\bar{X}$ y cuasidesviaci√≥n t√≠pica $S_{X}$, entonces:\n",
        "\n",
        "$$\\frac{\\bar{X}-\\mu}{S_{X}/\\sqrt{n}}\\thicksim t_{n-1}$$"
      ],
      "metadata": {
        "id": "mvaFnapKbrEZ"
      },
      "id": "mvaFnapKbrEZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Crea distribuci√≥n t de student\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "from scipy.stats import t\n",
        "\n",
        "# Definir el rango para x\n",
        "x = np.linspace(-5, 5, 1000)\n",
        "\n",
        "# Grados de libertad para las distribuciones t de Student\n",
        "grados_de_libertad = [1, 2, 3, 4, 5,6,7]  # A√±ad√≠ 1000 como infinito\n",
        "\n",
        "# Crear una nueva figura\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Graficar las distribuciones t de Student para diferentes grados de libertad\n",
        "for gl in grados_de_libertad[:-1]:\n",
        "    plt.plot(x, t.pdf(x, gl), label=f'{gl}', linestyle=\"--\")\n",
        "plt.plot(x, t.pdf(x, 10000000000), label=f'$+\\infty$', linestyle=\"-\", color=\"black\")\n",
        "\n",
        "# Agregar texto en x=0 que muestra la media\n",
        "plt.text(0, -0.03, 'x=0', ha='center', va='center', color='black', fontsize=12)\n",
        "\n",
        "# Agregar texto en x=0 que muestra la media\n",
        "plt.text(5.5, 0, 'y=0', ha='center', va='center', color='black', fontsize=12)\n",
        "\n",
        "# Eliminar los ticks de los ejes x e y\n",
        "plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
        "plt.tick_params(axis='y', which='both', left=False, right=False, labelleft=False)\n",
        "\n",
        "# L√≠neas verticales en x=a y x=b\n",
        "plt.axvline(x=0, color='pink', linestyle='--', linewidth=0.5)\n",
        "\n",
        "# L√≠neas verticales en x=a y x=b\n",
        "plt.axhline(y=0, color='pink', linestyle='--', linewidth=0.5)\n",
        "\n",
        "# A√±adir t√≠tulo y leyenda\n",
        "plt.title('Distribuci√≥n t de Student con diferentes grados de libertad')\n",
        "plt.legend(title=\"Grados de libertad\")\n",
        "\n",
        "# Mostrar la gr√°fica\n",
        "plt.grid(False)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Brri9EkjOPaF",
        "cellView": "form"
      },
      "id": "Brri9EkjOPaF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "***"
      ],
      "metadata": {
        "id": "1FbauPe_xqzl"
      },
      "id": "1FbauPe_xqzl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "# Inferencia\n",
        "***"
      ],
      "metadata": {
        "id": "yIGpkXYjxr7v"
      },
      "id": "yIGpkXYjxr7v"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "<img src=\"https://github.com/ednavivianasegura/Curso_PLN/blob/main/inferencia.png?raw=1\" alt=\"inferencia\" width=\"50%\" height=\"50%\">  \n",
        "</center>"
      ],
      "metadata": {
        "id": "rCqOVy82xQAs"
      },
      "id": "rCqOVy82xQAs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para estudiar alg√∫n aspecto de la distribuci√≥n de una variable $X$, tomamos una muestra de $n$ valores independientes de $X: x_{1}, x_{2}, ... , x_{n}$. En ***Estad√≠stica Inferencial*** se estudia qu√© afirmaciones podemos hacer sobre los aspectos que nos interesan de $X$, y con qu√© fiabilidad, en base a los valores de la muestra. La muestra puede consistir, por ejemplo, en los datos de $n$ personas encuestadas o los resultados de $n$ mediciones experimentales.\n",
        "\n",
        "Abordaremos dos aspectos:\n",
        "\n",
        "* **Inferencia sobre la media:** por ejemplo, ¬øpodemos afirmar que la palabra \"tecnolog√≠a\" aparece en promedio 10 veces en los art√≠culos de tecnolog√≠a?\n",
        "* **Inferencia sobre la proporci√≥n:** por ejemplo, ¬øpodemos afirmar que la un tercio de los documentos relacionados con la pol√≠tica mencionan al partido que se encuentra actualmente en el poder?"
      ],
      "metadata": {
        "id": "4P4kA8_9x2F2"
      },
      "id": "4P4kA8_9x2F2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inferencia sobre la media\n",
        "\n",
        "Sabemos (o suponemos) que la poblaci√≥n $X$ es normal, $X \\thicksim N(\\mu, \\sigma)$. El valor $\\mu$ es la media poblacional, y nos restringiremos a afirmaciones sobre $\\mu$. A $\\sigma$ se le llama desviaci√≥n t√≠pica poblacional.\n",
        "\n",
        "Como condiciones de validez, deben cumplirse que los casos $X_{i}$ sean independientes (el valor de cada uno no tiene influencia en los dem√°s) y para cada $i$ùëñ, $X_{i} \\thicksim N(\\mu, \\sigma)$.\n",
        "\n",
        "La **media muestral** es\n",
        "$$ \\bar{X}= \\frac{1}{n} \\sum_{i=1}^{n}X_{i}, $$\n",
        "y la **cuasidesviaci√≥n t√≠pica muestral** es\n",
        "\n",
        "$$S_{X}=\\sqrt{\\frac{1}{(n-1)}\\sum_{i=1}^{n}(X_{i}-\\bar{X})^{2}}$$\n",
        "\n",
        "La base de nuestra inferencia es que, como indicamos al tratar la distribuci√≥n normal y la distribuc√≠n $t$ de student,\n",
        "\n",
        "$$\\frac{\\bar{X}-\\mu}{S_{X}/\\sqrt{n}}\\thicksim t_{n-1}$$\n",
        "\n",
        "Obviamente $\\frac{\\bar{X}-\\mu}{S_{X}/\\sqrt{n}}$ es una variable aleatoria, su valor depende de nuestra muestra"
      ],
      "metadata": {
        "id": "I_ixiJbT0ihJ"
      },
      "id": "I_ixiJbT0ihJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inferencia sobre la proporci√≥n\n",
        "\n",
        "Estudiamos una caracter√≠stica. La probabilidad de que un caso aleatoriamente elegido la cumpla es $p$, la proporci√≥n poblacional. En una muestra de $n$ individuos (casos) la cumplen $k$, y $\\hat{p}=\\frac{k}{n}$ es la **proporci√≥n muestral**. La base de nuestra inferencia es que, si $n$ es grande,\n",
        "\n",
        "\n",
        "$$\\frac{\\hat{p}-p}{\\sqrt{\\frac{p(1-p)}{n}}} \\underset{\\text{aprox}}{\\sim} N(0,1) $$\n",
        "\n",
        "Adem√°s se deben cumplir las siguientes condiciones de validez:\n",
        "\n",
        "* La muestra es ***independiente y representativa*** (en los casos elegidos esperamos que la probabilidad de cumplir la caracter√≠stica sea efectivamente la proporci√≥n muestral).\n",
        "\n",
        "* O bien $n \\geq 30$; o bien $np \\geq 5$ y $n(1-p)\\geq 5$.\n",
        "\n",
        "  Esto significa que la aproximaci√≥n es mala cuando $p \\approx 0$ o $p \\approx 1$. Como no conocemos $p$ esta condici√≥n la sustituimos por $n \\hat{p} = k \\geq 5 $ y $n(1-\\hat{p})=n-k \\geq 5.$"
      ],
      "metadata": {
        "id": "MWPw4Xem2yak"
      },
      "id": "MWPw4Xem2yak"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resultados de inferencia\n",
        "\n",
        "Nos centramos en tres tipos de inferencia, que realizaremos para la media y para la proporci√≥n:\n",
        "* intervalos de confianza\n",
        "* contrastes de hip√≥tesis\n",
        "* tama√±o de la muestra\n",
        "\n",
        "Estos resultados depender√°n de la fiabilidad establecida, que se expresa mediante el nivel de confianza como $1-\\alpha$ donde $\\alpha$ (que suele representar una probabilidad peque√±a) es el nivel de significaci√≥n.\n",
        "\n",
        "Habitualmente:\n",
        "* $\\alpha = 0.1 $, por lo tanto $1-\\alpha = 0.9$, lo que indica que se tiene el $90\\%$ de confianza.\n",
        "\n",
        "* $\\alpha = 0.05$, por lo tanto $1-\\alpha = 0.95$, lo que indica que se tiene el $95\\%$ de confianza.\n",
        "\n",
        "* $\\alpha = 0.01$, por lo tanto $1-\\alpha = 0.99$, lo que indica que se tiene el $99\\%$ de confianza.\n",
        "\n"
      ],
      "metadata": {
        "id": "CqaYJjIF6E2X"
      },
      "id": "CqaYJjIF6E2X"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intervalos de confianza"
      ],
      "metadata": {
        "id": "WLW4QsRD6_pM"
      },
      "id": "WLW4QsRD6_pM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intervalo de confianza para la media\n",
        "\n",
        "Para obtener el intervalo de confianza para la media de una poblaci√≥n normal con nivel de confianza $1-\\alpha$ usamos que\n",
        "\n",
        "$$\\frac{\\bar{X}-\\mu}{S_{X}/\\sqrt{n}}\\thicksim t_{n-1}$$\n",
        "\n",
        "\n",
        "$$1- \\alpha = P\\Big( -t_{n-1;\\alpha/2} < \\frac{\\bar{X}-\\mu}{S_{X}/\\sqrt{n}}\\thicksim t_{n-1} < t_{n-1;\\alpha/2}\\Big)$$\n",
        "\n",
        "despejamos $\\mu$ y llamamos como error muestral a\n",
        "\n",
        "$$EM= t_{n-1;\\alpha/2}S_{X}/\\sqrt{n}$$\n",
        "\n",
        "Entonces:\n",
        "\n",
        "\\begin{align*}\n",
        "1-\\alpha &= P(-EM < \\bar{X} - \\mu < \\bar{X} + EM) \\\\\n",
        "&= P(\\bar{X} -EM < \\mu < \\bar{X} + EM) \\\\\n",
        "&= P(\\mu \\in (\\bar{X} -EM,\\bar{X} +EM))\n",
        "\\end{align*}\n",
        "\n",
        "El valor $1-\\alpha$ representa la probabilidad de que el intervalo dado (en la formula) contenga el verdadero valor del par√°metro $\\mu$."
      ],
      "metadata": {
        "id": "xE84EYJx7Cvs"
      },
      "id": "xE84EYJx7Cvs"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Crea grafico intervalo para la media\n",
        "# Par√°metros\n",
        "n = 100  # Tama√±o de la muestra\n",
        "alpha = 0.05  # Nivel de confianza\n",
        "\n",
        "# C√°lculo de los valores cr√≠ticos de la distribuci√≥n t de Student\n",
        "t_left = -t.ppf(1 - alpha / 2, n - 1)\n",
        "t_right = t.ppf(1 - alpha / 2, n - 1)\n",
        "\n",
        "# Definir el rango para la gr√°fica\n",
        "x = np.linspace(-4, 4, 1000)\n",
        "y = t.pdf(x, n - 1)\n",
        "\n",
        "# Crear la figura y los ejes\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x, y, 'b-')\n",
        "# Rellenar el √°rea bajo la curva\n",
        "plt.fill_between(x, y, where=(x >= -2) & (x <= 2), color='lightblue', alpha=0.5)\n",
        "\n",
        "# L√≠neas verticales y etiquetas\n",
        "plt.axvline(0, color='red', linestyle='--', linewidth=0.3)\n",
        "\n",
        "plt.axhline(0, color='blue', linestyle='--', linewidth=0.3)\n",
        "\n",
        "plt.axvline(t_left, color='red', linestyle='--')\n",
        "plt.text(t_left, -0.01, r'$-t_{n-1,\\alpha/2}$', ha='right', va='bottom', color='red')\n",
        "plt.axvline(t_right, color='red', linestyle='--')\n",
        "plt.text(t_right, -0.01, r'$t_{n-1,\\alpha/2}$', ha='left', va='bottom', color='red')\n",
        "\n",
        "plt.text(0, -0.01, '0', ha='center', va='bottom', color='black')\n",
        "plt.text(0, 0.25, r'$1-\\alpha$', ha='center', va='bottom', color='red')\n",
        "\n",
        "plt.text(-2.5, 0.25, r'$\\alpha/2$', ha='center', va='bottom', color='red')\n",
        "\n",
        "plt.text(2.5, 0.25, r'$\\alpha/2$', ha='center', va='bottom', color='red')\n",
        "\n",
        "plt.annotate('', xy=(-2, 0.25), xytext=(2, 0.25),\n",
        "             arrowprops=dict(facecolor='black', arrowstyle='<|-|>'))\n",
        "\n",
        "\n",
        "plt.annotate('', xy=(-2, 0.25), xytext=(-3, 0.25),\n",
        "             arrowprops=dict(facecolor='black', arrowstyle='<|-'))\n",
        "\n",
        "plt.annotate('', xy=(3, 0.25), xytext=(2, 0.25),\n",
        "             arrowprops=dict(facecolor='black', arrowstyle='-|>'))\n",
        "\n",
        "plt.title('Intervalo de Confianza para la Media')\n",
        "plt.grid(True)\n",
        "plt.axis('off')\n",
        "# Mostrar la gr√°fica\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mNHO-JroG2wS",
        "cellView": "form"
      },
      "id": "mNHO-JroG2wS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por lo tanto, el intervalo de confianza para media poblacional $\\mu$ con una fiabilidad de $(1-\\alpha)\\times 100 \\%$ es:\n",
        "\n",
        "$$IC_{1-\\alpha} (\\mu) = \\Big(\\bar{X}-t_{n-1\\;\\alpha/2}\\frac{S_{X}}{\\sqrt{n}},\\bar{X}+t_{n-1\\;\\alpha/2}\\frac{S_{X}}{\\sqrt{n}}\\Big)=\\Big( \\bar{X} \\pm t_{n-1\\;\\alpha/2}\\frac{S_{X}}{\\sqrt{n}}\\Big)$$\n",
        "\n",
        "El error muestral es lo que la media poblacional $\\mu$ puede distanciarse de la media muestral $\\bar{X}$ con el nivel de confianza establecido. En la expresi√≥n $EM=t_{n-1;\\alpha/2}\\frac{S_{X}}{\\sqrt{n}}$ tenemos que:\n",
        "\n",
        "* A menor $S_{X}$, menor $EM$. Esto significa que si la muestra es poco dispersa (presenta pocas variaciones), entonces es m√°s probable que $\\bar{X}$ se acerque a $\\mu$.\n",
        "\n",
        "* A mayor tama√±o de la muestra $n$, menor $EM$. Obviamente, si el denominador es mayor, el $EM$ ser√° menor. Esto significa que una muestra muy numerosa nos da m√°s informaci√≥n sobre la poblaci√≥n.\n",
        "\n",
        "* A menor $\\alpha$, mayor $EM$. Con un intervalo m√°s amplio tendremos m√°s garant√≠as de *acertar*, lo que se consigue con un mayor nivel de confianza $1-\\alpha$.\n"
      ],
      "metadata": {
        "id": "jlbkeD3_7VZC"
      },
      "id": "jlbkeD3_7VZC"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Crea figura intervalo de confianza para la media\n",
        "plt.figure(figsize=(6,2))\n",
        "\n",
        "plt.annotate('', xy=(-2, 0), xytext=(2, 0),size=15,\n",
        "             arrowprops=dict(facecolor='black', arrowstyle='<|-|>'))\n",
        "\n",
        "\n",
        "plt.annotate('[', xy=(-1, -0.01), xytext=(-1, -0.02))\n",
        "plt.annotate(']', xy=(1, -0.01), xytext=(1, -0.02))\n",
        "\n",
        "plt.annotate(r'$\\mu$', xy=(2, 0), xytext=(2, 0))\n",
        "\n",
        "plt.annotate('|', xy=(0, -0.01), xytext=(0, -0.02))\n",
        "\n",
        "plt.text(0.05, -0.35, r'$\\bar{X}$', ha='center', va='bottom', color='black')\n",
        "\n",
        "plt.text(-1, 0.1, r'$\\bar{X}-EM$', ha='center', va='bottom', color='black')\n",
        "plt.text(1, 0.1, r'$\\bar{X}+EM$', ha='center', va='bottom', color='black')\n",
        "\n",
        "\n",
        "plt.annotate('', xy=(-1, -0.5), xytext=(-2, -0.5),size=2,\n",
        "             arrowprops=dict(facecolor='black', arrowstyle='|-|'))\n",
        "\n",
        "\n",
        "plt.annotate('', xy=(1, -0.5), xytext=(2, -0.5),size=2,\n",
        "             arrowprops=dict(facecolor='black', arrowstyle='|-|'))\n",
        "\n",
        "plt.text(0, -0.6, 'Intervalo de confianza', ha='center', va='bottom', color='black')\n",
        "\n",
        "# Establece los l√≠mites del gr√°fico\n",
        "plt.xlim(-3, 3)\n",
        "plt.ylim(-1, 1)\n",
        "\n",
        "# Muestra el gr√°fico\n",
        "plt.axis('off')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ClFfgIbFxhzh",
        "cellView": "form"
      },
      "id": "ClFfgIbFxhzh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "## Ejemplo pr√°ctico:\n",
        "***\n",
        "\n",
        "\n",
        "El gerente de un centro comercial desea estimar la cantidad media que gastan los clientes.\n",
        "\n",
        "Se toma una muestra aleatoria de 40 clientes que gastan de media 63.75‚Ç¨ y una desviaci√≥n t√≠pica de 9.01‚Ç¨.\n",
        "\n",
        "\n",
        "Calcular el intervalo de confianza para la media con nivel de confianza del 95%.\n",
        "\n",
        "***\n"
      ],
      "metadata": {
        "id": "6yk6NdrfMlSK"
      },
      "id": "6yk6NdrfMlSK"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title C√°lculo del valor t:\n",
        "from scipy.stats import t\n",
        "# Definir el nivel de significancia alpha\n",
        "alpha = 0.05\n",
        "\n",
        "# Definir los grados de libertad\n",
        "grados_libertad = 39\n",
        "\n",
        "# Calcular el valor cr√≠tico de t\n",
        "valor_t = t.ppf(1 - alpha/2, grados_libertad)\n",
        "\n",
        "print(\"Valor cr√≠tico de t:\", round(valor_t,4))\n"
      ],
      "metadata": {
        "id": "VgclCvcuTXfx"
      },
      "id": "VgclCvcuTXfx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title C√°lculo del intervalo de confianza directamente con Python\n",
        "# Datos de la muestra\n",
        "media_muestral = 63.75\n",
        "desviacion_estandar_muestral = 9.01\n",
        "tamano_muestra = 40\n",
        "\n",
        "# Nivel de confianza (1 - alpha)\n",
        "alpha = 0.05\n",
        "\n",
        "# Calcula el intervalo de confianza para la media\n",
        "intervalo_confianza = stats.t.interval(1 - alpha,\n",
        "                                       df=tamano_muestra - 1,\n",
        "                                       loc=media_muestral,\n",
        "                                       scale=desviacion_estandar_muestral/np.sqrt(tamano_muestra))\n",
        "\n",
        "print(\"Intervalo de confianza para la media poblacional:\", intervalo_confianza)"
      ],
      "metadata": {
        "id": "yrPdoqqCXr3d"
      },
      "id": "yrPdoqqCXr3d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejemplo usando nuestro texto de inter√©s:\n",
        "\n",
        "Supongamos que queremos calcular la media de la longitud de palabras que se usan en un texto.\n",
        "\n",
        "Para eso, tomamos como *muestra* el texto de ejemplo que hemos venido trabajando.\n",
        "\n",
        "¬øQu√© necsitamos conocer para calcular un intervalo de confianza para la longitud promedio de las palabras usadas en un texto?\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kRwrzes7ZTai"
      },
      "id": "kRwrzes7ZTai"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Ejemplo1 intervalo de confianza para la media PLN:\n",
        "\n",
        "# texto = \"\"\"\n",
        "# Este texto que est√°s leyendo ahora mismo sirve como un ejemplo ilustrativo para calcular el n√∫mero mediano de la longitud de palabras dentro de un texto determinado. Es un ejercicio pr√°ctico que nos permite comprender c√≥mo funcionan los c√°lculos de estad√≠sticas descriptivas aplicadas al an√°lisis ling√º√≠stico.\n",
        "\n",
        "# En este contexto, la longitud de las palabras se refiere al n√∫mero de caracteres que componen cada palabra individualmente. Al analizar este texto, encontrar√°s palabras cortas, como \"este\", \"es\", \"un\", \"de\", as√≠ como palabras m√°s largas como \"ilustrativo\", \"estad√≠sticas\", \"descriptivas\", entre otras.\n",
        "\n",
        "# El objetivo es determinar el n√∫mero mediano de caracteres que conforman las palabras en este texto. Al calcular este valor, podemos tener una idea m√°s clara de la extensi√≥n promedio de las palabras utilizadas aqu√≠. Este proceso implica ordenar las longitudes de las palabras de menor a mayor y encontrar el valor medio.\n",
        "\n",
        "# Este ejercicio es √∫til en diversos contextos, desde an√°lisis de texto en ling√º√≠stica computacional hasta la elaboraci√≥n de informes y an√°lisis de contenido en campos como la investigaci√≥n acad√©mica, la publicidad y el procesamiento del lenguaje natural.\n",
        "\n",
        "# Es importante destacar que el c√°lculo del n√∫mero mediano de longitud de palabras nos brinda informaci√≥n valiosa sobre la estructura y complejidad del lenguaje utilizado, lo que puede tener implicaciones significativas en la comunicaci√≥n efectiva y la comprensi√≥n del texto.\n",
        "# \"\"\"\n",
        "\n",
        "# display(texto)\n",
        "# display(texto_limpio)\n",
        "\n",
        "\n",
        "# # Tokenizamos el texto en palabras\n",
        "palabras = nltk.word_tokenize(texto_limpio)\n",
        "\n",
        "# Calculamos la longitud de cada palabra y almacenamos en una lista\n",
        "longitudes_palabras = [len(palabra) for palabra in palabras]\n",
        "\n",
        "# Calculamos la media muestral y la desviaci√≥n est√°ndar muestral\n",
        "media_muestral = np.mean(longitudes_palabras)\n",
        "desviacion_estandar_muestral = np.std(longitudes_palabras, ddof=1)  # Usamos ddof=1 para el estimador no sesgado de la varianza\n",
        "\n",
        "# Tama√±o de la muestra\n",
        "n = len(longitudes_palabras)\n",
        "\n",
        "# # Realizamos una inferencia sobre la media poblacional usando un intervalo de confianza del 95%\n",
        "alpha = 0.05\n",
        "\n",
        "\n",
        "# Calcular el valor cr√≠tico de t\n",
        "valor_t = t.ppf(1 - alpha/2, n-1)\n",
        "\n",
        "\n",
        "print(\"\\nTama√±o de la muestra:\", round(n))\n",
        "print(\"\\n Media muestral:\", round(media_muestral,4))\n",
        "print(\"\\n Desviaci√≥n t√≠pica muestral:\", round(desviacion_estandar_muestral,4))\n",
        "print(\"\\n 1- alpha = 0.95\")\n",
        "print(\"Valor cr√≠tico de t:\", round(valor_t,4))\n",
        "\n",
        "# Calcula el intervalo de confianza para la media\n",
        "intervalo_confianza = stats.t.interval(1 - alpha,\n",
        "                                       df=n - 1,\n",
        "                                       loc=media_muestral,\n",
        "                                       scale=desviacion_estandar_muestral/np.sqrt(n))\n",
        "\n",
        "print(\"Intervalo de confianza para la media poblacional:\", intervalo_confianza)\n"
      ],
      "metadata": {
        "id": "ZubNVyDVYqdh"
      },
      "id": "ZubNVyDVYqdh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intervalo de confianza para la proporci√≥n\n",
        "\n",
        "Para obtener el intervalo de confianza para la proporci√≥n $p$ con nivel de confianza $1-\\alpha$ usamos que, si se dan las condiciones de validez, se tiene\n",
        "\n",
        "$$\\frac{\\hat{p}-p}{\\sqrt{\\frac{p(1-p)}{n}}}\\thicksim N(0,1)$$\n",
        "\n",
        "\n",
        "Por lo tanto,\n",
        "\n",
        "$$1-\\alpha=P\\Big(-z_{\\alpha/2} < \\frac{\\hat{p}-p}{\\sqrt{\\frac{p(1-p)}{n}}} < z_{\\alpha/2}\\Big)$$\n",
        "\n",
        "Se despeja $p$. Para simplificar, si $n$ es grande ($n\\geq30$), suponemos que $\\hat{p}\\approx p$, de manera que la probabilidad anterior sea pr√°cticamente igual si sustituimos en el denominador $p(1-p)$ por $\\hat{p}(1-\\hat{p})$ y llamemos a $EM = z_{\\alpha/2}\\sqrt{\\hat{p}(1-\\hat{p}/n)}$ error muestral, es decir,\n",
        "\n",
        "\\begin{align*}\n",
        "    1-\\alpha&=P(-EM < \\hat{p}-p < EM)\\\\\n",
        "    &=P(\\hat{p}-EM < p < \\hat{p}+EM)\\\\\n",
        "    &=P(p\\in(\\hat{p}-EM,\\hat{p}+EM))    \n",
        "\\end{align*}\n",
        "\n",
        "Por lo tanto el intervalo de confianza para la proporci√≥n  $p$ con una fiabilidad del $1-\\alpha\\times100\\%$ es:\n",
        "\n",
        "\n",
        "\\begin{align*}\n",
        "    IC_{1-\\alpha}&=\\Big(\\hat{p}-z_{\\alpha/2}\\frac{\\sqrt{\\hat{p}(1-\\hat{p}}}{\\sqrt{n}},\\hat{p}+z_{\\alpha/2}\\frac{\\sqrt{\\hat{p}(1-\\hat{p}}}{\\sqrt{n}}\\Big)\\\\\n",
        "    &=\\Big( \\hat{p} \\pm z_{\\alpha/2}\\frac{\\sqrt{\\hat{p}(1-\\hat{p}}}{\\sqrt{n}} \\Big)\n",
        "\\end{align*}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KOWKxuWaVD9K"
      },
      "id": "KOWKxuWaVD9K"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Crea graficos intervalo para la proporci√≥n\n",
        "# Par√°metros\n",
        "n = 100  # Tama√±o de la muestra\n",
        "alpha = 0.05  # Nivel de confianza\n",
        "\n",
        "# C√°lculo de los valores cr√≠ticos de la distribuci√≥n normal est√°ndar\n",
        "z_left = -norm.ppf(1 - alpha / 2)\n",
        "z_right = norm.ppf(1 - alpha / 2)\n",
        "\n",
        "# Definir el rango para la gr√°fica\n",
        "x = np.linspace(-4, 4, 1000)\n",
        "y = norm.pdf(x)\n",
        "\n",
        "# Crear la figura y los ejes\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x, y, 'b-')\n",
        "# Rellenar el √°rea bajo la curva\n",
        "plt.fill_between(x, y, where=(x >= -2) & (x <= 2), color='lightblue', alpha=0.5)\n",
        "\n",
        "# L√≠neas verticales y etiquetas\n",
        "plt.axvline(0, color='red', linestyle='--', linewidth=0.3)\n",
        "plt.axhline(0, color='blue', linestyle='--', linewidth=0.3)\n",
        "\n",
        "plt.axvline(z_left, color='red', linestyle='--')\n",
        "plt.text(z_left, -0.01, r'$-z_{\\alpha/2}$', ha='right', va='bottom', color='red')\n",
        "plt.axvline(z_right, color='red', linestyle='--')\n",
        "plt.text(z_right, -0.01, r'$z_{\\alpha/2}$', ha='left', va='bottom', color='red')\n",
        "\n",
        "plt.text(0, -0.01, '0', ha='center', va='bottom', color='black')\n",
        "plt.text(0, 0.25, r'$1-\\alpha$', ha='center', va='bottom', color='red')\n",
        "\n",
        "plt.text(-2.5, 0.25, r'$\\alpha/2$', ha='center', va='bottom', color='red')\n",
        "plt.text(2.5, 0.25, r'$\\alpha/2$', ha='center', va='bottom', color='red')\n",
        "\n",
        "plt.annotate('', xy=(-2, 0.25), xytext=(2, 0.25),\n",
        "             arrowprops=dict(facecolor='black', arrowstyle='<|-|>'))\n",
        "\n",
        "plt.annotate('', xy=(-2, 0.25), xytext=(-3, 0.25),\n",
        "             arrowprops=dict(facecolor='black', arrowstyle='<|-'))\n",
        "\n",
        "plt.annotate('', xy=(3, 0.25), xytext=(2, 0.25),\n",
        "             arrowprops=dict(facecolor='black', arrowstyle='-|>'))\n",
        "\n",
        "plt.title('Intervalo de Confianza para la Media')\n",
        "plt.grid(True)\n",
        "plt.axis('off')\n",
        "# Mostrar la gr√°fica\n",
        "plt.show()\n",
        "\n",
        "\n",
        "################################################\n",
        "################################################\n",
        "################################################\n",
        "################################################\n",
        "plt.figure(figsize=(6,2))\n",
        "\n",
        "plt.annotate('', xy=(-2, 0), xytext=(2, 0),size=15,\n",
        "             arrowprops=dict(facecolor='black', arrowstyle='<|-|>'))\n",
        "\n",
        "\n",
        "plt.annotate('[', xy=(-1, -0.01), xytext=(-1, -0.02))\n",
        "plt.annotate(']', xy=(1, -0.01), xytext=(1, -0.02))\n",
        "\n",
        "plt.annotate(r'$p$', xy=(2, 0), xytext=(2, 0))\n",
        "\n",
        "plt.annotate('|', xy=(0, -0.01), xytext=(0, -0.02))\n",
        "\n",
        "plt.text(0.05, -0.35, r'$\\hat{p}$', ha='center', va='bottom', color='black')\n",
        "\n",
        "plt.text(-1, 0.1, r'$\\hat{p}-EM$', ha='center', va='bottom', color='black')\n",
        "plt.text(1, 0.1, r'$\\hat{p}+EM$', ha='center', va='bottom', color='black')\n",
        "\n",
        "\n",
        "plt.annotate('', xy=(-1, -0.5), xytext=(-2, -0.5),size=2,\n",
        "             arrowprops=dict(facecolor='black', arrowstyle='|-|'))\n",
        "\n",
        "\n",
        "plt.annotate('', xy=(1, -0.5), xytext=(2, -0.5),size=2,\n",
        "             arrowprops=dict(facecolor='black', arrowstyle='|-|'))\n",
        "\n",
        "plt.text(0, -0.6, 'Intervalo de confianza', ha='center', va='bottom', color='black')\n",
        "\n",
        "# Establece los l√≠mites del gr√°fico\n",
        "plt.xlim(-3, 3)\n",
        "plt.ylim(-1, 1)\n",
        "\n",
        "# Muestra el gr√°fico\n",
        "plt.axis('off')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "2DoFUw_pfeeQ"
      },
      "id": "2DoFUw_pfeeQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "## Ejemplo pr√°ctico:\n",
        "***\n",
        "\n",
        "\n",
        "Se hace una encuesta a 100 alumnos de la universidad sobre la aprobaci√≥n de las asignaturas, y de esos 100 alumnos, 65 han aprobado todas las asignaturas.\n",
        "\n",
        "Lo que queremos saber es el intervalo de confianza para la proporci√≥n de alumnos que han aprobado todas las asignaturas con nivel de confianza del 95%.\n",
        "\n",
        "\n",
        "***"
      ],
      "metadata": {
        "id": "OvCMHN4WfDfA"
      },
      "id": "OvCMHN4WfDfA"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title C√°lculo del valor normal:\n",
        "\n",
        "# Nivel de confianza (1 - alpha)\n",
        "alpha = 0.05\n",
        "\n",
        "# Calcular el valor cr√≠tico de z\n",
        "valor_z = norm.ppf(1 - alpha/2)\n",
        "\n",
        "print(\"Valor cr√≠tico de z:\", round(valor_z,4))\n"
      ],
      "metadata": {
        "id": "azDc7M9MgNdf"
      },
      "id": "azDc7M9MgNdf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title C√°lculo del intervalo de confianza directamente con Python\n",
        "\n",
        "\n",
        "# Datos de la muestra\n",
        "proporcion_muestral = 0.65  # Proporci√≥n muestral\n",
        "n = 100  # Tama√±o de la muestra\n",
        "\n",
        "# Nivel de confianza (1 - alpha)\n",
        "alpha = 0.05\n",
        "\n",
        "# Calcular el error est√°ndar\n",
        "error_estandar = math.sqrt((proporcion_muestral * (1 - proporcion_muestral)) / n)\n",
        "\n",
        "# Calcular el intervalo de confianza usando norm.interval()\n",
        "intervalo_confianza = norm.interval(1 - alpha, loc=proporcion_muestral, scale=error_estandar)\n",
        "\n",
        "# Mostrar el intervalo de confianza\n",
        "print(\"Intervalo de confianza para la proporci√≥n:\", intervalo_confianza)\n"
      ],
      "metadata": {
        "id": "-ohB_Q-hgsHT"
      },
      "id": "-ohB_Q-hgsHT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejemplo usando nuestro texto de inter√©s:\n",
        "\n",
        "Supongamos que queremos calcular el intervalo de confianza de la proporci√≥n de palabras que comienzan con la letra *e* en un texto.\n",
        "\n",
        "\n",
        "Para eso, tomamos como *muestra* el texto de ejemplo que hemos venido trabajando.\n",
        "\n",
        "¬øQu√© necsitamos conocer para calcular este intervalo de confianza para dicha proporci√≥n de palabras que comienzan con la letra *e* usadas en un texto?\n",
        "\n"
      ],
      "metadata": {
        "id": "UWzmi2CYhryc"
      },
      "id": "UWzmi2CYhryc"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Ejemplo1 intervalo de confianza para la proporci√≥n en PLN:\n",
        "\n",
        "# Tokenizamos el texto en palabras\n",
        "palabras = nltk.word_tokenize(texto_limpio)\n",
        "# Contamos el n√∫mero de palabras que comienzan con la letra 'e'\n",
        "palabras_con_e = [palabra for palabra in palabras if palabra.lower().startswith('e')]\n",
        "numero_palabras_con_e = len(palabras_con_e)\n",
        "print(numero_palabras_con_e)\n",
        "\n",
        "# Calculamos la proporci√≥n muestral de palabras que comienzan con 'e'\n",
        "total_palabras = len(palabras)\n",
        "\n",
        "print(total_palabras)\n",
        "proporcion_muestral = numero_palabras_con_e / total_palabras\n",
        "\n",
        "alpha = 0.05\n",
        "z_critical = stats.norm.ppf(1 - alpha/2)  # Valor cr√≠tico de z para el intervalo de confianza\n",
        "\n",
        "# Calculamos el error est√°ndar de la proporci√≥n muestral\n",
        "error_estandar_proporcion = np.sqrt((proporcion_muestral * (1 - proporcion_muestral)) / total_palabras)\n",
        "\n",
        "# Calculamos el intervalo de confianza para la proporci√≥n poblacional\n",
        "intervalo_confianza_proporcion = (proporcion_muestral - z_critical * error_estandar_proporcion,\n",
        "                                  proporcion_muestral + z_critical * error_estandar_proporcion)\n",
        "\n",
        "# Imprimimos resultados\n",
        "print(\"Proporci√≥n muestral de palabras que comienzan con 'e':\", proporcion_muestral)\n",
        "print(\"Intervalo de confianza del 95% para la proporci√≥n poblacional:\", intervalo_confianza_proporcion)\n"
      ],
      "metadata": {
        "id": "c-fWtJR1dWyA"
      },
      "id": "c-fWtJR1dWyA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contraste de hip√≥tesis\n",
        "\n",
        "Planteamos una afirmaci√≥n sobre la variable que estudiamos, la **hip√≥tesis nula** $H_{0}$. Para contrastarla tomamos una muestra $X_{1}, X_{2}, ..., X_{n}$ aleatoria e independiente.\n",
        "\n",
        "Un contraste de hip√≥tesis $H_{0}$ consiste en aplicar un **estad√≠stico** $T(X_{1}, X_{2}, ..., X_{n})$ y determinar, para el **nivel de significaci√≥n** $\\alpha >0$ una **regi√≥n de rechazo** $R(\\alpha)$ tal que, si $H_{0}$ fuese cierta, la probabilidad de que $T(X_{1}, X_{2}, ..., X_{n})$ tome su valor en $R(\\alpha)$ para una muestra aleatoria ser√≠a tan peque√±a como $\\alpha$. En tal caso, $T(X_{1}, X_{2}, ..., X_{n}) \\in R(\\alpha)$, rechazamos $H_{0}$ y nos quedamos con su opuesta, la **hip√≥tesis alternativa** $H_{1}$.\n",
        "\n",
        "Si en cambio $T(X_{1}, X_{2}, ..., X_{n}) \\notin R(\\alpha)$, o sea $T(X_{1}, X_{2}, ..., X_{n}) \\in R(\\alpha)^{c}=RA(\\alpha)$ (**la regi√≥n de aceptaci√≥n**), entonces aceptamos $H_{0}$ (o mejor dicho en rigor, no rechazamos $H_{0}$).\n",
        "\n",
        "En caso de rechazo decimos que tenemos evidencia estad√≠stica suficiente para hacerlo (depende del nivel de significaci√≥n) y en caso contrario no podemos rechazarla porque no tenemos esa evidencia. ***Pero tampoco la tenemos de que $H_{0}$ sea cierta, ¬°el contraste no sirve para demostrar $H_{0}$!***\n",
        "\n",
        "\n",
        "En muchos contrastes de hip√≥tesis es posible calcular el p-valor, un valor dado por la muestra tal que si es menor que $\\alpha$ entones se rechaza $H_{0}$.\n",
        "\n",
        "Cuando el $p-valor$ es pr√°cticamente nulo significa que nuestros datos contradicen muy claramente $H_{0}$.\n",
        "\n",
        "Esto es, tenemos evidencia estad√≠stica abrumadora en contra de $H_{0}$ y la rechazaremos con cualquier nivel de significaci√≥n razonable.\n"
      ],
      "metadata": {
        "id": "yUGrYAM4cIaA"
      },
      "id": "yUGrYAM4cIaA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Contraste para la media\n",
        "\n",
        "El contraste (bilateral) de la media en una poblaci√≥n normal consiste en las hip√≥tesis\n",
        "$$H_{0}:  \\mu = \\mu_{}  \\hspace{0.5cm} \\text{frente a} \\hspace{0.5cm} H_{1}:  \\mu \\neq \\mu_{}$$\n",
        "con una muestra de tama√±o $n$, $X_{1}, X_{2}, ..., X_{n}$.\n",
        "\n",
        "\n",
        "**Condiciones de validez:**\n",
        "\n",
        "*   Para todo $i, X_{i} \\thicksim N(\\mu,\\sigma)$ independientes.\n",
        "\n",
        "    Aplicamos el estad√≠stico:\n",
        "\n",
        "    $$t_{obs}=\\frac{\\bar{X}-\\mu_{0}}{S_{x}/\\sqrt{n}}$$\n",
        "\n",
        "*   Si $H_{0}$ es cierta se tiene que  $t_{obs} \\thicksim t_{n-1}$\n",
        "($t_{obs}$ es una variable aleatoria que retorna un valor para cada muestra de tama√±o $n$).\n",
        "\n",
        "    Por ello, tomamos para un nivel de significaci√≥n $\\alpha>0$, cuya regi√≥n de rechazo es:\n",
        "\n",
        "    $$R(\\alpha)=(-\\infty, -t_{(n-1;\\alpha/2)}] \\cup [t_{(n-1;\\alpha/2)},+\\infty )$$\n",
        "\n",
        "    Por lo tanto, rechazamos $H_{0}$ si\n",
        "    \n",
        "    $$|t_{obs}| \\geq t_{(n-1;\\alpha/2)}$$\n",
        "\n",
        "    O sea, aceptamos (no rechazamos) $H_{0}$ si\n",
        "    \n",
        "    $$|t_{obs}| < t_{(n-1;\\alpha/2)}$$\n",
        "    \n",
        "    La regi√≥n de aceptaci√≥n (de no rechazo) vine dada por:\n",
        "\n",
        "    $$RA(\\alpha) = (-t_{(n-1;\\alpha/2)},t_{(n-1;\\alpha/2)})$$\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "A $t_{obs}$ se le denomina **valor observado** del contraste, y a $t_{(n-1;\\alpha/2)}$ se le llama **valor cr√≠tico** del contraste.\n",
        "\n",
        "\n",
        "***Observaci√≥n:***\n",
        "\n",
        "Otra forma de realizar el contraste:\n",
        "\n",
        "\\begin{align*}\n",
        "    t_{obs} \\in RA(\\alpha) &\\Leftrightarrow  -t_{(n-1;\\alpha/2)} < \\frac{\\bar{X}-\\mu_{0}}{S_{X}/\\sqrt{n}} < t_{(n-1;\\alpha/2)}\\\\\n",
        "    &\\Leftrightarrow \\bar{X} - \\mu_{0} < EM, \\text{ donde  }  EM= t_{n-1;\\alpha/2}S_{X}/\\sqrt{n} \\\\\n",
        "    &\\Leftrightarrow \\bar{X} -EM < \\mu_{0} < \\bar{X} +EM\\\\\n",
        "    &\\Leftrightarrow \\mu_{0} \\in IC_{1-\\alpha}(\\mu)\n",
        "\\end{align*}\n",
        "\n",
        "Esto es, aceptamos $H_{0}$ si y s√≥lo si la media a contrastar esta en el intervalo de confianza que nos proporciona la muestra, para el nivel de confianza $1-\\alpha$.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eD-oj7oXpf6h"
      },
      "id": "eD-oj7oXpf6h"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Crea gr√°fico zona de rechazo/aceptaci√≥n para la media\n",
        "# Par√°metros\n",
        "n = 100  # Tama√±o de la muestra\n",
        "alpha = 0.05  # Nivel de confianza\n",
        "\n",
        "# C√°lculo de los valores cr√≠ticos de la distribuci√≥n t de Student\n",
        "t_left = -t.ppf(1 - alpha / 2, n - 1)\n",
        "t_right = t.ppf(1 - alpha / 2, n - 1)\n",
        "\n",
        "# Definir el rango para la gr√°fica\n",
        "x = np.linspace(-4, 4, 1000)\n",
        "y = t.pdf(x, n - 1)\n",
        "\n",
        "# Crear la figura y los ejes\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x, y, 'b-')\n",
        "# Rellenar el √°rea bajo la curva\n",
        "plt.fill_between(x, y, where=(x >= -2) & (x <= 2), color='lightblue', alpha=0.5)\n",
        "\n",
        "# L√≠neas verticales y etiquetas\n",
        "plt.axvline(0, color='red', linestyle='--', linewidth=0.3)\n",
        "\n",
        "plt.axhline(0, color='blue', linestyle='--', linewidth=0.3)\n",
        "\n",
        "plt.axvline(t_left, color='red', linestyle='--')\n",
        "plt.text(t_left, -0.01, r'$-t_{n-1,\\alpha/2}$', ha='right', va='bottom', color='red')\n",
        "plt.axvline(t_right, color='red', linestyle='--')\n",
        "plt.text(t_right, -0.01, r'$t_{n-1,\\alpha/2}$', ha='left', va='bottom', color='red')\n",
        "\n",
        "plt.text(0, -0.01, '0', ha='center', va='bottom', color='black')\n",
        "\n",
        "\n",
        "plt.text(-2.5, 0.3, r'$R(\\alpha)$', ha='center', va='bottom', color='red')\n",
        "\n",
        "\n",
        "plt.text(0, 0.06, r'$RA(\\alpha)$', ha='center', va='bottom', color='black')\n",
        "\n",
        "\n",
        "plt.annotate('', xy=(-2, 0.05), xytext=(2, 0.05),\n",
        "             arrowprops=dict(facecolor='black', arrowstyle='<|-|>'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plt.annotate('', xy=(-2.5, 0.30), xytext=(2.6, 0.255),\n",
        "             arrowprops=dict(facecolor='grey', arrowstyle='<|-',linestyle='dashed', linewidth=0.3))\n",
        "\n",
        "\n",
        "\n",
        "plt.annotate('', xy=(-2.5, 0.30), xytext=(-2.5, 0.255),\n",
        "             arrowprops=dict(facecolor='grey', arrowstyle='<|-',linestyle='dashed', linewidth=0.3))\n",
        "\n",
        "\n",
        "\n",
        "plt.annotate('', xy=(-2, 0.25), xytext=(-3, 0.25),\n",
        "             arrowprops=dict(facecolor='red', arrowstyle='<|-',color='red'))\n",
        "\n",
        "plt.annotate('', xy=(3, 0.25), xytext=(2, 0.25),\n",
        "             arrowprops=dict(facecolor='red', arrowstyle='-|>',color='red'))\n",
        "\n",
        "plt.axis('off')\n",
        "# Mostrar la gr√°fica\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ILiewutLbQCe"
      },
      "id": "ILiewutLbQCe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El $p-valor$ para este contraste mide la probabilidad de que el estad√≠stico $t_{obs}$ se aleje de $0$ al menos una cantidad $|t_{obs}|$, esto es, dado $T \\thicksim t_{n-1}$\n",
        "\n",
        "\n",
        "$$p-valor = P(|T| > |t_{obs}|) = 2P(T > |t_{obs}|)$$\n",
        "\n",
        "\n",
        "\n",
        "\\begin{align*}\n",
        "    \\text{Rechazamos} H_{0} & \\Leftrightarrow p-valor = 2P(T > |t_{obs}|) \\leq \\alpha\\\\\n",
        "    & \\Leftrightarrow P(T > |t_{obs}| )\\leq \\frac{\\alpha}{2}\\\\\n",
        "    & \\Leftrightarrow t_{n-1;\\alpha/2} \\leq |t_{obs}|\n",
        "\\end{align*}"
      ],
      "metadata": {
        "id": "weFAydFzG6Z2"
      },
      "id": "weFAydFzG6Z2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "Ejemplo\n",
        "***\n",
        "\n",
        "Se desea investigar la superÔ¨Åcie (en $m^{2}$) de los pisos en venta en el mercado inmobiliario de una ciudad.\n",
        "\n",
        "Para esto, se cuenta con la informaci√≥n proporcionada por una muestra aleatoria de 31 viviendas:\n",
        "\n",
        "SUPERFICIE (en $m^{2}$):\n",
        "\n",
        "$$[ 85.8, 49.2, 65.7, 58.3, 65.2, 52.9, 87.2, 90.3, 51.6, 69.4,\n",
        "75.9, 70.3, 76.0, 71.9, 65.2, 82.3, 70.9, 70.8, 74.6, 52.1,\n",
        "34.1, 81.9, 86.1, 76.1, 55.4, 63.2, 105.1, 59.9, 62.7, 85.9,\n",
        "84.5]$$\n",
        "\n",
        "\n",
        "Queremos comprobar si la superficie media de los pisos ofertados es o no igual a $75 m^{2}$.\n",
        "\n",
        "¬øQu√© necesitamos para esto?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TpES8OTilwbq"
      },
      "id": "TpES8OTilwbq"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Ejemplo:\n",
        "superficies = [85.8, 49.2, 65.7, 58.3, 65.2, 52.9, 87.2, 90.3, 51.6, 69.4,75.9, 70.3, 76.0, 71.9, 65.2, 82.3, 70.9, 70.8, 74.6, 52.1,34.1, 81.9, 86.1, 76.1, 55.4, 63.2, 105.1, 59.9, 62.7, 85.9,84.5]\n",
        "\n",
        "X_barra = np.mean(superficies)\n",
        "print(f\" La superficie media de los pisos en la muestra es {X_barra}\")\n",
        "\n",
        "S_X = np.std(superficies, ddof=1)\n",
        "\n",
        "print(f\" La desviaci√≥n t√≠pica de la superficie de los pisos en la muestra es {S_X}\")\n",
        "\n",
        "n= len(superficies)\n",
        "\n",
        "print(f\"El tama√±o de la muestra es {n}\")\n",
        "\n",
        "\n",
        "#aqu√≠ mostramos los diferentes valores de t, dependiendo el valor del alfa:\n",
        "\n",
        "for alpha in [0.1,0.01,0.05]:\n",
        "\n",
        "  # Definir los grados de libertad\n",
        "  grados_libertad = n-1\n",
        "\n",
        "  # Calcular el valor cr√≠tico de t\n",
        "  valor_t = t.ppf(1 - alpha/2, grados_libertad)\n",
        "\n",
        "  print(f\"\\nValor cr√≠tico de t para alpha = {alpha}: {round(valor_t,4)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "SkkhfqY56tmo"
      },
      "id": "SkkhfqY56tmo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Contraste de hip√≥tesis para la media con Python\n",
        "\n",
        "\n",
        "# Hip√≥tesis nula: La media de los datos es igual a mu\n",
        "mu = 75\n",
        "\n",
        "# Realizamos la prueba de hip√≥tesis\n",
        "t_statistic, p_value = stats.ttest_1samp(superficies, mu)\n",
        "\n",
        "# Definimos el nivel de significancia\n",
        "print(\"Estad√≠stico t:\", t_statistic)\n",
        "print(\"Valor p:\", p_value)\n",
        "\n",
        "for alpha in [0.1,0.01,0.05]:\n",
        "  if p_value < alpha:\n",
        "      print(\"Rechazamos la hip√≥tesis nula: Hay evidencia suficiente para afirmar que la media es diferente de\", mu)\n",
        "  else:\n",
        "      print(\"No rechazamos la hip√≥tesis nula: No hay suficiente evidencia para afirmar que la media es diferente de\", mu)"
      ],
      "metadata": {
        "id": "4FbRIQdgPjxE"
      },
      "id": "4FbRIQdgPjxE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Contraste para la proporci√≥n\n",
        "\n",
        "En el contraste (bilateral) de la proporci√≥n se contrasta $p_{0}$ como valor de la proporci√≥n $p$, esto es:\n",
        "\n",
        "\\begin{cases}\n",
        "    H_{0}: p= p_{0}\\\\\n",
        "    H_{1}: p \\neq p_{0}\n",
        "\\end{cases}\n",
        "\n",
        "***Condiciones de validez:***\n",
        "\n",
        "*   La muestra es independiente y representativa de la poblaci√≥n\n",
        "*   $n \\geq 0$, o bien\n",
        "    \\begin{cases}\n",
        "        n\\hat{p} = k \\geq 5\\\\\n",
        "        n(1-\\hat{p})=n-k \\geq 5\n",
        "    \\end{cases}\n",
        "\n",
        "El **valor observado** de contraste es el estad√≠stico $$Z_{obs}= \\frac{\\hat{p}-p_{0}}{\\sqrt{\\frac{p_{0}(1-p_{0})}{n}}}$$\n",
        "\n",
        "Si $H_{0}$ es cierta se tiene que $Z_{obs}$ se distribuye aproximadamente como una normal est√°ndar.\n",
        "Con esto, y fijando un nivel de significancia $\\alpha$, si ocurre $Z_{obs}\\geq Z_{\\alpha/2}$ o $Z_{obs}\\leq -Z_{\\alpha/2}$, entonces rechazamos $H_{0}$ (lo que implica quee no se considera fiabl que $p=p_{0}$, porque en tal caso ser√≠a muy extra√±o que la muestra fuera as√≠).\n",
        "\n",
        "\n",
        "La muestra est√° en la **regi√≥n de rechazo** $R(\\alpha) \\Leftrightarrow |Z_{obs}| \\geq Z_{\\alpha/2}$.\n",
        "\n",
        "La muestra est√° en la **regi√≥n de aceptaci√≥n**  $RA(\\alpha) \\Leftrightarrow Z_{obs} \\in (-Z_{\\alpha/2},Z_{\\alpha/2})$.\n",
        "\n",
        "Sea $Z \\thicksim N(0,1)$,\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\\begin{align*}\n",
        "    \\text{Rechazamos } H_{0} & \\Leftrightarrow p-valor = P(|Z| > |Z_{obs}| )=2P(Z > |Z_{obs}|) \\leq \\alpha\\\\\n",
        "    & \\Leftrightarrow P(Z > |Z_{obs}| )\\leq \\frac{\\alpha}{2}\\\\\n",
        "    & \\Leftrightarrow Z_{\\alpha/2} \\leq |Z_{obs}|\n",
        "\\end{align*}\n"
      ],
      "metadata": {
        "id": "rkPJepkNJDZ6"
      },
      "id": "rkPJepkNJDZ6"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Crea gr√°fico zona de rechazo/aceptaci√≥n para la proporci√≥n\n",
        "# Par√°metros\n",
        "# Par√°metros\n",
        "n = 100  # Tama√±o de la muestra\n",
        "alpha = 0.05  # Nivel de confianza\n",
        "\n",
        "# C√°lculo de los valores cr√≠ticos de la distribuci√≥n normal est√°ndar\n",
        "z_left = -norm.ppf(1 - alpha / 2)\n",
        "z_right = norm.ppf(1 - alpha / 2)\n",
        "\n",
        "# Definir el rango para la gr√°fica\n",
        "x = np.linspace(-4, 4, 1000)\n",
        "y = norm.pdf(x)\n",
        "\n",
        "# Crear la figura y los ejes\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x, y, 'b-')\n",
        "# Rellenar el √°rea bajo la curva\n",
        "plt.fill_between(x, y, where=(x >= -2) & (x <= 2), color='lightblue', alpha=0.5)\n",
        "\n",
        "# L√≠neas verticales y etiquetas\n",
        "plt.axvline(0, color='red', linestyle='--', linewidth=0.3)\n",
        "\n",
        "plt.axhline(0, color='blue', linestyle='--', linewidth=0.3)\n",
        "\n",
        "plt.axvline(t_left, color='red', linestyle='--')\n",
        "plt.text(t_left, -0.01, r'$-Z_{\\alpha/2}$', ha='right', va='bottom', color='red')\n",
        "plt.axvline(t_right, color='red', linestyle='--')\n",
        "plt.text(t_right, -0.01, r'$Z_{\\alpha/2}$', ha='left', va='bottom', color='red')\n",
        "\n",
        "plt.text(0, -0.01, '0', ha='center', va='bottom', color='black')\n",
        "\n",
        "\n",
        "plt.text(-2.5, 0.3, r'$R(\\alpha)$', ha='center', va='bottom', color='red')\n",
        "\n",
        "\n",
        "plt.text(0, 0.06, r'$RA(\\alpha)$', ha='center', va='bottom', color='black')\n",
        "\n",
        "\n",
        "plt.annotate('', xy=(-2, 0.05), xytext=(2, 0.05),\n",
        "             arrowprops=dict(facecolor='black', arrowstyle='<|-|>'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plt.annotate('', xy=(-2.5, 0.30), xytext=(2.6, 0.255),\n",
        "             arrowprops=dict(facecolor='grey', arrowstyle='<|-',linestyle='dashed', linewidth=0.3))\n",
        "\n",
        "\n",
        "\n",
        "plt.annotate('', xy=(-2.5, 0.30), xytext=(-2.5, 0.255),\n",
        "             arrowprops=dict(facecolor='grey', arrowstyle='<|-',linestyle='dashed', linewidth=0.3))\n",
        "\n",
        "\n",
        "\n",
        "plt.annotate('', xy=(-2, 0.25), xytext=(-3, 0.25),\n",
        "             arrowprops=dict(facecolor='red', arrowstyle='<|-',color='red'))\n",
        "\n",
        "plt.annotate('', xy=(3, 0.25), xytext=(2, 0.25),\n",
        "             arrowprops=dict(facecolor='red', arrowstyle='-|>',color='red'))\n",
        "\n",
        "plt.axis('off')\n",
        "# Mostrar la gr√°fica\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DLOwzrkEBtII"
      },
      "id": "DLOwzrkEBtII",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "**Ejemplo:**\n",
        "***\n",
        "\n",
        "Continuamos con el ejercicio de las superficies de los pisos de la ciudad.\n",
        "Pero esta vez, la informaci√≥n de la que disponemos es que un directivo inmobiliario afirma que el porcentaje de pisos de menos de $60m^{2}$ es el 15%.\n",
        "\n",
        "¬øSe sostiene su afirmaci√≥n con los tres niveles de significaci√≥n que estamos trabajando? ($\\alpha = \\{0.1, 0.01, 0.05\\}$)\n",
        "\n"
      ],
      "metadata": {
        "id": "crVSrSIgWX_Y"
      },
      "id": "crVSrSIgWX_Y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "\n",
        "***"
      ],
      "metadata": {
        "id": "joh94SjpnW5Q"
      },
      "id": "joh94SjpnW5Q"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Extracci√≥n de los datos:\n",
        "\n",
        "# Contar el n√∫mero de superficies menores a 60 metros\n",
        "menor_60 = sum(1 for superficie in superficies if superficie < 60)\n",
        "\n",
        "n = len(superficies)\n",
        "print(f\"El tama√±o de la muestra es {n}\")\n",
        "\n",
        "# Proporci√≥n muestral\n",
        "proporcion_muestral = menor_60 / n\n",
        "\n",
        "print(f\"La proporci√≥n muestral p gorro es {round(proporcion_muestral,2)}\")\n",
        "\n",
        "# Proporci√≥n hipot√©tica\n",
        "proporcion_hipotetica = 0.15\n",
        "\n",
        "print(f\"La proporci√≥n hipot√©tica p0 es {round(proporcion_hipotetica,2)}\")\n",
        "\n",
        "# Nivel de confianza (1 - alpha)\n",
        "for alpha in [0.1,0.01,0.05]:\n",
        "\n",
        "  # Calcular el valor cr√≠tico de z\n",
        "  valor_z = norm.ppf(1 - alpha/2)\n",
        "\n",
        "  print(f\"Valor cr√≠tico de z para un Alfa igual a {alpha} es : {round(valor_z,4)}\")\n",
        "\n",
        "\n",
        "# Calcular el estad√≠stico de contraste Z\n",
        "Z_obs = (proporcion_muestral - proporcion_hipotetica) / ((proporcion_hipotetica * (1 - proporcion_hipotetica)) ** 0.5 / n ** 0.5)\n",
        "\n",
        "# print(f\"El valor del estad√≠stico observado es {Z_obs}\")\n"
      ],
      "metadata": {
        "id": "QSSQg2EwaLDu"
      },
      "id": "QSSQg2EwaLDu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ejemplo con Python:\n",
        "\n",
        "# Calcular el valor p\n",
        "p_valor = 2 * stats.norm.cdf(-abs(Z_obs))\n",
        "\n",
        "print(f\"El p-valor es: {p_valor}\")\n",
        "\n",
        "for alpha in [0.1,0.01,0.05]:\n",
        "  # Decisi√≥n\n",
        "  if p_valor < alpha:\n",
        "      print(\"Rechazamos la hip√≥tesis nula.\")\n",
        "  else:\n",
        "      print(\"No podemos rechazar la hip√≥tesis nula.\")"
      ],
      "metadata": {
        "id": "94VogyRhadKf"
      },
      "id": "94VogyRhadKf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Consideraciones sobre la muestra:\n",
        "\n",
        "\n",
        "\n",
        "*   **Obtenci√≥n**: Debe ser independiente (los valores de cada caso no deben tener influencia en los dem√°s) y tambi√©n representativa (se necesita, para que la inferencia sea justificada, esperar que los valores de la muestra se asemejen en principio a los que se dan en toda la poblaci√≥n).\n",
        "\n",
        "*   **Tama√±o de la muestra**: Cuando calculamos un intervalo de confianza el grado de imprecisi√≥n lo da el error muestral. Obviamente, es menor cuanto mayor es el tama√±o de la muestra, pero puede ser impracticable (o tal vez car√≠simo) conseguir que el tama√±o sea muy grande. Lo que se hace es calcular el tama√±o que necesitar√≠amos para no rebasar un error muestral objetivo, y luego decidir en consecuencia.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1Pjf52tSNZO6"
      },
      "id": "1Pjf52tSNZO6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tama√±o de la muestra para la media\n",
        "\n",
        "En la inferencia sobre la media de una poblaci√≥n normal vimos que el error muestral se calcula\n",
        "$$EM= t_{n-1;\\alpha/2}\\frac{S_{X}}{n}$$\n",
        "\n",
        "Llamamos $EM_{obj}$ a nuestro objetivo y necesitamos que\n",
        "\n",
        "$$t_{n-1;\\alpha/2}\\frac{S_{X}}{\\sqrt{n}} \\leq EM_{obj}$$\n",
        "\n",
        "Para poder *despejar* $n$ sustituimos $t_{n-1;\\alpha/2}$ por $Z_{\\alpha/2}$.\n",
        "Obviamente no tenemos $S_{X}$, por lo que tomamos $S$, ya sea la *cuasivarianza* t√≠pica de una muestra previa o alguna otra estimaci√≥n fiable de la desviaci√≥n t√≠pica poblacional.\n",
        "\n",
        "Buscamos entonces:\n",
        "\n",
        "$$Z_{\\alpha/2}\\frac{S}{\\sqrt{n}} \\leq EM_{obj} \\Leftrightarrow Z_{\\alpha/2}\\frac{S}{EM_{obj}} \\leq \\sqrt{n} \\Leftrightarrow n \\geq Z_{\\alpha/2}^{2}\\frac{S^{2}}{EM_{obj}^{2}}$$\n",
        "\n"
      ],
      "metadata": {
        "id": "fKDZtWwBwlC6"
      },
      "id": "fKDZtWwBwlC6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "**Ejemplo**\n",
        "***\n",
        "\n",
        "Se quiere estimar el sueldo medio de un trabajador de transporte p√∫blico. Si se quiere que el error m√°ximo de la estimaci√≥n sea de 10 euros, calcular el tama√±o m√≠nimo de la muestra que se debe tomar considerando un nivel de confianza del 99% y admitiendo que un buen estimador de la desviaci√≥n t√≠pica poblacional es la cuasidesviaci√≥n t√≠pica muestral de 250 euros.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uRbbZsEIf2g3"
      },
      "id": "uRbbZsEIf2g3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tama√±o de la muestra para la proporci√≥n\n",
        "\n",
        "Vimos que el error muestral para $IC_{1-\\alpha}(p)$ es:\n",
        "\n",
        "$$EM = z_{\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}$$\n",
        "\n",
        "Nos planteamos dos situaciones:\n",
        "\n",
        "*   Hemos obtenido ya $\\hat{p}$ con una muestra y queremos calcular $n$ para conseguir un valor $EM_{obj}$ *(error muestral objetivo)* con la misma $\\hat{p}$.\n",
        "\n",
        "    Por lo tanto tenemos:\n",
        "\n",
        "    \n",
        "$$Z_{\\alpha/2}\\frac{\\sqrt{\\hat{p}(1-\\hat{p})}}{\\sqrt{n}} \\leq EM_{obj} \\Leftrightarrow Z_{\\alpha/2}\\frac{\\sqrt{\\hat{p}(1-\\hat{p})}}{EM_{obj}} \\leq \\sqrt{n} \\Leftrightarrow n \\geq Z_{\\alpha/2}^{2}\\frac{\\hat{p}(1-\\hat{p})}{EM_{obj}^{2}}$$\n",
        "\n",
        "*   Queremos conseguir un valor $EM_{obj}$ independientemente de qu√© $\\hat{p}$ obtengamos.\n",
        "Entonces, cambiamos $\\hat{p}(1-\\hat{p})$ por su m√°ximo valor posible en $\\hat{p} \\in (0,1)$, que es cuando $\\hat{p}=1/2$, en cuyo caso, $\\hat{p}(1-\\hat{p})=1/4$.\n",
        "Tomamos entonces\n",
        "\n",
        "$$n \\geq Z_{\\alpha/2}^{2} \\frac{1}{4EM^{2}_{obj}}$$\n",
        "\n",
        "\n",
        "***NOTA:***\n",
        "\n",
        "En ambas situaciones, como nos basamos en la aproximaci√≥n a la distribuci√≥n normal est√°ndar de $$\\frac{\\hat{p}-p}{\\sqrt{\\frac{p(1-p)}{n}}}$$ para dar por bueno lo anterior, necesitamos obtener $n \\geq 30$.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cflJzc5d3GqD"
      },
      "id": "cflJzc5d3GqD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "**Ejemplo:**\n",
        "***\n",
        "\n",
        "Un investigador pretende estimar la proporci√≥n de hombres que fuman en exceso y que desarrollar√°n c√°ncer pulmonar en los pr√≥ximos a√±os.\n",
        "El investigador desea seleccionar cierto n√∫mero de hombres que hayan fumado mucho durante los √∫ltimos a√±os y observarlos en los a√±os venideros para saber cu√°ntos desarrollar√°n c√°ncer pulmonar. ¬øCu√°l debe ser el tama√±o de la muestra para que, con una probabilidad aproximada de 0.95, la proporci√≥n muestral se encuentre a menos de 0.02 unidades de la proporci√≥n verdadera?"
      ],
      "metadata": {
        "id": "HwvR4jjmf-2W"
      },
      "id": "HwvR4jjmf-2W"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RESUMEN:\n",
        "\n",
        "| Sobre la media                                                                 | Sobre la proporci√≥n                                                      |\n",
        "|--------------------------------------------------------------------------------|---------------------------------------------------------------------------|\n",
        "| - Intervalo de Confianza:                                                      | - Intervalo de confianza:                                                |\n",
        "|                                                                                  |                                                                           |\n",
        "| $$\\Big( \\bar{X} \\pm t_{n-1\\;\\alpha/2}\\frac{S_{X}}{\\sqrt{n}}\\Big)$$             | $$\\Big( \\hat{p} \\pm z_{\\alpha/2}\\frac{\\sqrt{\\hat{p}(1-\\hat{p})}}{\\sqrt{n}} \\Big)$$ |\n",
        "|                                                                                  |                                                                           |\n",
        "| - Contraste de hip√≥tesis:                                                       | - Contraste de hip√≥tesis:                                                |\n",
        "|                                                                                  |                                                                           |\n",
        "| $$t_{obs}=\\frac{\\bar{X}-\\mu_{0}}{S_{x}/\\sqrt{n}},$$                            | $$Z_{obs}= \\frac{\\hat{p}-p_{0}}{\\sqrt{\\frac{p_{0}(1-p_{0})}{n}}},$$        |\n",
        "| $$RA(\\alpha) = (\\pm t_{(n-1;\\alpha/2)}),$$                                      | $$RA(\\alpha) = (\\pm Z_{\\alpha/2}),$$                                      |\n",
        "| $$p-valor = 2P(T > |t_{obs}|) \\text{ con } T \\thicksim t_{n-1}$$                | $$p-valor = 2P(Z > |Z_{obs}|) \\text{ con } Z \\thicksim N(0,1)$$           |\n",
        "|                                                                                  |                                                                           |\n",
        "| - Tama√±o de la muestra:                                                         | - Tama√±o de la muestra:                                                   |\n",
        "|                                                                                  |                                                                           |\n",
        "| $$n \\geq Z_{\\alpha/2}^{2} \\frac{S^{2}}{EM^{2}_{obj}}$$                         | - Con informaci√≥n muestral:                                              |\n",
        "|                                                                                  |                                                                           |\n",
        "|                                                                                  | $$n \\geq Z_{\\alpha/2}^{2}\\frac{\\hat{p}(1-\\hat{p})}{EM_{obj}^{2}}$$        |\n",
        "|                                                                                  |                                                                           |\n",
        "|                                                                                  | - Sin informaci√≥n adicional:                                              |\n",
        "|                                                                                  |                                                                           |\n",
        "|                                                                                  | $$n \\geq Z_{\\alpha/2}^{2} \\frac{1}{4EM^{2}_{obj}}$$                       |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9m4_LYJdAQYU"
      },
      "id": "9m4_LYJdAQYU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "***\n"
      ],
      "metadata": {
        "id": "yJYm4ZiqQGtJ"
      },
      "id": "yJYm4ZiqQGtJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "***\n",
        "\n",
        "# **Estad√≠stica Multivariante**\n",
        "\n",
        "Estudiamos los datos, en $N$ casos de dos variables $X$ e $Y$.\n",
        "\n",
        "Queremos analizar la relaci√≥n entre ellas y medir el grado de asociaci√≥n detectada.\n",
        "\n",
        "En este contexto, la distribuci√≥n marginal de $X$ es la de dicha variable por s√≠ sola, sin tener en cuenta los valores de $Y$.\n",
        "\n",
        "**Para las variables de tipo num√©rico:**\n",
        "*   *Presentaci√≥n de los datos:* tabla de frecuencias de doble entrada.\n",
        "*   *Relaci√≥n lineal:* recta de regresi√≥n (con predicci√≥n) y coeficiente de correlaci√≥n.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PGMhC6C8QKeZ"
      },
      "id": "PGMhC6C8QKeZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tablas de frecuencia de doble entrada\n",
        "\n",
        "Se parte de dos variables $X$ e $Y$ tomadas en $N$ casos.\n",
        "\n",
        "En las filas se indican los valores (o las marcas de clase de los intervalos) de $X$ y en las columnas los de $Y$.\n",
        "\n",
        "El valor de la fila $i$ y la columna $j$ es la frecuencia absoluta (n√∫mero de casos) que se sit√∫an en el $i$-√©simo valor (o intervalo) de $X$ y el $j$-√©simo de $Y : n_{ij}$.\n",
        "\n",
        "La tabla se completa a√±adiendo:\n",
        "\n",
        "* Una nueva columna que refleja las frecuencias marginales de $X (n_{i}: \\text{ frecuencia absoluta del i-√©simo valor de } X)$\n",
        "* Una nueva fila con las frecuencias marginales de $Y (n_{j}: \\text{ frecuencia absoluta del j-√©simo valor de } Y)$\n",
        "\n",
        "Tambi√©n se pueden calcular las frecuencias relativas, aunque estas no se suelen ubicar en la tabla. Se calculan haciendo el cociente con la frecuencia total de casos:\n",
        "\n",
        "$$f_{ij}=\\frac{n_{ij}}{N}$$\n",
        "$$ f_{i}=\\frac{n_{i}}{N}$$\n",
        "$$f_{j}=\\frac{n_{j}}{N}$$\n",
        "\n",
        "\n",
        "Esta informaci√≥n puede ser presentada en formato largo por columnas, donde cada fila recoge el par $x_{i}$ e $y_{i}$ junto con su frecuencia $n_{ij}$.\n",
        "\n",
        "***\n",
        "**Ejemplo**\n",
        "***\n",
        "\n",
        "El **Polinesio simplificado** es un *lenguaje* simplificado que tiene s√≥lo 8 letras y cuya estructura sil√°bica vine dada por las siguientes frecuencias:\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>X ‚Üì \\ Y ‚Üí</th>\n",
        "    <th>$y_{1}=a$</th>\n",
        "    <th>$y_{2}=i$</th>\n",
        "    <th>$y_{3}=u$</th>\n",
        "    <th>$n_{(i‚àô)}$</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>$x_{1}=p$</td>\n",
        "    <td>$n_{1,1}=80$</td>\n",
        "    <td>$n_{1,2}=80$</td>\n",
        "    <td>$n_{1,3}=0$</td>\n",
        "    <td>$n_{(1‚àô)}=160$</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>$x_{2}=t$</td>\n",
        "    <td>$n_{2,1}=80$</td>\n",
        "    <td>$n_{2,2}=40$</td>\n",
        "    <td>$n_{2,3}=40$</td>\n",
        "    <td>$n_{(2‚àô)}=160$</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>$x_{3}=k$</td>\n",
        "    <td>$n_{3,1}=40$</td>\n",
        "    <td>$n_{3,2}=0$</td>\n",
        "    <td>$n_{3,3}=40$</td>\n",
        "    <td>$n_{(3‚àô)}=80$</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>$x_{4}=s$</td>\n",
        "    <td>$n_{4,1}=80$</td>\n",
        "    <td>$n_{4,2}=40$</td>\n",
        "    <td>$n_{4,3}=40$</td>\n",
        "    <td>$n_{(4‚àô)}=160$</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>$x_{5}=h$</td>\n",
        "    <td>$n_{5,1}=40$</td>\n",
        "    <td>$n_{5,2}=0$</td>\n",
        "    <td>$n_{5,3}=40$</td>\n",
        "    <td>$n_{(5‚àô)}=80$</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>$n_{(‚àôj)}$</td>\n",
        "    <td>$n_{(‚àô1)}=320$</td>\n",
        "    <td>$n_{(‚àô2)}=160$</td>\n",
        "    <td>$n_{(‚àô3)}=160$</td>\n",
        "    <td>$N=640$</td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "* La variable $X$ representa las consonantes\n",
        "* La variable $Y$ representa las vocales.\n",
        "\n",
        "Adem√°s, podemos observar que:\n",
        "\n",
        "* El valor $ùëõ_{(1‚àô)}=ùëõ_{1,1}+ùëõ_{1,2}+ùëõ_{1,3}= 80+80+0 = 160$ es la frecuencia marginal de $ùë•_1$ e indica que 160 de las palabras del texto ejemplo en este *lenguaje* contiene la consonante $p$.\n",
        "\n",
        "* El valor $ùëõ_{(‚àô2)}=ùëõ_{1,2}+ùëõ_{2,2}+ùëõ_{3,2}+ùëõ_{4,2}+n_{5,2}=80+40+0+40+0=160$ es la frecuencia marginal de $ùë¶_2$ e indica que 160 de las palabras del texto ejemplo en este *lenguaje* contiene la vocal $i$.\n",
        "\n",
        "* El valor $f_{1,2}=\\frac{n_{1,2}}{N}=\\frac{80}{640}=\\frac{1}{8} = 0.125 \\times 100 \\text{%} = 12.5 \\text{%} $, lo que implica que el $12.5\\text{%}$ de las palabras contienen la consonante  $p$ y la vocal $i$.\n",
        "\n",
        "\n",
        "* El valor $f_{(1‚àô)} = \\frac{n_{(1‚àô)}}{N}=\\frac{160}{640}=\\frac{1}{4}=0.25\\times100\\text{%}=25\\text{%}$, lo que implica que el $25\\text{%}$ de las palabras de este lenguaje contiene la consonante $p$.\n",
        "\n",
        "\n",
        "* El valor $f_{(‚àô1)} = \\frac{n_{(‚àô1)} }{N}=\\frac{320}{640}=\\frac{1}{2}=0.5\\times100\\text{%}=50\\text{%}$, lo que implica que el $50\\text{%}$ de las palabras de este lenguaje contiene la vocal $a$.\n",
        "\n"
      ],
      "metadata": {
        "id": "gDbKlHqNSK9q"
      },
      "id": "gDbKlHqNSK9q"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title C√°lculo de las respectivas frecuencias con Python\n",
        "# Definici√≥n de la tabla de frecuencias\n",
        "tabla_frecuencias = {\n",
        "    ('p', 'a'): 80, ('p', 'i'): 80, ('p', 'u'): 0,\n",
        "    ('t', 'a'): 80, ('t', 'i'): 40, ('t', 'u'): 40,\n",
        "    ('k', 'a'): 40, ('k', 'i'): 0,  ('k', 'u'): 40,\n",
        "    ('s', 'a'): 80, ('s', 'i'): 40, ('s', 'u'): 40,\n",
        "    ('h', 'a'): 40, ('h', 'i'): 0,  ('h', 'u'): 40\n",
        "}\n",
        "\n",
        "# Calcular frecuencias marginales\n",
        "frecuencias_marginales_x = {}\n",
        "frecuencias_marginales_y = {}\n",
        "\n",
        "for (x, y), frecuencia in tabla_frecuencias.items():\n",
        "    if x in frecuencias_marginales_x:\n",
        "        frecuencias_marginales_x[x] += frecuencia\n",
        "    else:\n",
        "        frecuencias_marginales_x[x] = frecuencia\n",
        "\n",
        "    if y in frecuencias_marginales_y:\n",
        "        frecuencias_marginales_y[y] += frecuencia\n",
        "    else:\n",
        "        frecuencias_marginales_y[y] = frecuencia\n",
        "\n",
        "# Calcular frecuencias absolutas marginales\n",
        "frecuencia_absoluta_marginal_x = sum(frecuencias_marginales_x.values())\n",
        "frecuencia_absoluta_marginal_y = sum(frecuencias_marginales_y.values())\n",
        "\n",
        "# Calcular frecuencias totales\n",
        "total_casos = sum(tabla_frecuencias.values())\n",
        "\n",
        "# Imprimir frecuencias marginales\n",
        "print(\"Frecuencias marginales de X:\")\n",
        "for x, frecuencia in frecuencias_marginales_x.items():\n",
        "    print(f\"{x}: {frecuencia} ({frecuencia / frecuencia_absoluta_marginal_x * 100:.2f}%)\")\n",
        "\n",
        "print(\"\\nFrecuencias marginales de Y:\")\n",
        "for y, frecuencia in frecuencias_marginales_y.items():\n",
        "    print(f\"{y}: {frecuencia} ({frecuencia / frecuencia_absoluta_marginal_y * 100:.2f}%)\")\n",
        "\n",
        "# Calcular frecuencias relativas\n",
        "frecuencias_relativas = {}\n",
        "for (x, y), frecuencia in tabla_frecuencias.items():\n",
        "    frecuencias_relativas[(x, y)] = frecuencia / total_casos\n",
        "\n",
        "# Imprimir frecuencias relativas\n",
        "print(\"\\nFrecuencias relativas:\")\n",
        "for (x, y), frecuencia in frecuencias_relativas.items():\n",
        "    print(f\"{x}-{y}: {frecuencia:.3f}\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "C1KmvoD150Dh"
      },
      "id": "C1KmvoD150Dh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "\n",
        "## **Correlaci√≥n**\n",
        "\n",
        "---\n",
        "\n",
        "**Definici√≥n de Correlaci√≥n**\n",
        "\n",
        "- La correlaci√≥n es una medida estad√≠stica que describe la relaci√≥n entre dos variables (esto es, cambian conjuntamente a una tasa o velocidad constante).\n",
        "\n",
        "- Indica la fuerza y la direcci√≥n de la relaci√≥n entre dichas variables.\n",
        "\n",
        "- Se mide con base al **coeficiente de correlaci√≥n**.\n",
        "\n",
        "El m√°s utizado es el **Coeficiente de correlaci√≥n de Pearson** ($r$).\n",
        "\n",
        "\n",
        "**Coeficiente de Correlaci√≥n de Pearson ($r$)**\n",
        "\n",
        "- El coeficiente de correlaci√≥n de Pearson es una medida num√©rica de la relaci√≥n lineal entre dos variables num√©ricas.\n",
        "\n",
        "- $r$ var√≠a entre -1 y 1:\n",
        "\n",
        "  - **1**: Correlaci√≥n perfecta positiva.\n",
        "  - **-1**: Correlaci√≥n perfecta negativa.\n",
        "  - **0**: No hay correlaci√≥n lineal.\n",
        "\n",
        "El coeficiente de correlaci√≥n de Pearson ($r$) se calcula utilizando la siguiente f√≥rmula:\n",
        "\n",
        "$$r = \\frac{S_{XY}}{S_{X}S_{Y}},$$\n",
        "\n",
        "donde:\n",
        "\n",
        "* $S_{XY}=\\text{cov}(X, Y) = \\frac{1}{N} \\sum_{i=1}^{N} (X_i - \\bar{X})(Y_i - \\bar{Y}) \\text{ es la covarianza de  } X \\text{ e } Y$\n",
        "* $S_{X}$ es la desviaci√≥n t√≠pica de $X$\n",
        "* $S_{Y}$ es la desviaci√≥n t√≠pica de $Y$\n",
        "\n",
        "Dependiendo del valor de $r$ habitualmente obtendremos dos tipos de correlaciones:\n",
        "\n",
        "* Directa o positiva\n",
        "* Inversa o negativa\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0vVQqaaD3mfT"
      },
      "id": "0vVQqaaD3mfT"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Ejemplos de tipos de correlaci√≥n:\n",
        "\n",
        "# Generar datos para correlaci√≥n positiva\n",
        "np.random.seed(0)\n",
        "x_positivo = np.random.rand(100)\n",
        "y_positivo = 2 * x_positivo + np.random.rand(100)\n",
        "\n",
        "# Generar datos para correlaci√≥n negativa\n",
        "x_negativo = np.random.rand(100)\n",
        "y_negativo = -2 * x_negativo + np.random.rand(100)\n",
        "\n",
        "# Configurar la figura\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Gr√°fico de correlaci√≥n positiva\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(x_positivo, y_positivo, color='blue')\n",
        "plt.title('Correlaci√≥n Positiva')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "\n",
        "# Gr√°fico de correlaci√≥n negativa\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(x_negativo, y_negativo, color='red')\n",
        "plt.title('Correlaci√≥n Negativa')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "\n",
        "# Mostrar los gr√°ficos\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TIwGXYLVCLuP"
      },
      "id": "TIwGXYLVCLuP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Muestra diferentes valores de $r$:\n",
        "\n",
        "\n",
        "# Generar datos de x\n",
        "x = np.linspace(0, 10, 100)\n",
        "\n",
        "# Configurar la figura\n",
        "fig, axes = plt.subplots(4, 2, figsize=(12, 16))\n",
        "\n",
        "# Coeficientes de correlaci√≥n de Pearson\n",
        "coeficientes = [-1, -0.8, -0.2, 0, 0.2, 0.8, 1]\n",
        "\n",
        "# Iterar sobre los coeficientes y crear los subgr√°ficos\n",
        "for i, coeficiente in enumerate(coeficientes):\n",
        "    # Generar datos de y con el coeficiente de correlaci√≥n dado\n",
        "    y = coeficiente * x + np.random.normal(0, 1, size=len(x))\n",
        "\n",
        "    # Calcular el coeficiente de correlaci√≥n de Pearson\n",
        "    r, _ = pearsonr(x, y)\n",
        "\n",
        "    # Indices de la posici√≥n del subgr√°fico\n",
        "    row = i // 2\n",
        "    col = i % 2\n",
        "\n",
        "    # Dibujar el subgr√°fico\n",
        "    axes[row, col].scatter(x, y, color='blue')\n",
        "    axes[row, col].plot(x, coeficiente * x, color='red', linestyle='--', label=f'Recta: y = {coeficiente}x')\n",
        "    axes[row, col].set_title(f'Correlaci√≥n: {coeficiente}')\n",
        "    axes[row, col].tick_params(axis='both', which='both', bottom=False, top=False, left=False, right=False, labelbottom=False, labelleft=False)\n",
        "\n",
        "# Ajustar dise√±o y mostrar gr√°ficos\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "CBL5EnsIDYWd"
      },
      "id": "CBL5EnsIDYWd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "**Consideraciones**\n",
        "***\n",
        "\n",
        "* El coeficiente de correlaci√≥n no nos peremite establecer una asociaci√≥n causa-efecto.\n",
        "\n",
        "* El coeficiente de correlaci√≥n debe ser interpretado conjuntamente con el gr√°fico de dispersi√≥n.\n",
        "\n",
        "* Las variables deben estar asociadas seg√∫n el conocimiento establecido y con base a la l√≥gica y a la teor√≠a (sentido l√≥gico).\n",
        "\n",
        "\n",
        "***\n",
        "**Ejemplo**\n",
        "***\n",
        "\n",
        "Los manat√≠s son unos animales grandes y d√≥ciles que viven a lo largo de la costa de la Florida.\n",
        "Cada a√±o, lanchas motoras hieren o matan muchos manat√≠s.\n",
        "Se presenta una tabla con el n√∫mero de licencias para lanchas motoras (miles de licencias por a√±o) expedidas en la florida y el n√∫mero de manat√≠s muertos entre los a√±os 1977 y 1990.\n",
        "\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>A√±o</th>\n",
        "    <th>Licencias expedidas (1000)</th>\n",
        "    <th>Manat√≠s muertos</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1977</td>\n",
        "    <td>447</td>\n",
        "    <td>13</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1978</td>\n",
        "    <td>460</td>\n",
        "    <td>21</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1979</td>\n",
        "    <td>481</td>\n",
        "    <td>24</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1980</td>\n",
        "    <td>498</td>\n",
        "    <td>16</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1981</td>\n",
        "    <td>513</td>\n",
        "    <td>24</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1982</td>\n",
        "    <td>512</td>\n",
        "    <td>20</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1983</td>\n",
        "    <td>526</td>\n",
        "    <td>15</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1984</td>\n",
        "    <td>559</td>\n",
        "    <td>34</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1985</td>\n",
        "    <td>585</td>\n",
        "    <td>33</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1986</td>\n",
        "    <td>614</td>\n",
        "    <td>33</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1987</td>\n",
        "    <td>645</td>\n",
        "    <td>39</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1988</td>\n",
        "    <td>675</td>\n",
        "    <td>43</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1989</td>\n",
        "    <td>711</td>\n",
        "    <td>50</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1990</td>\n",
        "    <td>719</td>\n",
        "    <td>47</td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "\n",
        "¬øExiste relaci√≥n entre el n√∫mero de licencias y el de manat√≠s muertos?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MWTKVDuwCKWx"
      },
      "id": "MWTKVDuwCKWx"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Soluci√≥n del ejemplo:\n",
        "# Crear el DataFrame\n",
        "data = {\n",
        "    'A√±o': [1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990],\n",
        "    'Licencias expedidas (1000)': [447, 460, 481, 498, 513, 512, 526, 559, 585, 614, 645, 675, 711, 719],\n",
        "    'Manat√≠s muertos': [13, 21, 24, 16, 24, 20, 15, 34, 33, 33, 39, 43, 50, 47]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Mostrar el DataFrame\n",
        "display(df)\n",
        "\n",
        "licencias_expedidas = df['Licencias expedidas (1000)']\n",
        "manatis_muertos = df['Manat√≠s muertos']\n",
        "\n",
        "\n",
        "#Genera el gr√°fico:\n",
        "\n",
        "# Crear el gr√°fico de dispersi√≥n\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(licencias_expedidas, manatis_muertos, color='blue')\n",
        "\n",
        "# Etiquetas y t√≠tulo\n",
        "plt.xlabel('Licencias expedidas (1000)')\n",
        "plt.ylabel('Manat√≠s muertos')\n",
        "plt.title('Diagrama de dispersi√≥n: Licencias expedidas vs. Manat√≠s muertos')\n",
        "\n",
        "# Mostrar el gr√°fico\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Calcular la media de cada conjunto de datos\n",
        "mean_licencias = np.mean(licencias_expedidas)\n",
        "mean_manatis = np.mean(manatis_muertos)\n",
        "\n",
        "# Calcular la covarianza\n",
        "covariance = np.mean((licencias_expedidas - mean_licencias) * (manatis_muertos - mean_manatis))\n",
        "\n",
        "# Calcular las desviaciones est√°ndar\n",
        "std_licencias = np.std(licencias_expedidas)\n",
        "std_manatis = np.std(manatis_muertos)\n",
        "\n",
        "# Calcular el coeficiente de correlaci√≥n de Pearson\n",
        "pearson_corr = covariance / (std_licencias * std_manatis)\n",
        "\n",
        "print(\"Coeficiente de correlaci√≥n de Pearson:\", pearson_corr)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "AQYAcFiGUFyv"
      },
      "id": "AQYAcFiGUFyv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title C√°lculo de $r$ directamente:\n",
        "# Calcular el coeficiente de correlaci√≥n de Pearson\n",
        "pearson_corr, _ = pearsonr(licencias_expedidas, manatis_muertos)\n",
        "\n",
        "print(\"Coeficiente de correlaci√≥n de Pearson:\", pearson_corr)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "rgEhu9krVV8I"
      },
      "id": "rgEhu9krVV8I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regresi√≥n Lineal\n",
        "\n",
        "Sean las variables $X$ e $Y$ (de manera que $X_{i}$ e $Y_{i}$ son, respectivamente, los valores de $X$ e $Y$ en cada caso $i=1,...,N$), la **recta de regresi√≥n** $r_{Y|X}$ es la recta $Y^{*}=a+bX$ para la que es m√≠nimo\n",
        "\n",
        "$$\\sum_{i=1}^{N}e_{i}^{2}=\\sum_{i=1}^{N}(Y_{i}^{*}-Y_{i})^{2}=\\sum_{i=1}^{N}(a+bX_{i}-Y_{i})^{2}$$\n",
        "\n",
        "Esto se consigue tomando los valores\n",
        "\n",
        "$$b=\\frac{S_{X,Y}}{S_{X}^{2}},$$\n",
        "\n",
        "y $$a=\\bar{Y}-b\\bar{X},$$\n",
        "\n",
        "por lo tanto, $\\bar{Y}=a+b\\bar{X}$, esto es, la recta pasa por el punto $(\\bar{X},\\bar{Y}).$\n",
        "\n",
        "Se denota $r_{Y|X}\\equiv a+bX$. A $X$ se le denomina la **variable explicativa**, e $Y$ es la **variable explicada**.\n",
        "\n",
        "***\n",
        "**Ejemplo**\n",
        "***\n",
        "\n",
        "Continuando con el ejemplo de los manat√≠s, vamos a calcular la recta de regresi√≥n de la muerte de los manat√≠s en funci√≥n de las licencias expedidas.\n",
        "\n",
        "\n",
        "¬øCu√°ntos manat√≠s muertos habr√≠an en el a√±o 1991 si se expiden 748 licencias en ese a√±o?\n",
        "\n",
        "***\n",
        "\n"
      ],
      "metadata": {
        "id": "NerYu_-NvR5B"
      },
      "id": "NerYu_-NvR5B"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Paso a paso c√°lculo de la recta de regresi√≥n:\n",
        "# Paso 1: Calcular las medias\n",
        "mean_licencias = np.mean(licencias_expedidas)\n",
        "mean_manatis = np.mean(manatis_muertos)\n",
        "\n",
        "# Paso 2: Calcular la covarianza\n",
        "covariance = np.mean((np.array(licencias_expedidas) - mean_licencias) * (np.array(manatis_muertos) - mean_manatis))\n",
        "\n",
        "# Paso 3: Calcular la varianza de licencias_expedidas\n",
        "var_licencias = np.var(licencias_expedidas)\n",
        "\n",
        "# Paso 4: Calcular la pendiente (b)\n",
        "b = covariance / var_licencias\n",
        "\n",
        "# Paso 5: Calcular el t√©rmino independiente (a)\n",
        "a = mean_manatis - b * mean_licencias\n",
        "\n",
        "# Mostrar los resultados\n",
        "print(\"Pendiente (b):\", b)\n",
        "print(\"T√©rmino independiente (a):\", a)\n",
        "\n",
        "\n",
        "print(f\"La recta de regresi√≥n resultante es: N√∫mero de manat√≠s muertos = {round(b,2)} * el n√∫mero de licencias expedidas {round(a,2)} \")"
      ],
      "metadata": {
        "id": "jvfM_ptJYnDt"
      },
      "id": "jvfM_ptJYnDt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Pron√≥stico (1):\n",
        "\n",
        "n_exp_1991=748\n",
        "\n",
        "n_muertos= b*n_exp_1991+a\n",
        "\n",
        "print(f\"Si en el a√±o 1991 se expiden {n_exp_1991} licencias, se esperar√≠an {round(n_muertos)} manat√≠s muertos.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "t0r-d5bFbijh"
      },
      "id": "t0r-d5bFbijh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title C√°lculo directo con librer√≠a de Python:\n",
        "\n",
        "# Datos proporcionados\n",
        "licencias_expedidas = np.array([447, 460, 481, 498, 513, 512, 526, 559, 585, 614, 645, 675, 711, 719]).reshape(-1, 1)\n",
        "manatis_muertos = np.array([13, 21, 24, 16, 24, 20, 15, 34, 33, 33, 39, 43, 50, 47])\n",
        "\n",
        "# Crear el modelo de regresi√≥n lineal\n",
        "modelo_regresion = LinearRegression()\n",
        "\n",
        "# Ajustar el modelo a los datos\n",
        "modelo_regresion.fit(licencias_expedidas, manatis_muertos)\n",
        "\n",
        "# Obtener el coeficiente de la pendiente (a) y el t√©rmino independiente (b)\n",
        "a = modelo_regresion.coef_[0]\n",
        "b = modelo_regresion.intercept_\n",
        "\n",
        "# Imprimir la ecuaci√≥n de la recta de regresi√≥n\n",
        "print(f\"La recta de regresi√≥n resultante es: N√∫mero de manat√≠s muertos = {a:.2f} * el n√∫mero de licencias expedidas + {b:.2f}\")\n",
        "\n",
        "# Realizar el pron√≥stico para el n√∫mero de licencias 748\n",
        "numero_licencias = np.array([[748]])\n",
        "pronostico_manatis_muertos = modelo_regresion.predict(numero_licencias)\n",
        "print(f\"El pron√≥stico de manat√≠s muertos para {numero_licencias[0][0]} licencias expedidas es: {pronostico_manatis_muertos[0]:.2f}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "sbHaha5McIl2"
      },
      "id": "sbHaha5McIl2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "# Clasificaci√≥n. Clasificador bayesiano ingenuo\n",
        "***\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NM7kptGhY0et"
      },
      "id": "NM7kptGhY0et"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clasificaci√≥n: aplicaciones\n",
        "\n",
        "A grandes rasgos, la tarea de la clasificaci√≥n se basa en asignar una categor√≠a a una entrada.\n",
        "\n",
        "Ejemplos de tareas de clasificaci√≥n en el contexto del PLN:\n",
        "\n",
        "1. Reconocimiento de letras o palabras.\n",
        "2. Clasificaci√≥n autom√°tica de textos.\n",
        "3. An√°lisis de sentimiento.\n",
        "4. Categorizaci√≥n de emails, en concreto, detecci√≥n de spam.\n",
        "5. Identificaci√≥n de idioma.\n",
        "6. Atribuci√≥n de autor√≠a\n",
        "7. Asignaci√≥n de categor√≠a bibliogr√°fica o tem√°tica a un texto.\n",
        "8. El etiquetado gramatical de palabras clasifica las palabras que conforman una frase.\n",
        "\n",
        "Algunas tareas de clasificaci√≥n, m√°s all√° del PLN: reconocimiento de cara, de voz o de un objeto en imagen, asignaci√≥n de grados a tareas‚Ä¶\n",
        "\n",
        "***\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t1dLMPJwZwP9"
      },
      "id": "t1dLMPJwZwP9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Clasificaci√≥n**\n",
        "\n",
        "La **tarea de la clasificaci√≥n** consiste en tomar una observaci√≥n, extraer de ella una serie de caracter√≠sticas √∫tiles y asignarle una categor√≠a de un conjunto de clases discretas.\n",
        "\n",
        "**Las clases representan** grupos exhaustivos y mutuamente excluyentes, es decir, cada uno de los individuos pertenece a uno de los grupos y s√≥lo a uno.\n",
        "\n",
        "Para esta **asignaci√≥n** se pueden usar reglas prestablecidas, pero lo apropiado es usar el aprendizaje autom√°tico supervisado. Esto es, entrenar un modelo de clasificaci√≥n con un conjunto de observaciones ya etiquetados de manera correcta.\n",
        "\n",
        "Los **modelos de clasificaci√≥n** proporcionan las probabilidades (o verosimilitudes) de que la observaci√≥n pertenezca a cada una de las categor√≠as. Lo habitual es asignar la categor√≠a que resulte m√°s probable (o veros√≠mil).\n",
        "\n",
        "Es importante distinguir entre las **tareas de clasificaci√≥n** (supervisada) y las de **clasificaci√≥n autom√°tica** (tambi√©n denominado an√°lisis cl√∫ster o de conglomerados).\n",
        "\n",
        "Las **metodolog√≠as de clasificaci√≥n autom√°tica** son t√©cnicas de aprendizaje no supervisado que agrupan los casos de un conjunto de datos seg√∫n sus caracter√≠sticas (se agrupa sin atender a un etiquetado ya definido).\n",
        "\n",
        "***\n",
        "\n"
      ],
      "metadata": {
        "id": "W_d5IN7FoL7w"
      },
      "id": "W_d5IN7FoL7w"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Clasificador bayesiano ingenuo**\n",
        "\n",
        "El **clasificador bayesiano ingenuo (CBI)** es un m√©todo de clasificaci√≥n que asigna a una entrada la categor√≠a a la que le corresponde una mayor probabilidad a posteriori.\n",
        "\n",
        "Dado un texto $t$ con caracter√≠sticas $x_{1},x_{2},...,x_{n}$, la probabilidad que sea de la categor√≠a $c$ la podemos calcular a trav√©s del **Teorema de Bayes**:\n",
        "\n",
        "$$\\underbrace{P(c|t)}_{\\substack{\\text{Probabilidad}\\\\ \\text{a posteriori}}}=\\frac{P(t|c)P(c)}{P(t)}=\\frac{P(x_{1},...,x_{n}|c)P(c)}{P(x_{1},...,x_{n})}\\propto \\underbrace{P(x_{1},...,x_{n}|c)}_{\\text{verosimilitud}}.\\underbrace{P(c)}_{\\substack{\\text{Probabilidad}\\\\ \\text{a priori}}}$$\n",
        "\n",
        "\n",
        "Se omite el denominador porque no depende de la clase $c$, por lo que no alterar√° los √≥rdenes de los resultados para cada clase.\n",
        "De esta manera, no es necesario calcular $P(x_{1},...,x_{n})$.\n",
        "\n",
        "La ***ingenuidad*** del modelo se debe a que asume la independencia entre las caracter√≠sticas tenidas en cuenta.\n",
        "\n",
        "El s√≠mbolo delta ($\\Delta$) encima de la segunda igualdad en la siguiente formulaci√≥n indica que la igualdad se cumple bajo una hip√≥tesis.\n",
        "\n",
        "$$P(c|t)=P(x_{1},...,x_{n}|c).P(c)\\stackrel{\\Delta}{=}P(x_{1}|c)...P(x_{n}|c)P(c) =P(c)\\prod_{i=1}^{n}P(x_{i}|c).$$\n",
        "\n",
        "***\n"
      ],
      "metadata": {
        "id": "RK9n2oguouDB"
      },
      "id": "RK9n2oguouDB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CBI multinomial: procedimiento**\n",
        "\n",
        "Partimos de un **conjunto de entrenamiento** con $N$ entradas conformadas por un texto y una etiqueta $c \\in C$ ($C$ es el conjunto de clases discretas).\n",
        "\n",
        "Disponemos de $N_{c}$ entradas para cada clase $c$. Para cada $c \\in C$ construimos su bolsa de palabras, esto es, la tabla de frecuencias de las palabras que constituyen los textos del subconjunto de entrenamiento tales que su etiqueta es $c$.\n",
        "\n",
        "Dada un texto no etiquetado, el **CBI** le asigna la clase $\\hat{c}\\in C$ con mayor probabilidad a *posteriori*, esto es:\n",
        "\n",
        "\n",
        "$$\\hat{c}=\\substack{\\text{argmax}P(c|t)\\\\ c \\in C}\\stackrel{\\Delta}{=}\\substack{\\text{argmax}\\\\ c \\in C}P(c)\\prod_{i=1}^{n}P(x_{i}|c).$$\n",
        "\n",
        "\n",
        "$P(c)$ es la probabilidad a priori de la clase $c$, y $P(x_{i}|c)$ es la probabilidad de la caracter√≠stica $i$ condicionada a ser de la clase $c$.\n",
        "\n",
        "***\n",
        "\n",
        "Las **probabilidades a priori** de cada clase se pueden estimar emp√≠ricamente considerando sus frecuencias relativas en el conjunto de entrenamiento (siempre que sea representativo de la poblaci√≥n).\n",
        "\n",
        "Tambi√©n pueden estimarse seg√∫n estudios previos u opiniones de expertos.\n",
        "\n",
        "Nos limitamos al denominado **CBI multinomial**.\n",
        "\n",
        "Este modelo se basa en las bolsas de palabras que son tablas de frecuencias de las palabras que aparecen en un conjunto de textos.\n",
        "\n",
        "Esto implica que el modelo toma como caracter√≠sticas de los textos las frecuencias de las palabras y no atiende a sus posiciones\n",
        "\n",
        "La probabilidad de una palabra $w$ condicionada a provenir de un texto de la categor√≠a $c$ se estima de manera emp√≠rica:\n",
        "\n",
        "$$P(w|c)=\\frac{n(w,c)}{n(c)};$$\n",
        "\n",
        "donde $n(c)$ es el n√∫mero total de palabras usadas en la bolsa (de la categor√≠a) $c$ y $n(w,c)$ es el n√∫mero de veces que aparece la palabra $w$ en la bolsa $c$.\n",
        "\n",
        "Si una palabra no aparece en todo el corpus de entrenamiento, entonces se omite su c√°lculo de probabilidad condicionada.\n",
        "\n",
        "***\n",
        "\n",
        "Una palabra del corpus que no est√© en la bolsa $c$ tendr√° probabilidad condicionada a $c$ nula, ya que $n(w,c))=0$, lo cual anular√≠a toda la probabilidad a posteriori de la clase $c$.\n",
        "Para evitarlo, se aplica el denominado **suavizado de Laplace** que modifica el c√°lculo de las probabilidades condicionadas de manera que se asigna una probabilidad (peque√±a) a cualquier palabra del corpus no registrada en la bolsa $c$ empleando la siguiente formulaci√≥n:\n",
        "\n",
        "$$P(w|c)=\\frac{n(w,c)+1}{n(c)+|V|}.$$\n",
        "\n",
        "Se modifica el numerador para que las palabras del corpus que no forman parte de la bolsa $c$ tengan una peque√±a probabilidad; y el denominador para que, en rigor, defina una funci√≥n de probabilidad (reporta valores entre 0 y 1, y la suma de todos los sucesos elementales es 1).\n",
        "\n",
        "$V$ representa el vocabulario del corpus (las distintas palabras empleadas) y $|V|$ su cardinalidad (el n√∫mero de palabras de $V$).\n",
        "\n",
        "***\n",
        "\n"
      ],
      "metadata": {
        "id": "inNQgHJ0pY7E"
      },
      "id": "inNQgHJ0pY7E"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "**Ejemplo ilustrativo**:\n",
        "***\n",
        "\n",
        "Entrenamos y probamos el CBI multinomial con suavizado de Laplace en un an√°lisis de sentimiento de dimensi√≥n muy reducida para poder ilustrar los pasos de esta metodolog√≠a. El ejemplo est√° tomado de la secci√≥n 4.3 de Jurafsky & Martin (2023). Los textos que vamos a analizar son simplificaciones de revisiones reales de pel√≠culas.\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>Dataset</th>\n",
        "    <th>Category</th>\n",
        "    <th>Document</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td rowspan=\"5\" style=\"text-align:center\">Training</td>\n",
        "    <td>-</td>\n",
        "    <td>just plain boring</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>-</td>\n",
        "    <td>entirely predictable and lacks energy</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>-</td>\n",
        "    <td>no surprises and very few laughs</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>+</td>\n",
        "    <td>very powerful</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>+</td>\n",
        "    <td>the most fun film of the summer</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Test</td>\n",
        "    <td>?</td>\n",
        "    <td>predictable with no fun</td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "\n",
        "\n",
        "Tenemos un total de $N=5$ textos de entrenamiento, $N_{-}=3$ con valoraci√≥n negativa y $N_{+}=2$ con valoraci√≥n positiva.\n",
        "\n",
        "Podemos utilizar esta **distribuci√≥n emp√≠rica** para considerar las probabilidades a priori para las dos categor√≠as:\n",
        "\n",
        "$P(-) = \\frac{N_{-}}{N}=\\frac{3}{5}=0.6 \\hspace{1.5cm} P(+)=\\frac{N_{+}}{N}=\\frac{2}{5}=0.4$\n",
        "\n",
        "***\n",
        "\n"
      ],
      "metadata": {
        "id": "E4bnvzV5qKSk"
      },
      "id": "E4bnvzV5qKSk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bolsa de palabras** del conjunto de entrenamiento en formato de tabla de frecuencias. Las palabras est√°n ordenadas seg√∫n sus frecuencias de manera decreciente y los empates se resuelven por alfabetizaci√≥n.\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>#</th>\n",
        "    <th>(Word, w)</th>\n",
        "    <th>(Frequency, n(w))</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>2</td>\n",
        "    <td>the</td>\n",
        "    <td>2</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>3</td>\n",
        "    <td>very</td>\n",
        "    <td>2</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>4</td>\n",
        "    <td>boring</td>\n",
        "    <td>1</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>5</td>\n",
        "    <td>energy</td>\n",
        "    <td>1</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>6</td>\n",
        "    <td>entirely</td>\n",
        "    <td>1</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>7</td>\n",
        "    <td>few</td>\n",
        "    <td>1</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>8</td>\n",
        "    <td>film</td>\n",
        "    <td>1</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>9</td>\n",
        "    <td>fun</td>\n",
        "    <td>1</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>10</td>\n",
        "    <td>just</td>\n",
        "    <td>1</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>11</td>\n",
        "    <td>lacks</td>\n",
        "    <td>1</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>12</td>\n",
        "    <td>laughs</td>\n",
        "    <td>1</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>13</td>\n",
        "    <td>most</td>\n",
        "    <td>1</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>14</td>\n",
        "    <td>no</td>\n",
        "    <td>1</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>15</td>\n",
        "    <td>of</td>\n",
        "    <td>1</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>16</td>\n",
        "    <td>plain</td>\n",
        "    <td>1</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>17</td>\n",
        "    <td>powerful</td>\n",
        "    <td>1</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>18</td>\n",
        "    <td>predictable</td>\n",
        "    <td>1</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>19</td>\n",
        "    <td>summer</td>\n",
        "    <td>1</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>20</td>\n",
        "    <td>surprises</td>\n",
        "    <td>1</td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "Sumando las frecuencias observamos que en el entrenamiento tiene un total de $23$ palabras, con un vocabulario $V$ de $20$ palabras distintas. Por lo tanto, $|V|=20$.\n",
        "\n",
        "***\n",
        "\n"
      ],
      "metadata": {
        "id": "b3pFgPrbqltD"
      },
      "id": "b3pFgPrbqltD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bolsa de palabras del conjunto de entrenamiento** con clase <<->>.\n",
        "\n",
        "La tabla incluye el c√°lculo de verosimilitudes de cada una de las palabras para esta clase (hay dos valores distintos porque tan solo hay dos valores de frecuencia).\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>#</th>\n",
        "    <th>(Word, w)</th>\n",
        "    <th>(Frequency, n(w,-))</th>\n",
        "    <th>(Probability, P(w|-))</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1</td>\n",
        "    <td>and</td>\n",
        "    <td>2</td>\n",
        "    <td>0.0882</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>2</td>\n",
        "    <td>boring</td>\n",
        "    <td>1</td>\n",
        "    <td>0.0588</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>3</td>\n",
        "    <td>energy</td>\n",
        "    <td>1</td>\n",
        "    <td>0.0588</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>4</td>\n",
        "    <td>entirely</td>\n",
        "    <td>1</td>\n",
        "    <td>0.0588</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>5</td>\n",
        "    <td>few</td>\n",
        "    <td>1</td>\n",
        "    <td>0.0588</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>6</td>\n",
        "    <td>just</td>\n",
        "    <td>1</td>\n",
        "    <td>0.0588</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>7</td>\n",
        "    <td>lacks</td>\n",
        "    <td>1</td>\n",
        "    <td>0.0588</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>8</td>\n",
        "    <td>laughs</td>\n",
        "    <td>1</td>\n",
        "    <td>0.0588</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>9</td>\n",
        "    <td>no</td>\n",
        "    <td>1</td>\n",
        "    <td>0.0588</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>10</td>\n",
        "    <td>plain</td>\n",
        "    <td>1</td>\n",
        "    <td>0.0588</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>11</td>\n",
        "    <td>predictable</td>\n",
        "    <td>1</td>\n",
        "    <td>0.0588</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>12</td>\n",
        "    <td>surprises</td>\n",
        "    <td>1</td>\n",
        "    <td>0.0588</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>13</td>\n",
        "    <td>very</td>\n",
        "    <td>1</td>\n",
        "    <td>0.0588</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>TOTAL:</td>\n",
        "    <td></td>\n",
        "    <td>14</td>\n",
        "    <td></td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "\n",
        "Utiliza 14 palabras con un vocabulario de 13 palabras distintas.\n",
        "\n",
        "Por lo tanto, $n(-)=14$.\n",
        "\n",
        "Las probabilidades dependen de las frecuencias y a trav√©s del **suavizado de Laplace**, se tiene:\n",
        "\n",
        "$$P(w|-)=\\frac{n(w,-)+1}{n(-)+|V|}=\\frac{n(w,-)+1}{14+20}=\\frac{n(w,-)+1}{34}$$\n",
        "\n",
        "Por lo tanto, las respectivas probabilidades seg√∫n las frecuencias sean 2, 1 y 0 son:\n",
        "\n",
        "\n",
        "$$\n",
        "\\frac{{(2+1)}}{{34}} = \\frac{3}{{34}} \\approx 0.0588,\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{{(1+1)}}{{34}} = \\frac{2}{{34}} \\approx 0.0690,\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{{(0+1)}}{{34}} = \\frac{1}{{34}} \\approx 0.0294.\n",
        "$$\n",
        "\n",
        "***\n",
        "\n"
      ],
      "metadata": {
        "id": "52D8136sqyJL"
      },
      "id": "52D8136sqyJL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bolsa de palabras del conjunto de entrenamiento** con clase $+$.\n",
        "\n",
        "La tabla incluye el c√°lculo de verosimilitudes de cada una de las palabras para esta clase (hay dos valores distintos porque tan s√≥lo hay dos valores de frecuencia).\n",
        "\n",
        "Utiliza 9 palabras con vocabulario de 8 palabras distintas.\n",
        "\n",
        "Por lo tanto, $c(+)=9$.\n",
        "\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>#</th>\n",
        "    <th>(Word, w)</th>\n",
        "    <th>(Frequency, n(w,+))</th>\n",
        "    <th>(Probability, P(w|+))</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1</td>\n",
        "    <td>the</td>\n",
        "    <td>2</td>\n",
        "    <td>0.1034</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>2</td>\n",
        "    <td>film</td>\n",
        "    <td>1</td>\n",
        "    <td>0.0690</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>3</td>\n",
        "    <td>fun</td>\n",
        "    <td>1</td>\n",
        "    <td>0.0690</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>4</td>\n",
        "    <td>most</td>\n",
        "    <td>1</td>\n",
        "    <td>0.0690</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>5</td>\n",
        "    <td>of</td>\n",
        "    <td>1</td>\n",
        "    <td>0.0690</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>6</td>\n",
        "    <td>powerful</td>\n",
        "    <td>1</td>\n",
        "    <td>0.0690</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>7</td>\n",
        "    <td>summer</td>\n",
        "    <td>1</td>\n",
        "    <td>0.0690</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>8</td>\n",
        "    <td>very</td>\n",
        "    <td>1</td>\n",
        "    <td>0.0690</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>TOTAL:</td>\n",
        "    <td></td>\n",
        "    <td>9</td>\n",
        "    <td></td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "Las probabilidades se definen de manera an√°loga a la clase (-):\n",
        "\n",
        "\n",
        "$$P(w|+)=\\frac{n(w,+)+1}{n(+)+|V|}=\\frac{n(w,+)+1}{9+20}=\\frac{n(w,+)+1}{29}.$$\n",
        "\n",
        "Por lo tanto, las respectivas probabilidades seg√∫n las frecuencias sean 2, 1 y 0 son:\n",
        "\n",
        "\n",
        "\n",
        "$$\n",
        "\\frac{{(2+1)}}{{29}} = \\frac{3}{{29}} \\approx 0.1034,\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{{(1+1)}}{{29}} = \\frac{2}{{29}} \\approx 0.0690,\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{{(0+1)}}{{29}} = \\frac{1}{{29}} \\approx 0.0345.\n",
        "$$\n",
        "\n",
        "***"
      ],
      "metadata": {
        "id": "sb_Z-17Bq9LF"
      },
      "id": "sb_Z-17Bq9LF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para cada clase, calculamos sus probabilidades a posteriori, proporcionales a $$\\frac{1}{P(\\text{\"predictable } \\underbrace{with}_{\\substack{\\text{No est√° en}}\\\\{\\text{  el corpus}}} \\text{ no fun\"})},$$\n",
        "\n",
        "\n",
        "las comparamos y estimamos que el texto proviene de la clase con mayor probabilidad.\n",
        "\n",
        "Esto es:\n",
        "\n",
        "$$\n",
        "P(-|\\text{\"predictable } \\underbrace{with}_{\\substack{\\text{No est√° en}}\\\\{\\text{  el corpus}}} \\text{ no fun})\") \\propto P(\"predictable\"|-)P(\"no\"|-)P(\"fun\"|-)P(-) = \\frac{2}{34} .\\frac{2}{34} . \\frac{1}{34} . \\frac{3}{5} \\approx 6 . 10^{-5}\n",
        "$$\n",
        "\n",
        "$$\n",
        "P(+|\\text{\"predictable } \\underbrace{with}_{\\substack{\\text{No est√° en}}\\\\{\\text{  el corpus}}} \\text{ no fun})\") \\propto P(\"predictable\"|+)P(\"no\"|+)P(\"fun\"|+)P(+) = \\frac{1}{29} . \\frac{1}{29} . \\frac{2}{29} . \\frac{2}{5} \\approx 3 . 10^{-5}\n",
        "$$\n",
        "\n",
        "Tenemos que\n",
        "\n",
        "$$P(-|\\text{\"predictable } \\underbrace{with}_{\\substack{\\text{No est√° en}}\\\\{\\text{  el corpus}}} \\text{ no fun})\") > P(+|\\text{\"predictable } \\underbrace{with}_{\\substack{\\text{No est√° en}}\\\\{\\text{  el corpus}}} \\text{ no fun})\"),$$\n",
        "\n",
        "por lo que consideraremos que el texto proviene de una rese√±a con sentimiento negativo.\n",
        "\n",
        "Como ***with*** no est√° en el corpus, se ignora en el c√°lculo de las probabilidades.\n",
        "\n",
        "\n",
        "***\n"
      ],
      "metadata": {
        "id": "rg9ldmiXBQFZ"
      },
      "id": "rg9ldmiXBQFZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Ejemplo desarrollado en Python.\n",
        "\n",
        "# Definir los datos de entrenamiento y prueba\n",
        "category = [\"-\", \"-\", \"-\", \"+\", \"+\"]\n",
        "document = [\"just plain boring\",\n",
        "            \"entirely predictable and lacks energy\",\n",
        "            \"no surprises and very few laughs\",\n",
        "            \"very powerful\",\n",
        "            \"the most fun film of the summer\"]\n",
        "training = pd.DataFrame({\"category\": category, \"document\": document})\n",
        "\n",
        "print(f\"\\n------------------Entrenamiento------------------------------\\n\")\n",
        "display(training)\n",
        "print(f\"\\n------------------Prueba------------------------------\\n\")\n",
        "test = \"predictable with no fun\"\n",
        "display(test)\n",
        "\n",
        "\n",
        "def get_bow(documents, t=0): #t=0, bolsa inicial de palabras\n",
        "    words = ' '.join(documents).split()\n",
        "    word_counts = Counter(words)\n",
        "    bow = pd.DataFrame(list(word_counts.items()), columns=['Word', 'Freq'])\n",
        "    bow = bow.sort_values(by='Freq', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    if not t==0:\n",
        "        # Calcular la probabilidad a posteriori\n",
        "        sum_freq = sum(bow['Freq'])\n",
        "        bow['posterior_prob'] = (bow['Freq'] + 1) / (sum_freq + 20)  # Laplace smoothing with k=1\n",
        "        print(f\"Total palabras: {sum_freq}\")\n",
        "    return bow\n",
        "\n",
        "\n",
        "print(f\"\\n------------------Bolsa de palabras------------------------------\\n\")\n",
        "# Obtener la bolsa de palabras para todos los documentos\n",
        "bow = get_bow(training['document'])\n",
        "display(bow)\n",
        "\n",
        "\n",
        "print(f\"\\n------------------Bolsa de palabras negativas------------------------------\\n\")\n",
        "# Obtener la bolsa de palabras para las categor√≠as negativas\n",
        "bow_neg = get_bow(training[training['category'] == '-']['document'],1)\n",
        "display(bow_neg)\n",
        "\n",
        "# Obtener la bolsa de palabras para las categor√≠as positivas\n",
        "print(f\"\\n------------------Bolsa de palabras positivas------------------------------\\n\")\n",
        "bow_pos = get_bow(training[training['category'] == '+']['document'],1)\n",
        "display(bow_pos)\n",
        "\n",
        "# Calcular par√°metros para el clasificador Naive Bayes\n",
        "n_V = len(bow)\n",
        "n_neg = sum(bow_neg['Freq'])\n",
        "n_pos = sum(bow_pos['Freq'])\n",
        "\n",
        "bow_neg['likel'] = (bow_neg['Freq'] + 1) / (n_neg + n_V)\n",
        "bow_pos['likel'] = (bow_pos['Freq'] + 1) / (n_pos + n_V)\n",
        "\n",
        "prior_neg = np.mean(training['category'] == '-')\n",
        "\n",
        "print(f\"\\n------------------Probabilidad a priori negativa------------------------------\\n\")\n",
        "display(prior_neg)\n",
        "\n",
        "prior_pos = np.mean(training['category'] == '+')\n",
        "\n",
        "print(f\"\\n------------------Probabilidad a priori positiva------------------------------\\n\")\n",
        "display(prior_pos)\n",
        "\n",
        "\n",
        "# Tokenizar las palabras del texto de prueba\n",
        "test_words = test.split()\n",
        "loglik_neg = 0\n",
        "loglik_pos = 0\n",
        "\n",
        "# Calcular la probabilidad posterior para la categor√≠a negativa\n",
        "for w in test_words:\n",
        "    if w in bow['Word'].values:\n",
        "        if w in bow_neg['Word'].values:\n",
        "            loglik_neg += math.log(bow_neg.loc[bow_neg['Word'] == w, 'likel'].values[0])\n",
        "        else:\n",
        "            loglik_neg -= math.log(n_neg + n_V)\n",
        "        if w in bow_pos['Word'].values:\n",
        "            loglik_pos += math.log(bow_pos.loc[bow_pos['Word'] == w, 'likel'].values[0])\n",
        "        else:\n",
        "            loglik_pos -= math.log(n_pos + n_V)\n",
        "\n",
        "post_neg = math.log(prior_neg) + loglik_neg\n",
        "post_pos = math.log(prior_pos) + loglik_pos\n",
        "\n",
        "print(\"\\n--------------------------------------------------------------------\\n\")\n",
        "\n",
        "print(\"Probabilidad a posteriori de clase negativa:\", math.exp(post_neg))\n",
        "print(\"Probabilidad a posteriori de clase positiva:\", math.exp(post_pos))\n",
        "\n",
        "if math.exp(post_pos)>math.exp(post_neg):\n",
        "    print(f\"\\nC√≥mo la probabilidad a posteriori de la clase positiva, {math.exp(post_pos)} es mayor que la de la clase negativa, {math.exp(post_neg)}, \\nse estima que el texto proviene de una la clase positiva\")\n",
        "else:\n",
        "    print(f\"\\nC√≥mo la probabilidad a posteriori de la clase negativa, {math.exp(post_neg)} es mayor que la de la clase positiva, {math.exp(post_pos)}, \\nse estima que el texto proviene de una la clase negativa\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "W9Ptsb3kyXNd"
      },
      "id": "W9Ptsb3kyXNd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Alternativas de implementaci√≥n del CBI**\n",
        "\n",
        "**CBI multinomial binario**:\n",
        "\n",
        "En cada texto se anota la incidencia de una palabra (1 si est√°, y 0 si no), en vez del recuento de esa palabra. En las bolsas de palabras se anota el n√∫mero de veces que una palabra incide en los distintos textos analizados. As√≠, en nuestro ejemplo la palabra ‚Äúthe‚Äù que solo aparece en el texto \"*the most fun film of the summer*\" de la clase $+$ tendr√° una √∫nica incidencia en la bolsa de palabras de esta clase.\n",
        "\n",
        "***\n",
        "**Ejercicio**:\n",
        "\n",
        "***\n",
        "Comprobar en el ejemplo que hemos hecho que con esta variante la probabilidad a posteriori de la clase $-$ se mantiene igual y que la de la clase $+$ resulta, aproximadamente, 0.00004.\n",
        "\n",
        "\n",
        "Usar una **adici√≥n de valor $\\alpha$** para hacer el suavizado, esto es, calculamos la probabilidad de la palabra $w$ condicionada a la clase $c$ con la siguiente formulaci√≥n:\n",
        "\n",
        "\n",
        "$$P(w|c) = \\frac{n(w,c) + \\alpha}{n(c) + |V|\\alpha} $$\n",
        "\n",
        "Si $\\alpha=1$ tenemos el **suavizado de Laplace**, el que hemos usado.\n",
        "***\n"
      ],
      "metadata": {
        "id": "7pwXykvAKcL_"
      },
      "id": "7pwXykvAKcL_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Evaluaci√≥n de un modelo de clasificaci√≥n (I)**\n",
        "\n",
        "**Matriz de confusi√≥n.**\n",
        "\n",
        "Al probar un modelo de clasificaci√≥n sobre un conjunto test, anotamos en cada celda de la estructura de filas y columnas el n√∫mero de casos que pertenecen a la correspondiente clase indicada en la columna y el sistema ha evaluado en la correspondiente clase indicada en la fila.\n",
        "\n",
        "\n",
        "Veamos el esquema de una matriz de confusi√≥n para una clasificaci√≥n de dos niveles, por ejemplo: positivo o negativo\n",
        "\n",
        "\n",
        "\n",
        "<center>\n",
        " <img src=\"https://github.com/ednavivianasegura/Curso_PLN/blob/main/matriz_confusion.png?raw=1\" alt=\"confusion\" width=\"70%\" height=\"70%\">  \n",
        "</center>\n",
        "\n",
        "\n",
        "***\n",
        "\n",
        "Tengamos en cuenta que:\n",
        "\n",
        "* #p.v. es el n√∫mero de positivos verdaderos, esto es, el n√∫mero de instancias que el modelo ha etiquetado como positivas siendo realmente positivas (esto es, el modelo acierta poniendo etiqueta positiva);\n",
        "\n",
        "* #p.f. es el n√∫mero de positivos falsos, esto es, el n√∫mero de instancias que el modelo ha etiquetado como positivas siendo realmente negativas (esto es, el modelo falla poniendo etiqueta positiva);\n",
        "\n",
        "\n",
        "* #n.f. es el n√∫mero de negativos falsos, esto es, el n√∫mero de instancias que el modelo ha etiquetado como negativas siendo realmente positivas (esto es, el n√∫mero de veces que el modelo falla poniendo etiqueta negativa);\n",
        "\n",
        "* #n.v. es el n√∫mero de negativos verdaderos, esto es, el n√∫mero de instancias que el modelo ha etiquetado como negativas siendo realmente negativas (esto es, el n√∫mero de veces que el modelo acierta poniendo etiqueta negativa).\n",
        "\n",
        "\n",
        "Utilizando estos par√°metros hemos visto tres medidas del nivel de acierto de un modelo de clasificaci√≥n.\n",
        "\n",
        "La m√°s evidente es accuracy, que implica una tasa de acierto global.\n",
        "\n",
        "Mientras tanto, precision es la tasa de acierto dentro de las instancias que el modelo ha clasificado como positivas, y recall es la tasa de acierto dentro de las instancias que tienen etiqueta ‚Äúreal‚Äù o ‚Äúhumana‚Äù positiva.\n",
        "\n",
        "***\n",
        "\n",
        "El accuracy se puede considerar una tasa de acierto fiable si el n√∫mero de instancias de cada clase est√° equilibrado. Pero podr√≠a no darse esa circunstancia.\n",
        "\n",
        "**Por ejemplo**, podr√≠amos estar estudiando la incidencia de una patolog√≠a muy poco com√∫n que solo aparezca una de cada 10000 veces.\n",
        "Un modelo que se limite a clasificar todos los casos como negativos aplicado testado sobre una gran cantidad de individuos ser√≠a  muy disfuncional en la detecci√≥n de la patolog√≠a, pero contar√≠a con un accuracy muy competente ya que se esperar√≠a que fuera de alrededor del 99.99%.\n",
        "Sin embargo, como no contar√≠a con ning√∫n positivo verdadero, sus resultados de precision y recall ser√°n de 0, representando mejor la inutilidad de ese modelo.\n",
        "\n",
        "* Analicemos este hecho:\n",
        "Si nuestro modelo solo etiqueta los casos como negativos, en la matriz de confusi√≥n encontraremos instancias negativas verdaderas y negativas falsas (sin embargo, #p.v=#p.f.=0).\n",
        "Para una muestra representativa de la poblaci√≥n se espera tener, por cada 10000 casos, 1 individuo que padece la patolog√≠a y 9999 individuos que no. As√≠ podr√≠amos considerar que #n.f.=1 y #n.v.=9999. De esta forma, se tiene\n",
        "\n",
        "$$\n",
        "\\text{accuracy} = \\frac{\\#p.v. + \\#n.v.}{\\#p.v. + \\#p.f. + \\#n.f. + \\#n.v.} = \\frac{0 + 9999}{0 + 9999 + 0 + 1} = \\frac{9999}{10000} = 0.9999 \\rightarrow 99.99\\%\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{precision} = \\frac{\\#p.v.}{\\#p.v. + \\#p.f.} = \\frac{0}{0 + 9999} = \\frac{0}{9999} = 0 \\rightarrow 0\\%\n",
        "$$\n",
        "$$\n",
        "\\text{recall} = \\frac{\\#p.v.}{\\#p.v. + \\#n.f.} = \\frac{0}{0 + 1} = \\frac{0}{1} = 0 \\rightarrow 0\\%\n",
        "$$\n",
        "\n",
        "***\n",
        "\n",
        "El $F_1$-score es una medida para evaluar el desempe√±o de un modelo de clasificaci√≥n que combina los resultados del precision y recall:\n",
        "$$ F_1 = \\frac{2 \\times \\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}}. $$\n",
        "\n",
        "El $F_1$-score es el caso particular para $\\beta=1$ del $F_{\\beta}$-score:\n",
        "$$ F_{\\beta} = \\frac{(\\beta^2 + 1) \\times \\text{precision} \\times \\text{recall}}{\\beta^2 \\times \\text{precision} + \\text{recall}}. $$\n",
        "El par√°metro $\\beta$ pondera la importancia entre precision y recall:\n",
        "\n",
        "* si $\\beta < 1$, precision tendr√° m√°s peso en el score;\n",
        "* si $\\beta>1$, recall tendr√° m√°s peso;\n",
        "* si $\\beta=1$, ambas tendr√°n el mismo peso.\n",
        "\n",
        "***\n",
        "Ilustraci√≥n de la evaluaci√≥n de un modelo de clasificaci√≥n de una variable con m√°s de dos niveles: **macroavering** y **microavering** (secci√≥n 4.7.1 de Jurafsky & Martin, 2024)\n",
        "\n",
        "\n",
        "<center>\n",
        " <img src=\"https://github.com/ednavivianasegura/Curso_PLN/blob/main/matriz-confusion.PNG?raw=1\" alt=\"matriz-confusion\" width=\"60%\" height=\"60%\">  \n",
        "</center>\n",
        "\n",
        "<center>\n",
        " <img src=\"https://github.com/ednavivianasegura/Curso_PLN/blob/main/averings.PNG?raw=1\" alt=\"averings\" width=\"75%\" height=\"75%\">  \n",
        "</center>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NTuRPwFaL4OQ"
      },
      "id": "NTuRPwFaL4OQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "# **MODELOS DE LENGUAJE**\n",
        "***\n",
        "\n",
        "**Modelos de lenguaje ‚Äì Introducci√≥n**\n",
        "\n",
        "Considerar veros√≠mil, o no, la palabra que sigue en una frase incompleta parece una tarea sencilla en algunas situaciones.\n",
        "\n",
        "**Por ejemplo**, en *‚Äúllevar√© el coche al taller a que le hagan una ‚Ä¶‚Äù* estaremos de acuerdo en considerar *‚Äúrevisi√≥n‚Äù* o *‚Äúreparaci√≥n‚Äù* como candidatas muy probables a ser la siguiente palabra, mientras que *‚Äúhielo‚Äù* o *‚Äúpaella‚Äù* no lo parecen.\n",
        "\n",
        "\n",
        "La formalizaci√≥n de esta intuici√≥n determina los modelos de lenguaje.\n",
        "\n",
        "En general, son **modelos** que asignan *probabilidades* a cada posible palabra, o secuencia de palabras, como sucesora de una serie de palabras observadas.\n",
        "\n",
        "Aunque parezca una tarea simple, predecir la siguiente palabra es una de las claves en el aprendizaje del lenguaje para los **Large Language Models** (LLM).\n",
        "\n",
        "Los modelos de lenguaje tambi√©n pueden asignar una probabilidad a una frase completa.\n",
        "\n",
        "**Por ejemplo**\n",
        "\n",
        "Deber√≠an estimar como mucho m√°s probable el texto *‚Äúllevar√© el coche al taller a que le hagan una revisi√≥n‚Äù* que una permutaci√≥n incongruente del mismo texto como *‚Äúque al una a revisi√≥n el hagan coche llevar√© taller le‚Äù*.\n",
        "\n",
        "Encontramos aplicaciones de los modelos de lenguaje en los correctores gramaticales o tipogr√°ficos y en los sistemas de reconocimiento del habla.\n",
        "\n",
        "<center>\n",
        " <img src=\"https://github.com/ednavivianasegura/Curso_PLN/blob/main/corrector-Word.png?raw=1\" alt=\"corrector-Word\" width=\"70%\" height=\"70%\">  \n",
        "</center>\n",
        "\n",
        "El **n-grama** es el tipo de modelo de lenguaje m√°s simple.\n",
        "\n",
        "Ya hemos visto el concepto de n-grama como una secuencia de n palabras.\n",
        "\n",
        "Ahora, con algo de *ambig√ºedad*, denominamos **n-grama** al modelo que estima la *probabilidad* de una palabra como sucesora de una secuencia de $n-1$ palabras dadas.\n",
        "Con ello, tambi√©n podremos asignar probabilidades a frases completas.\n",
        "\n",
        "***\n",
        "**Modelos de lenguaje ‚Äì n-gramas**\n",
        "***\n",
        "\n",
        "Nos planteamos la tarea de calcular $P(w|h)$, la probabilidad de una palabra $w$ como sucesora de una historia $h$.\n",
        "\n",
        "**Por ejemplo**\n",
        "\n",
        "Consideramos la historia *h* es *‚Äúcinco vueltas para llegar al‚Äù* y queremos conocer la probabilidad de que la siguiente palabra *w* sea *‚Äúfinal‚Äù*:\n",
        "\n",
        "$$P(\\text{\"final\"}|\\text{''cinco vueltas para llegar al''}).$$\n",
        "\n",
        "Una manera de estimar esta *probabilidad* es a trav√©s de las **frecuencias relativas** del recuento de *w* seguido de *h* con respecto al de *h*.\n",
        "\n",
        "As√≠, en un corpus contar√≠amos el n√∫mero de veces que aparece *‚Äúcinco vueltas para llegar al‚Äù* y el n√∫mero de veces que aparece seguido de *‚Äúfinal‚Äù*, esto es\n",
        "\n",
        "\n",
        "$$P(\\text{‚Äúfinal‚Äù}|\\text{‚Äúcinco vueltas para llegar al‚Äù})=\\frac{n(\\text{‚Äúcinco vueltas para llegar al final‚Äù})}{n(\\text{‚Äúcinco vueltas para llegar al‚Äù})}$$\n",
        "\n",
        "Consideramos la sucesi√≥n de *m* palabras $w_1, w_2, ... w_m \\equiv w_{1:m} $.\n",
        "\n",
        "Aplicando la **regla de la cadena**, tenemos:\n",
        "\n",
        "\n",
        "$$P(w_{1:m}) = P(w_1)P(w_2|w_1)P(w_3|w_{1:2}) \\cdots P(w_m|w_{1:m-1}) = P(w_1) \\prod_{i=2}^m P(w_i|w_{1:i-1}).$$\n",
        "\n",
        "La **regla de la cadena** ilustra con nitidez la relaci√≥n entre la probabilidad de una *sucesi√≥n de palabras* y la probabilidad de una *palabra* condicionada a las que la anteceden.  \n",
        "As√≠, a trav√©s de un corpus, tenemos c√≥mo estimar las probabilidades de secuencias de palabras con la formulaci√≥n\n",
        "\n",
        "\n",
        "$$P(w_i|w_{1:i-1}) = \\frac{n(w_{1:i})}{n(w_{1:i-1})}$$\n",
        "\n",
        "donde $n(x)$ es el n√∫mero de veces **(la frecuencia absoluta)** que aparece la secuencia de palabras $x$ en el corpus.\n",
        "\n",
        "***\n",
        "\n",
        "El **lenguaje natural** es un fen√≥meno creativo, por lo que es sencillo que se originen nuevas frases o sucesiones de palabras nunca antes registradas.\n",
        "\n",
        "Esta estimaci√≥n de probabilidades solo modeliza sucesiones de palabras exactas encontradas en el corpus.\n",
        "\n",
        "As√≠ que esta propuesta no parece √∫til.\n",
        "\n",
        "Necesitamos una metodolog√≠a que se adapte mejor a la informaci√≥n que nos proporciona un corpus.\n",
        "\n",
        "Para ello, los modelos de *n-gramas* aproximan la probabilidad de una palabra condicionada a su historia previa truncando esta historia a las $n-1$ √∫ltimas palabras.\n",
        "\n",
        "As√≠, el modelo de bigramas condiciona la probabilidad de cada palabra de una sucesi√≥n a su palabra inmediatamente anterior.\n",
        "\n",
        "En el ejemplo anterior, esto ser√≠a:\n",
        "\n",
        "$$P(\\text{‚Äúfinal\"}|\\text{‚Äúcinco vueltas para llegar al\"}) \\triangleq P(\\text{‚Äúfinal\"}|\\text{‚Äúal\"}).$$\n",
        "\n",
        "Por lo tanto, para determinar la probabilidad de toda la secuencia, tendr√≠amos\n",
        "\n",
        "$$ P(\\text{‚Äú * cinco vueltas para llegar al final}\\text{\\STOP\"}) \\triangleq P(\\text{‚Äúcinco \"}|\\text{‚Äú * \"}) \\cdot P(\\text{‚Äúvueltas\"}|\\text{‚Äúcinco\"}) \\cdot P(\\text{‚Äúpara\"}|\\text{‚Äúvueltas\"}) \\cdot P(\\text{‚Äúllegar\"}|\\text{‚Äúvueltas\"}) \\cdot P(\\text{‚Äúal\"}|\\text{‚Äúllegar\"}) \\cdot P(\\text{‚Äúfinal\"}|\\text{‚Äúal\"}) \\cdot P(\\text{‚Äú\\STOP\"}|\\text{‚Äúfinal\"})$$\n",
        "\n",
        "$$ = \\frac{n(‚Äúcinco\")}{n(‚Äú * cinco\")} \\quad \\frac{n(‚Äúvueltas\")}{n(\\text{‚Äúcinco vueltas\"})} \\quad \\frac{n(‚Äúpara\")}{n(\\text{‚Äúvueltas para\"})} \\quad \\frac{n(‚Äúllegar\")}{n(\\text{‚Äúpara llegar\"})} \\quad \\frac{n(‚Äúal\")}{n(\\text{‚Äúllegar al\"})} \\quad \\frac{n(‚Äúfinal\")}{n(\\text{‚Äúal final\"})} \\quad \\frac{n(\\text{‚Äú\\STOP\"})}{n(\\text{‚Äúfinal \\STOP\"})} $$\n",
        "\n",
        "\n",
        "Donde $\\text{‚Äú*‚Äù}$ y $\\text{‚Äú\\STOP‚Äù}$ son palabras especiales (o pseudopalabras) que representan, respectivamente, el inicio y el final de una frase (se suelen insertar en la parte de procesado de texto).\n",
        "\n",
        "De esta manera, podemos interpretar $P(\\text{‚Äúcinco\"}|\\text{\"*\"})$ como la probabilidad de que una frase comience con la palabra *‚Äúcinco‚Äù* y $P(\\text{‚Äú\\STOP\"}‚îÇ\\text{‚Äúfinal\"})$ como la probabilidad de que una frase termine con la palabra *‚Äúfinal‚Äù*.\n",
        "\n",
        "Podemos generalizar el modelo de *n-gramas* de manera que laprobabilidad de una secuencia $w_{1:m}$ sea $$P(w_{1:m})=\\prod_{i=1}^{m+1}P(w_{i}|w_{i-n-1:i-1}),$$\n",
        "\n",
        "\n",
        "donde $w_{i|i\\leq0}= \\ast$ y $w_{m+1}=\\text{\\STOP}$.\n",
        "\n",
        "Se utilizan tantos $\\text{‚Äú*\"}$ como requiera el modelo para comenzar la frase en las $n-1$ primeras palabras.\n",
        "\n",
        "**Ejemplo**\n",
        "\n",
        "La probabilidad de la frase que venimos ilustrando seg√∫n el modeloo de trigramas ser√≠a:\n",
        "\n",
        "\n",
        "$$P(\\text{‚Äú ** cinco vueltas para llegar al final \\STOP \"}) \\triangleq $$\n",
        "$$P(\\text{‚Äúcinco\"|‚Äú ** \"})P(\\text{‚Äúvueltas\"|‚Äú * cinco\"})P(\\text{‚Äúpara\"|‚Äúcinco vueltas\"})P(\\text{‚Äúllegar\"|‚Äúvueltas para\"})$$\n",
        "$$P(\\text{‚Äúal\"|‚Äúpara llegar\"})P(\\text{‚Äúfinal\"|‚Äúllegar al\"})P(\\text{‚Äú\\STOP\"|‚Äúal final\"})$$\n",
        "\n",
        "Aqu√≠, podemos interpretar:\n",
        "\n",
        "* $P(\\text{‚Äúcinco\"|‚Äú ** \"})$ como la probabilidad de que una frase comience con la palabra \"*cinco*\"\n",
        "* $P(\\text{‚Äúvueltas\"|‚Äú* cinco\"})$ como la probabilidad de que una frase comience con el bigrama \"*cinco vueltas*\"\n",
        "* $P(\\text{‚Äú\\STOP\"|‚Äúel final\"})$ como la probabilidad de que una frase termine con el bigrama \"*el final*\".\n",
        "\n",
        "Este modelo se puede usar para predecir la siguiente palabra en una frase incompleta comparando las probabilidades de todas las palabras del vocabulario condicionadas a las $n-1$ palabras anteriores.\n",
        "\n",
        "Esto es, establecer√≠amos un ranquin por el orden decreciente de las probabilidades obtenidas.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZPf0DF8aVp_5"
      },
      "id": "ZPf0DF8aVp_5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "**Etiquetado gramatical ‚Äì Introducci√≥n**\n",
        "***\n",
        "\n",
        "Podemos considerar la categorizaci√≥n *‚Äúm√°s universal‚Äù* de palabras con los siguientes ocho tipos:\n",
        "\n",
        "* nombre                \n",
        "* verbo\n",
        "* pronombre\n",
        "* preposici√≥n\n",
        "* adverbio\n",
        "* conjunci√≥n\n",
        "* art√≠culo.\n",
        "\n",
        "\n",
        "El **etiquetado gramatical** (conocido como **POS tagging**, siendo POS el acr√≥nimo de part-of-speech) es importante en la construcci√≥n y el sentido de una frase.\n",
        "Si se conoce la categor√≠a gramatical de una palabra, podremos deducir sus entornos m√°s veros√≠miles o establecer la estructura sint√°ctica de la frase.\n",
        "\n",
        "Por lo tanto, en etiquetado gramatical consiste en asignar a cada palabra $w_i$ (perteneciente a una sucesi√≥n de palabras $w_{(1:m)}$) una etiqueta $t_i$.\n",
        "Por lo tanto, se trata de una tarea de clasificaci√≥n que opera con secuencias.\n",
        "\n",
        "***\n",
        "**Etiquetado gramatical ‚Äì Modelo**\n",
        "***\n",
        "\n",
        "Dada una frase de m palabras $w_{(1:m)}$ , queremos asignar apropiadamente una sucesi√≥n de categor√≠as gramaticales a $t_{(1:m)}$ de manera que $t_i$ sea la etiqueta de $w_i$ para todo $i$ entre $1$ y $n$.\n",
        "Elegiremos entonces la sucesi√≥n de etiquetas que maximice la probabilidad condicionada a la sucesi√≥n de palabras $w_{(1:m)}$, esto es\n",
        "\n",
        "$$\\hat{t}_{1:m} = \\arg\\max_{t_{1:m}} P(t_{1:m} | w_{1:m}) = \\arg\\max_{t_1,\\ldots,t_m} P(t_1, \\ldots, t_m | w_1, \\ldots, w_m).\n",
        "$$\n",
        "Si atendemos a la probabilidad que queremos maximizar, por el *teorema de Bayes*, se tiene:\n",
        "\n",
        "$$P(t_1, \\ldots, t_m | w_1, \\ldots, w_m) = \\frac{P(w_1, \\ldots, w_m | t_1, \\ldots, t_m) P(t_1, \\ldots, t_m)}{P(w_1, \\ldots, w_m)} \\propto P(w_1, \\ldots, w_m | t_1, \\ldots, t_m) P(t_1, \\ldots, t_m)\n",
        "$$\n",
        "\n",
        "Hemos quitado el denominador porque no depende del etiquetado $t_{(1:m)}$.\n",
        "\n",
        "Aplicando la regla de la cadena de la probabilidad de la intersecci√≥n a las dos probabilidades del producto que queremos maximizar, se tiene:\n",
        "\n",
        "\n",
        "$$P(t_1, \\ldots, t_m) = P(t_1) \\prod_{i=2}^{m} P(t_i | t_{1:i-1})\n",
        "$$\n",
        "\n",
        "$$P(w_1, \\ldots, w_m | t_1, \\ldots, t_m) = P(w_1 | t_1, \\ldots, t_m) \\prod_{i=2}^{m} P(w_i | w_1, \\ldots, w_{i-1}, t_1, \\ldots, t_m)\n",
        "$$\n",
        "\n",
        "Para simplificar el modelo, se asume independencias en los c√°lculos de estas probabilidades.\n",
        "\n",
        "Para la probabilidad de la sucesi√≥n de etiquetas se considera que cada etiqueta tan s√≥lo depende de la que le precede, esto es:\n",
        "\n",
        "$$P(t_1, \\ldots, t_m) \\triangleq \\prod_{i=1}^{m+1} P(t_i | t_{i-1})$$\n",
        "\n",
        "donde $t_0=\\text{‚Äú*\"}$ y $t_{(m+1)}=\\text{‚Äú\\STOP\"}$ son etiquetas especiales para el inicio y cierre de la frase.\n",
        "Para la probabilidad de la sucesi√≥n de palabras condicionadas a la sucesi√≥n de etiquetas, se asume que cada palabra s√≥lo depende de su correspondiente etiqueta, esto es:\n",
        "\n",
        "\n",
        "$$P(w_1, \\ldots, w_m | t_1, \\ldots, t_m) \\triangleq \\prod_{i=1}^{m} P(w_i | t_i)\n",
        "$$\n",
        "\n",
        "Recopilando lo obtenido, tenemos\n",
        "\n",
        "$$\\hat{t}_{1:m} = \\arg \\max_{t_{1:m}} P(t_{1:m} | w_{1:m}) = \\arg \\max_{t_1 \\ldots t_m} \\prod_{i=1}^{m+1} P(t_i | t_{i-1}) \\prod_{i=1}^{m} P(w_i | t_i)\n",
        "$$\n",
        "\n",
        "$$= \\arg \\max_{t_1 \\ldots t_m} \\left[ \\prod_{i=1}^{m} P(t_i | t_{i-1}) P(w_i | t_i) \\right] P(\\text{‚Äú\\STOP\"} | t_{m})\n",
        "$$\n",
        "\n",
        "En esta formulaci√≥n, entre corchetes podemos ver la parte principal del proceso de generaci√≥n de lenguaje a trav√©s del etiquetado.  \n",
        "\n",
        "* $P(t_i‚îÇt_{(i-1)})$ representa la probabilidad de la **transici√≥n** de la etiqueta\n",
        "\n",
        "* $t_{(i-1)}$ a la $t_i$ y $P(w_i‚îÇt_i)$ representa la probabilidad de la **emisi√≥n** de la palabra $w_i$ dado que su etiqueta es $t_i$\n",
        "\n",
        "La primera transici√≥n ser√° $P(t_1‚îÇt_0=\"\\ast\")$, que representa la probabilidad de que una frase comience por una palabra de tipo $t_1$.\n",
        "\n",
        "\n",
        "Las probabilidades de transici√≥n de etiquetado se pueden tomar de un corpus que haya sido etiquetado.\n",
        "\n",
        "**Por ejemplo:**\n",
        "\n",
        "En el **cap√≠tulo 8 de Martin & Jurafsky (2024)** muestran la siguiente matriz de transici√≥n de etiquetado (con las ocho categor√≠as gramaticales b√°sicas) al analizar un corpus. $\\text{‚Äú<s>\"}$ representa la etiqueta especial de inicio de inicio de frase, lo que aqu√≠ estamos denotando con $\\text{‚Äú*\"}$.\n",
        "\n",
        "\n",
        "<center>\n",
        " <img src=\"https://github.com/ednavivianasegura/Curso_PLN/blob/main/matriz-transicion.png?raw=1\" alt=\"matriz-transicion\" width=\"70%\" height=\"70%\">  \n",
        "</center>\n",
        "\n",
        "\n",
        "Las probabilidades de emisi√≥n de palabras tambi√©n se extraen del corpus.\n",
        "\n",
        "Si tratamos de analizar la frase *‚ÄúJanet will back the bill‚Äù* utilizaremos la matriz de emisi√≥n obtenida para usar las probabilidades de emisi√≥n de las palabras que conforman la frase.\n",
        "\n",
        "En la primera columna tenemos las probabilidades de emitir la palabra *‚ÄúJanet‚Äù* condicionado a que esa palabra sea de la categor√≠a gramatical que indica la fila.\n",
        "\n",
        "Podemos ver que la √∫nica opci√≥n veros√≠mil es que *‚ÄúJanet‚Äù* haya sido emitida como nombre propio; esto es, $P(\\text{‚ÄúJanet\"‚îÇNNP})=3.2\\times 10^{-5}>0$ y resulta $0$ si se condiciona a cualquiera de las otras categor√≠as gramaticales.\n",
        "\n",
        "\n",
        "<center>\n",
        " <img src=\"https://github.com/ednavivianasegura/Curso_PLN/blob/main/matriz-emision.png?raw=1\" alt=\"matriz-emision\" width=\"70%\" height=\"70%\">  \n",
        "</center>\n",
        "\n",
        "\n",
        "Operando con alg√∫n m√©todo de optimizaci√≥n de sucesi√≥n de etiquetas (por ejemplo, el algoritmo Viterbi o por fuerza bruta evaluando todas las combinaciones) se estima que el etiquetado de la frase es:\n",
        "\n",
        " ‚ÄúJanet \\ **NNP** will \\ **MD** back \\ **VB** the \\ **DT** bill \\ **NN**‚Äù.\n",
        "\n",
        "N√≥tese que en todas las palabras la etiqueta asignada es la que proporciona una mayor probabilidad de emisi√≥n, pero esto no tiene por qu√© ocurrir siempre.\n",
        "\n",
        "Se ha tomado la sucesi√≥n de etiquetas $\\hat{t}_{(1:5)}$ que maximiza la siguiente formulaci√≥n:\n",
        "\n",
        "\n",
        "$$P(t_1 | \"\\ast\") P(‚Äú\\text{Janet}\" | t_1) P(t_2 | t_1) P(‚Äú\\text{will}\" | t_2) P(t_3 | t_2) P(‚Äú\\text{back}\" | t_3)\n",
        "P(t_4 | t_3) P(‚Äú\\text{the}\" | t_4) P(t_5 | t_4) P(‚Äú\\text{bill}\" | t_5)\n",
        "$$\n",
        "\n",
        "Notemos que no hay cierre $(\\text{‚Äú\\STOP\"})$ en este ejemplo.\n",
        "\n",
        "Podemos considerar que la frase no ha terminado y que hemos encontrado el etiquetado m√°s veros√≠mil hasta llegar a la palabra *‚Äúbill‚Äù*.\n",
        "\n",
        "Glosario de etiquetas para las ocho categor√≠as gramaticales fundamentales:\n",
        "\n",
        "\n",
        "* **NNP**: nombre propio.\n",
        "* **MD**: verbo modal.\n",
        "* **VB**: verbo.\n",
        "* **JJ**: adjetivo.\n",
        "* **NN**: nombre com√∫n.\n",
        "* **RB**: adverbio.\n",
        "* **DT**: determinante.\n"
      ],
      "metadata": {
        "id": "Z7gG4jaGPugi"
      },
      "id": "Z7gG4jaGPugi"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s7JrWhyw7w2U"
      },
      "id": "s7JrWhyw7w2U",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}